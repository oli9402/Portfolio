{
  "hash": "bb66f722562ec80e3fbfc920cfe5a648",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"GLM\"\nformat: \n  html:\n    code-fold: false\n---\n\n\n# Generalized Linear Models\n\nDepending on the outcome variable, we can use different types of models.\nFor example, if the outcome variable is binary, we can use a logistic\nregression model. If the outcome variable is continuous, we can use a\nlinear regression model. If the outcome variable is count data, we can\nuse a Poisson regression model.\n\n## Binary and binomial dependet variable\n\nBinary and binomial data naturally fall within the range of 0 to 1 (and\nare mathematically the same: binary = total number of trials is 1).\nUsing a linear regression model would there predict values outside of\nthis range. This would be a extrapolation which would make sense.\nFurthermore, in the case of logistic regression using a identity link\nfunction (i.e. linear regression) the predicted values would be heavily\nbe influences by extreme values. Therefore, we adapt the linear\nregression based on attributes of the dependent variables. Additionally,\nwe can't assume normal distribution of errors for inference. Therefore,\nwith this kind of data we assume other distributions as binomial\ndistribution.\n\n::: {.callout-note}\nNot all proportions are binomial. Binomial data always refer to a ration\namong two integers. Proportions between conitous variables are not\nbinomial data. Binomial data is something that was tested several times\nand has only two possible outcomes. The number of trials is **essential\nto define the precision of each observation**. So we need independent\ntrials with outcome 0 or 1.\n:::\n\n::: callout-note\nProportions between continous variables can be analysed with LM after\ntransformation (i.e. arc-sin-square-root)\n:::\n\n::: callout-note\nBinomial distribution is a discrete probability distribution that\ndescribes the number of successes in a fixed number of independent\nBernoulli trials, each with the same probability of success. Coin\ntosses.\n:::\n\nFor binary and binomial data we use a logit link function that bounds\nthe y between 0 and 1. The logit link function is defined as:\n\n$$logit(y) = log(\\frac{y}{1-y})$$\n\nWe therefore model the logit of y. The model transforms probabilities\ninto log-odds so we can use linear modeling\n\n$$ logit(\\text{probability of success: p}) = log(\\frac{p}{1-p}) = \\beta_0 + x_1*\\beta_1...$$\n\n$$exp(log(\\frac{p}{1-p})) = \\frac{p}{1-p} = exp(\\beta_0) + exp(\\beta_1)$$\n\n::: callout-note\n-   Logit transforms probabilities into something you can model\n    linearly.\n-   Exponentiation converts model coefficients back to a scale you can\n    interpret (odds ratios).\n-   If you want to interpret effects on probability, use the divide by 4\n    rule or predict actual probabilities.\n:::\n\nFor binomial data we use `cbind()`, for logistic regression we don't use\n`cbind()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.model <- glm(cbind(\"nr_of_successes\", \"nr_of_failures\") ~ \"predictor1\" + \"predictor2\", \n                 data = \"data\", \n                 family = \"binomial\")\n```\n:::\n\n\n## Multinomial dependent variable\n\nIs a categorical variable with more than two levels. For example, the\noutcome variable could be a categorical variable with three levels:\n\"low\", \"medium\", and \"high\". In this case, we can use a multinomial\nlogistic regression model. The multinomial logistic regression model is\nan extension of the binary logistic regression model that allows for\nmore than two categories. The multinomial logistic regression model uses\na softmax function to model the probabilities of each category. The\nsoftmax function converts a vector of raw scores (logits) into a\nprobability distribution, ensuring the sum of probabilities for all\nclasses equals one.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nnet)\nmultinom.model <- multinom(\"outcome_variable\" ~ \"predictor1\" + \"predictor2\", \n                             data = \"data\")\n```\n:::\n\n\n### Set cutoff\n\nFor logistic regression we set a cutoff the classify the outcome\nvariable. The cutoff is the probability threshold above which we\nclassify the outcome variable as 1 (success) and below which we classify\nthe outcome variable as 0 (failure). The default cutoff is 0.5, but we\ncan set a different cutoff based on the specific problem and the cost of\nfalse positives and false negatives.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.log <- glm(\"outcome_variable\" ~ \"predictor1\" + \"predictor2\", \n                 data = \"data\", \n                 family = \"binomial\")\nclassification <- ifelse(fitted(glm.log) > 0.5, 1, 0)\n```\n:::\n\n\nNow let's compare fitted vs. observed (confusion matrix)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompare <- data.frame(obs = df$obs,\n                      fitted = classification)\n# absolute\ntable(obs = compare$obs,\n      fitted = compare$fitted)\n# percentage\ntable(obs = compare$obs,\n      fitted = compare$fitted) %>% \n  prop.table() %>%\n  round(digits = 2)\n```\n:::\n\n\nNaturally we would continue analysing different cutoff points with ROC\nand AUC.\n\n## Poisson regression\n\nThis model handels count data which is often encountered in real life\nsituations where y data doens't follow a normal distribution. Using a\nLinear model to model count data can lead to following problems:\n\n-   The predicted values can be negative, which is not possible for\n    count data.\n-   The predicted values are not integers, which is also not possible\n    for count data.\n-   The variance of count data naturally increases with the expected\n    value (i.e. mean)\n\n::: callout-note\nMean-variance dependence: the variance of the data increases with the\nmean. This is not the case for normal distribution, where the variance\nis constant. Therefore heteroscedasticity is a problem when using linear\nmodels to model count data.\n:::\n\n### Idea\n\nChange the linear model formular so that we only predict positive\nvalues:\n\n$$\\hat y = exp(\\hat \\beta_0 + \\hat \\beta_1*x_1) $$ this is equal to:\n\n$$log(\\hat y) = \\hat \\beta_0 + \\hat \\beta_1*x_1$$\n\nWe see that we can use natural logarithm as a link function. Next, we\nwant y to be integer values and that the variance is dependent on the\nmean. Therefore, we use a Poisson distribution. The Poisson distribution\nis a discrete probability distribution that describes the number of\nevents occurring in a fixed interval of time or space, given a known\naverage rate of occurrence. The Poisson distribution is characterized by\nits mean (λ), which is also equal to its variance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.poisson <- glm(\"outcome_variable\" ~ \"predictor1\" + \"predictor2\", \n                     data = \"data\", \n                     family = \"poisson\")\n```\n:::\n\n\n### Overdispersion\n\n*Dispersion parameter for poisson family taken to be 1* refers to the\nassumption that the variance increases linearly with the mean. In real\nlife situations this is not the case. The variance is often larger than\nthe mean. This is called overdispersion. In this case we can use a\nquasipoisson model.\n\nCompare Residual deviance with the degrees of freedom. If the residual\ndeviance is larger than the degrees of freedom, then we have\noverdispersion.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.poisson <- glm(\"outcome_variable\" ~ \"predictor1\" + \"predictor2\", \n                     data = \"data\", \n                     family = \"quasipoisson\")\n```\n:::\n\n\nThis imokies that the variance increases faster than linearly. The\nestimated coefficents are identifical but the standard error change and\nas a consequences the p-values change. \\## GAM Extension of Linear Model\nto non-linearity\n\n## Interpretation of Coefficients\n\n### Poisson Model\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n#install.packages(\"faraway\")\nlibrary(faraway)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.complaints <- glm(complaints ~ . , data = esdcomp,\n                      family = \"poisson\")\n\nsummary(glm.complaints)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```{style='color: blue'}\n\nCall:\nglm(formula = complaints ~ ., family = \"poisson\", data = esdcomp)\n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)   \n(Intercept) -0.0803448  1.1542122  -0.070  0.94450   \nvisits       0.0009499  0.0003386   2.806  0.00502 **\nresidencyY  -0.2319740  0.2029388  -1.143  0.25301   \ngenderM      0.1122391  0.2235043   0.502  0.61554   \nrevenue     -0.0033827  0.0041553  -0.814  0.41560   \nhours       -0.0001569  0.0006634  -0.237  0.81298   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 89.447  on 43  degrees of freedom\nResidual deviance: 49.995  on 38  degrees of freedom\nAIC: 184.77\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\nSince we are modelling log(y) we have to exponentiate (inverse\nfunctions) the coefficients to get the multiplicative effect of a one\nunit increase in the predictor variable on the expected value of y.\n\n#### Factor\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(coef(glm.complaints)[\"genderM\"]) %>% round(digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ngenderM \n   1.12 \n```\n\n\n:::\n:::\n\n\n::: callout-note\nMales get on average 12% more complaints than women doctors\n:::\n\n#### Continous\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(coef(glm.complaints)[\"visits\"]) %>% round(digits = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n visits \n1.00095 \n```\n\n\n:::\n:::\n\n\n::: callout-note\nFor a given doctor, increasing its number of visits by one,would results\nin about 0.1% more complaints.\n:::\n\nIncrease in visist by one is not really interesting:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrange(esdcomp$visits)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  879 3763\n```\n\n\n:::\n\n```{.r .cell-code}\nexp(coef(glm.complaints)[\"visits\"]*50) %>% round(digits = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n visits \n1.04864 \n```\n\n\n:::\n:::\n\n\n::: callout-note\nFor a given doctor, if you were to increase its number of visits by 50,\nthen we expect this doctor to get about 4.86% more complaints\n:::\n\n### Binomial Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.insects <- glm(cbind(dead, alive)~ conc,\n                  family = \"binomial\",\n                  data = bliss)\n\nsummary(glm.insects)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```{style='color: blue'}\n\nCall:\nglm(formula = cbind(dead, alive) ~ conc, family = \"binomial\", \n    data = bliss)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -2.3238     0.4179  -5.561 2.69e-08 ***\nconc          1.1619     0.1814   6.405 1.51e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 64.76327  on 4  degrees of freedom\nResidual deviance:  0.37875  on 3  degrees of freedom\nAIC: 20.854\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n#### Continous\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(coef(glm.insects)[\"conc\"]) %>% round(digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nconc \n 3.2 \n```\n\n\n:::\n:::\n\n\n::: callout-note\nBy increasing the concentration of the insecticide by one unit, we will\nobtain an increased risk in the odds of about 3 times. Where odds are\nthe ratio of the probability of success (p) to the probability of\nfailure (1-p). In this case success is the death of the insects.\n:::\n\n![Odds of our model](pics/odds.png)\n\nWith one unit increase in concentration, the odds of death increase by a\nfactor of 3. This means that the odds of death are 3 times higher for\neach unit increase in concentration:\n\n$$1:\\frac{1}{3} \\rightarrow 1:1 \\rightarrow 1:3$$\n\n#### Factors\n\n#### Divide by 4\n\nQuick interpretation of odds as upper bound increase in probability of y\n= 1.\n\n### Overdispersion\n\nYou can add quasibinomial to the family argument. This will give you a\ndispersion parameter that is greater than 1. But only works for binomial\ndata and not binary.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}