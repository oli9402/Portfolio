{
  "hash": "2b300a1f937f8f3cfbae700b11b88ec6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Norm'\ncode-fold: false\nformat: pdf\nheader-includes:\n  - \\usepackage{amsmath}\n  - \\usepackage{amsfonts}\n  - \\usepackage{amssymb}\n  - \\usepackage{graphicx}\n---\n\n# Dot Product \nUsed in matrix multiplication.\nFor two vectors use 1D array in python \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nv = np.array([[1],[2],[3]])\ng = np.array([[3],[2],[1]])\n\nprint((v.T@g)[0,0])\n#or\nprint(np.dot(v.T,g)[0,0])\n```\n:::\n\n\n**Definition**:\n\n$$\n\\mathbf{x} \\bullet \\mathbf{y} = \n\\begin{bmatrix}\nx_1 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n\\bullet\n\\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n= x_1 y_1 + \\cdots + x_n y_n = \\sum_{i=1}^n x_i y_i\n$$\n\n\n## Exercise 6.1.\n\n\n\n::: {.callout-caution collapse=\"true\"}\n## Expand $(x+y) \\bullet (x+y)$ (can we express with norm?)\n\n$(x+y) \\bullet (x+y) = x \\bullet x + 2(x\\bullet y)+y \\bullet y$\n\nExpress with norm if x and y are orthogonal because dot product is zero:\n\n$\\left\\|x\\right\\|^2 +\\left\\|y\\right\\|^2$\n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n## Expand $(x+y) \\bullet (x-y)$ (can we express with norm?)\n\n$(x+y) \\bullet (x-y) = x \\bullet x - y \\bullet y$\n\nWe can express with norm\n\n$\\left\\|x\\right\\|^2 - \\left\\|y\\right\\|^2$\n:::\n\n# Norm\n\n**Euclidean Norm** is has a geometrical interpretation. Using pythagorean theorem as often as $\\mathbb{R}^2-1$ we get the norm or length of a vector\n\n$$\n\\|x\\| = \\left\\| \\begin{bmatrix} \nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n \n\\end{bmatrix} \\right\\| = \\sqrt{x_1^2 + \\cdots + x_n^2}\n$$\n\nLinearity of the Euclidean norm doesn't hold:\n\n$$\n\\left\\|x + y \\right\\| \\not= \\left\\|x\\right\\|+ \\left\\|y\\right\\|\n$$\n\nIn fact: \n\n$$\n\\left\\|x + y \\right\\| < \\left\\|x\\right\\|+ \\left\\|y\\right\\|\n$$\n\nSince geometrically x+y represents the fast way (Triangle inequality).\n\nThe dot product of a vector with itself is the squared  Euclidean norm:\n\n$$\nx \\bullet x = \\left\\| x\\right\\|^2\n$$ \n\n### Manhattan norm\n\nDefined by: $\\left\\| x \\right\\|_1 = |x_1|+\\cdots+|x_n|$\n\n### Norm\n\n### Metric\n\n## Orthogonal Projection\n\n\n![Orthogonal Projection onto a line spanned by vector v](images/w.png)\nOrthogonal Projection onto a line spanned by vector v. \n\n$$\nprojection = \\frac{u \\bullet v}{\\left\\|v\\right\\|^2}*v \n$$\nFind orthogonal vector w: \n\n$$\nw = u-p\n$$\nwe can write w as $u = p + w$ and yields a decomposition of *u* as sum of two vectors which are orthogonal.\n\n## Orthonormal\n\nA set is **orthogonal** if any two vectors in that set are orthogonal (i.e., linear independent: $dot \\space product = 0$ or $u\\bullet v = 0$).\n\nA set is **orthonormal** if any two vectors in that set are orthogonal and each vector is normalized (i.e., $norm = 1$ or $u\\bullet u = 1^2$).\n\n- An orthonormal set is also called *orthonormal system* (**ONS** for short). \n- Normalize vectors by diving each components with the norm ($\\left\\|vector\\right\\|$) of vector.\n- *Orthonormal basis* span a vector space:\n  - each vector $v$ in that space can be formulated as linear transformation of those basis $b_1,b_2$. \n  -  Finding the coordinates $a_1,a_2$ can be done by $a_1 = v\\bullet b_1$ and $a_2=v \\bullet b_2$. \n\n$$\nv = (v\\bullet b_1)*b_1 + (v\\bullet b_2)*b_2\n$$\n\n:::{.callout-note}\n$(v\\bullet b_1)*b_1$ is the orthogonal projection of v  onto the line span($b_i$).\n:::\n\n### Whats the use of orthonormal basis (ONB)?\n\nU linear subspace of $\\mathbb{R}^2$ and $b_1,...b_k$ is orthonormal basis of U. Then, for each $vector \\in U$ we have:\n\n$$\nvector = a_1*b_1 +a_2*b_2 +...a_k*b_k \n$$\nso we can decompose each vector into a linear combination of the orthonormal basis (think in geometrical terms where we scale and add x and y basis to display a vector). $a_1,..$ are the coordinates with respect to the basis and we normally have to solve a linear system with gaussian elimination to find these coordinates (where coordinates are the unknown: $s_1,s_2,..$).\n\nBut with ONB we can directly calculate these coordinates with dot product:\n\n$$\nvector = (v\\bullet b_1)b1 + ...\n$$\n\nIn $\\mathbb{R}^2$ with $Ã®$ and $\\hat j$: \n\n$$\n[5 \\space 6] = ([5 \\space 6]^t \\bullet [1 \\space 0]^t) * [1 \\space 0]^t+ ([5\\space  6]^t \\bullet [0 \\space 1]^t) * [0 \\space 1]^t\n$$\n\nThe coordinate vector of $[5 \\space 6]$ is $[5 \\space 6]$ with respect to basis $i$ and $j$ but these are normally not the same. \n\n:::{.callout-note}\nIf possible you can adjuste the vectors before taking dot product:\n\n$$\nv \\bullet b_1 = \\begin{pmatrix} 2 \\\\ 5 \\\\ -7 \\\\ 3\\end{pmatrix} * \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1\\end{pmatrix} = \\frac{1}{2}(2*1 + 5*1 + (-7)*1 + 3 * 1). =\\frac{3}{2}\n$$\nInstead of using: \n\n$$\n\\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{2}\\end{pmatrix}\n$$\n\n\n:::\n\n# Gram-Schmidt Process\n\n:::{.callout-note}\n# Gram-Schmidt Process\nAlgorithm to transform basis into orthonormal basis:\n\nSteps: \n\n1. Normalize: divide by norm \n2. \n:::\n\nTips for normalizing (extract annoying variables that get canceld out later here $\\pi$ ): \n\n$$\n\\left\\|\\begin{pmatrix} \\frac{6}{\\pi} \\\\ \\frac{-7}{\\pi} \\\\ \\frac{-6}{\\pi}\\end{pmatrix} \\right\\| = \\frac{\\frac{1}{\\pi} \\begin{pmatrix} 6 \\\\ -7 \\\\ -6\\end{pmatrix}}{\\frac{1}{\\pi}\\left\\|\\begin{pmatrix} 6 \\\\ -7 \\\\ -6\\end{pmatrix}\\right\\|} = \\frac{1}{\\sqrt{6^2+(-7)^2+(-6)^2}}\\begin{pmatrix} 6 \\\\ -7 \\\\ -6\\end{pmatrix}\n$$\n\nPython: \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nnumpy.linalg.norm(v,1) #Manhattan-norm\nnumpy.linalg.norm(v,2) #Euclidean-norm Deafault\nnumpy.linalg.norm(v,np.inf) #Maxnorm\n\n# Gram-Schmidt-Process\n\nQ,R = numpy.linalg.qr(A) #retursn two matrices: Q is results, R additional Infos\n```\n:::\n\n\n### Spectral Theorem\n\n**Facts** about: symmetric matrices (A = A.T)\n\n- Must be square matrix ($n * n = \\mathbb{R}^n$)\n- Diagonalizable (so there exist an eigenbasis of $\\mathbb{R}^n$: so we can transfrom A with matrix consisting of eigenbasis)\n- if eigenvectors are different -> eigenspaces of these vectors are orthogonal\n  - decomposed original space into subspaces which are orthogonal\n- Applying Gram-Schmidt on all eigenspaces, one obtains an orthonormal basis consisting of eigenvectors of A$ \\mathbb{R}^n$\n\n\nFacts about: Orthogonal Matrices:\n\n- if $v_1,..v_n \\in \\mathbb{R}^n$ and form ONB:\n  - Then the matrix V consisting of $v_1,..v_n$ satisfies: $V^TV=I_n$ since dot product of individual columns are zero except with itself (gives 1).\n- Matrix V is called orthogonal (better: orthonormal?)\n- Inverse of V is $V^T$\n-Finally: $V^TAV=diagonale(A)$\n- Principal Axis Transformation(Matrix decomposition) $ A = VDV^T$\n\nHow to calculate V?:\n\n- Find all eigenvectors(+values)\n- Apply Gram-Schmidt to all eigenspaces\n\n# Singular Value Decomposition (SVD)\n\n\"Generalization of eigenvalue decomposition\". SVD exists for all matrices not just squared ones. \n\n\n\nPrincipale Axis Decomposition: \n\n- A symmetric: $A = VDV^t$\n  - $V$ is orthogonal, $D$ diagonale\n  - If A symmetric then SVD is the same.\n- A general (rectangle): $A= U\\Sigma V^T$ \n  - $U$ orthogonal, $\\Sigma$ orthogonal(\"diagonal\")\n\n\n$\\Sigma$ has same shape as A with $\\sigma_1,..\\sigma_n$ as diagonal entries everything else is zero. These entries are the singular values of A (generalization of eigenvalues).  \nWe are only interested in non-zero singular values which equals the rank of A. \n\n\n\n# Angles between vectors\n\n",
    "supporting": [
      "norm_files/figure-pdf"
    ],
    "filters": []
  }
}