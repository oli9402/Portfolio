{
  "hash": "7610c7b19e56f2c96154bf63343fbc7a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Norm'\ncode-fold: false\n---\n\n# Dot Product \nUsed in matrix multiplication.\nFor two vectors use 1D array in python \n\n::: {#4b774b7f .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nv = np.array([1,2,3])\ng = np.array([3,2,1])\n\n#dot product\nnp.dot(v,g)\n#or\nv@g\n```\n:::\n\n\n**Definition**:\n\n$$\n\\mathbf{x} \\bullet \\mathbf{y} = \n\\begin{bmatrix}\nx_1 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n\\bullet\n\\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n= x_1 y_1 + \\cdots + x_n y_n = \\sum_{i=1}^n x_i y_i\n$$\n\n\n## Exercise 6.1.\n\n\n\n::: {.callout-caution collapse=\"true\"}\n## Expand $(x+y) \\bullet (x+y)$ (can we express with norm?)\n\n$(x+y) \\bullet (x+y) = x \\bullet x + 2(x\\bullet y)+y \\bullet y$\n\nExpress with norm if x and y are orthogonal because dot product is zero:\n\n$\\left\\|x\\right\\|^2 +\\left\\|y\\right\\|^2$\n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n## Expand $(x+y) \\bullet (x-y)$ (can we express with norm?)\n\n$(x+y) \\bullet (x-y) = x \\bullet x - y \\bullet y$\n\nWe can express with norm\n\n$\\left\\|x\\right\\|^2 - \\left\\|y\\right\\|^2$\n:::\n\n# Norm\n\n**Euclidean Norm** is has a geometrical interpretation. Using pythagorean theorem as often as $\\mathbb{R}^2-1$ we get the norm or length of a vector\n\n$$\n\\|x\\| = \\left\\| \\begin{bmatrix} \nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n \n\\end{bmatrix} \\right\\| = \\sqrt{x_1^2 + \\cdots + x_n^2}\n$$\n\nLinearity of the Euclidean norm doesn't hold:\n\n$$\n\\left\\|x + y \\right\\| \\not= \\left\\|x\\right\\|+ \\left\\|y\\right\\|\n$$\n\nIn fact: \n\n$$\n\\left\\|x + y \\right\\| < \\left\\|x\\right\\|+ \\left\\|y\\right\\|\n$$\n\nSince geometrically x+y represents the fast way (Triangle inequality).\n\nThe dot product of a vector with itself is the squared  Euclidean norm:\n\n$$\nx \\bullet x = \\left\\| x\\right\\|^2\n$$ \n\n### Manhattan norm\n\nDefined by: $\\left\\| x \\right\\|_1 = |x_1|+\\cdots+|x_n|$\n\n### Norm\n\n### Metric\n\n## Orthogonal Projection\n\n\n![Orthogonal Projection onto a line spanned by vector v](images/w.png)\nOrthogonal Projection onto a line spanned by vector v. \n\n$$\nprojection = \\frac{u \\bullet v}{\\left\\|v\\right\\|^2}*v \n$$\nFind orthogonal vector w: \n\n$$\nw = u-p\n$$\nwe can write w as $u = p + w$ and yields a decomposition of *u* as sum of two vectors which are orthogonal.\n\n## Orthonormal\n\nA set is **orthogonal** if any two vectors in that set are orthogonal (i.e., linear independent: $dot \\space product = 0$ or $u\\bullet v = 0$).\n\nA set is **orthonormal** if any two vectors in that set are orthogonal and each vector is normalized (i.e., $norm = 1$ or $u\\bullet u = 1$).\n\n- An orthonormal set is also called *orthonormal system* (**ONS** for short). \n- Normalize vectors by diving each components with the norm ($\\left\\|vector\\right\\|$) of vector.\n- Orthonormal basis span a vector space:\n  - each vector $v$ in that space can be formulated as linear transformation of those basis $b_1,b_2$. Finding the coordinates $a_1,a_2$ can be done by $a_1 = v\\bullet b_1$ and $a_2=v \\bullet b_2$. \n\n\n# Angles between vectors\n\n",
    "supporting": [
      "norm_files"
    ],
    "filters": [],
    "includes": {}
  }
}