[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "AusbildungBerufserfahrung\n\n\nM.Sc. in Psychologie | September 2021 – Aktuell | Universität Zürich | Praktikum noch ausstehend\nB.Sc. in Psychologie | September 2018 - Juli 2021 | Universität Zürich | Nebenfach: Filmwissenschaft\nAssessmentjahr Wirtschaftsinformatik | September 2017 - Juli 2018 | Universität Zürich\n\n\nKabinenreiniger (Flugzeuge) | Juli 2019 - November 2019 | Vebego Airport AG | Zürich\nMitarbeiter Wäscherei Bodensee | November 2015 - Juli 2016 | Wäscherei Bodensee AG | Münsterlingen (TG)\nKommissionierer | August 2015 - September 2015 | Lidl Schweiz | Weinfelden (TG)\n\n\n\n\nSkillsSprachkenntnisse\n\n\nProgramming | R | Matlab | C | Python\nProgramme | RStudio | SPSS | LaTex | MS-Office | Abelton Live | Markdown\n\n\nEnglisch | Bilingual\nDeutsch | Bilingual"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "CV",
    "section": "",
    "text": "Aktuell bin ich auf der Suche nach einem Praktikum im Bereich Marketing, Meinungsforschung oder User Experience.\n\nAusbildung\n\n\nUniversität Zürich\nM.Sc. in Psychologie (Praktikum noch offen)\n\n\nZürich\nVoraussichtlicher Abschluss Sommer 2024\n\n\n\nMasterarbeit: Investigating the Neuronal Basis of Learning Processes and Memory Formation in Children and Adolescents with Various Psychiatric Disorders\nNote: 6\n\n\n\nUniversität Zürich\nB.Sc. in Psychologie\n\n\nZürich 2018 - 2021\n\n\n\nBachelorarbeit: Comparison between Support Vector Machine and Convolutional Neural Network for Alzheimer’s Disease Classification\nNote: 6\n\n\n\nUniversität Zürich\nAssessmentjahr Wirtschaftsinformatik\n\n\nZürich\n2017 - 2018\n\n\n\nBerufserfahrungSkillsSprachkenntnisse\n\n\nKabinenreiniger (Flugzeuge) | Juli 2019 - November 2019 | Vebego Airport AG | Zürich\nMitarbeiter Wäscherei Bodensee | November 2015 - Juli 2016 | Wäscherei Bodensee AG | Münsterlingen (TG)\nKommissionierer | August 2015 - September 2015 | Lidl Schweiz | Weinfelden (TG)\n\n\nProgramming | R | Matlab | C | Python\nProgramme | RStudio | SPSS | LaTex | MS-Office | Abelton Live | Markdown\n\n\nEnglisch | Bilingual\nDeutsch | Bilingual\n\n\n\n Download CV"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Willkommen auf meiner Webseite",
    "section": "",
    "text": "Willkommen auf meiner Webseite\nHier findest du mehr Infos über mich, wie auch Beschreibungen meiner bisherigen Projekte. Für Fragen bin ich erreichbar per Email."
  },
  {
    "objectID": "posts/Bachelorarbeit/index.html",
    "href": "posts/Bachelorarbeit/index.html",
    "title": "Bachelorarbeit",
    "section": "",
    "text": "My bachelor thesis “Comparison between Support Vector Machine and Convolutional Neural Network for Alzheimer’s Disease Classification” can be found on GitHub.\n\nKey Takeaways\n\nTopic: Using machine learning for analysing MRI images.\nMethod: Systematic review\nMain pitfalls: Overfitting, unknown ground truth, black box\nGrade: 6\n\n\n\nWhat was is about?\nThere is a rise in using machine learning to analyse MRI images to help identifying Alzheimer’s Disease. In my thesis the current literature on this topic was systematically reviewed. More specifically, two mayor types of machine learning methods were compared: Support Vector Machine and Convolutional Neural Network.\n\n\nWhat I’ve learned?\nMachine learning provides big potential in a lot of areas. Still, there are many pitfalls that need to be addressed. A main problem is overfitting where a statistical model learns random noise that is tied to the training data. Many studies use methods to account for overfitting (e.g., crossvalidiation or independent test data). Nevertheless, inexpensive diagnostical questionnaires tend to perform equally well as more expensive machine learning methods."
  },
  {
    "objectID": "posts/confidence_intervals/index.html",
    "href": "posts/confidence_intervals/index.html",
    "title": "Variable selection",
    "section": "",
    "text": "Author: Oliver Zingg\nFull credit goes to: https://www.kaggle.com/code/hamelg/intro-to-r-part-23-confidence-intervals/notebook\nMy contribution is only adding comments to learn from this code as much as possible"
  },
  {
    "objectID": "posts/confidence_intervals/index.html#generating-data",
    "href": "posts/confidence_intervals/index.html#generating-data",
    "title": "Variable selection",
    "section": "Generating data",
    "text": "Generating data\nThe following example simulates the process of finding the mean age value of a population of voters.\n\nFirst we generate a population with their age values\n\n1 Million values a generated from an exponential distribution and 1.5 Million values from a poisson distribution\n18 is added to make sure that no age value is below 18 years old.\n\n\nFor numbers higher than 100 the modulo (%%) is taken and 18 is added to make sure we don’t end up with too large numbers.\n\n\nCodeset.seed(12)\n\n# Generate a population\npopulation_ages &lt;- c(rexp(1000000,0.015)+18,   \n                    rpois(500000,20)+18,\n                    rpois(500000,32.5)+18,\n                    rpois(500000,45)+18)\n\npopulation_ages &lt;- ifelse(population_ages&lt;100, \n                          population_ages, population_ages%%100+18)\n\n\n\n\n\n\n\n\nCodetrue_mean &lt;- mean(population_ages)   # Check the population mean\n\ntrue_mean"
  },
  {
    "objectID": "posts/confidence_intervals/index.html#drawing-different-sample-from-the-population-and-calculating-the-mean",
    "href": "posts/confidence_intervals/index.html#drawing-different-sample-from-the-population-and-calculating-the-mean",
    "title": "Variable selection",
    "section": "Drawing different sample from the population and calculating the mean",
    "text": "Drawing different sample from the population and calculating the mean\n\nThe central limit theorem states that the distribution of the mean values are normally distributed no matter what the distribution is of the population.\n\nLet’s visualize the distribution of mean values. A skewness of 0 would be a normal distribution.\n\n\n\n\n\n[1] -0.2609927\n\n\nThe skewness is -0.1349 indicating a left sided skewdness"
  },
  {
    "objectID": "posts/confidence_intervals/index.html#confidence-intervals-for-the-mean-values",
    "href": "posts/confidence_intervals/index.html#confidence-intervals-for-the-mean-values",
    "title": "Variable selection",
    "section": "Confidence Intervals for the mean values",
    "text": "Confidence Intervals for the mean values\n\nFirst the sample size is determined then an empty vector is created that will store the boundaries of all confidence intervals\nThe for loop is set to 25, meaning we draw a sample of size 1000 25 times.\nWith qnorm we can get the z value that is used to calculate the boundaries of the confidence intervals\n\n\nCodeset.seed(12)\nsample_size &lt;- 1000\n\nintervals &lt;- c()  # Create and store 25 intervals\n \nfor (sample in 1:25){\nsample_ages &lt;- sample(population_ages, size=sample_size)  # Take a sample of 1000 ages\n\nsample_mean &lt;- mean(sample_ages)  # Get the sample mean\n\nz_critical &lt;- qnorm(0.975)        # Get the z-critical value*\n\npop_stdev &lt;- sd(population_ages)  # Get the population standard deviation\n\nmargin_of_error &lt;- z_critical * (pop_stdev / sqrt(sample_size)) # Calculate margin of error\n\nconfidence_interval  &lt;- c(sample_mean - margin_of_error,  # Calculate the the interval\n                          sample_mean + margin_of_error)  \n\nintervals &lt;- c(intervals, confidence_interval)    \n}\n\ninterval_df &lt;- data.frame(t(matrix(intervals,2,25)))  # Store intervals as data frame"
  },
  {
    "objectID": "posts/confidence_intervals/index.html#plotting-the-25-confidence-intervals",
    "href": "posts/confidence_intervals/index.html#plotting-the-25-confidence-intervals",
    "title": "Variable selection",
    "section": "Plotting the 25 confidence intervals",
    "text": "Plotting the 25 confidence intervals\nThe red line shows the true mean. We see the first confidence interval not including the true mean. (1/25 = 0.04)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nSummary\n\nSimulating confidence intervall shows that some will not include the true value."
  },
  {
    "objectID": "posts/Data Visualization with R/index.html",
    "href": "posts/Data Visualization with R/index.html",
    "title": "Themes for Data Visualization with R",
    "section": "",
    "text": "https://datavizs23.classes.andrewheiss.com/example/05-example.html\n\n\nCodelibrary(tidyverse)  # For ggplot, dplyr, and friends\nlibrary(gapminder)  # For gapminder data\nlibrary(scales)     # For nice axis labels\n\ngapminder_filtered &lt;- gapminder %&gt;% \n  filter(year &gt; 2000)\n\nbase_plot &lt;- ggplot(data = gapminder_filtered,\n                    mapping = aes(x = gdpPercap, y = lifeExp, \n                                  color = continent, size = pop)) +\n  geom_point() +\n  # Use dollars, and get rid of the cents part (i.e. $300 instead of $300.00)\n  scale_x_log10(labels = dollar_format(accuracy = 1)) +\n  # Format with commas\n  scale_size_continuous(labels = comma) +\n  # Use viridis\n  scale_color_viridis_d(option = \"plasma\", end = 0.9) +\n  labs(x = \"GDP per capita\", y = \"Life expectancy\",\n       color = \"Continent\", size = \"Population\",\n       title = \"Here's a cool title\",\n       subtitle = \"And here's a neat subtitle\",\n       caption = \"Source: The Gapminder Project\") +\n  facet_wrap(vars(year))\n\n\n\n\nCodewindowsFonts(`Roboto Condensed` = windowsFont(\"Roboto Condensed\"))\n\nmy_pretty_theme &lt;- theme_minimal(base_family = \"Roboto Condensed\", base_size = 12) +\n  theme(panel.grid.minor = element_blank(),\n        # Bold, bigger title\n        plot.title = element_text(face = \"bold\", size = rel(1.7)),\n        # Plain, slightly bigger subtitle that is grey\n        plot.subtitle = element_text(face = \"plain\", size = rel(1.3), color = \"grey70\"),\n        # Italic, smaller, grey caption that is left-aligned\n        plot.caption = element_text(face = \"italic\", size = rel(0.7), \n                                    color = \"grey70\", hjust = 0),\n        # Bold legend titles\n        legend.title = element_text(face = \"bold\"),\n        # Bold, slightly larger facet titles that are left-aligned for the sake of repetition\n        strip.text = element_text(face = \"bold\", size = rel(1.1), hjust = 0),\n        # Bold axis titles\n        axis.title = element_text(face = \"bold\"),\n        # Add some space above the x-axis title and make it left-aligned\n        axis.title.x = element_text(margin = margin(t = 10), hjust = 0),\n        # Add some space to the right of the y-axis title and make it top-aligned\n        axis.title.y = element_text(margin = margin(r = 10), hjust = 1),\n        # Add a light grey background to the facet titles, with no borders\n        strip.background = element_rect(fill = \"grey90\", color = NA),\n        # Add a thin grey border around all the plots to tie in the facet titles\n        panel.border = element_rect(color = \"grey90\", fill = NA))\n\n\n\nCodebase_plot + \n  my_pretty_theme"
  },
  {
    "objectID": "posts/Data Visualization with R/index.html#create-and-save-theme-my_pretty_theme",
    "href": "posts/Data Visualization with R/index.html#create-and-save-theme-my_pretty_theme",
    "title": "Themes for Data Visualization with R",
    "section": "",
    "text": "CodewindowsFonts(`Roboto Condensed` = windowsFont(\"Roboto Condensed\"))\n\nmy_pretty_theme &lt;- theme_minimal(base_family = \"Roboto Condensed\", base_size = 12) +\n  theme(panel.grid.minor = element_blank(),\n        # Bold, bigger title\n        plot.title = element_text(face = \"bold\", size = rel(1.7)),\n        # Plain, slightly bigger subtitle that is grey\n        plot.subtitle = element_text(face = \"plain\", size = rel(1.3), color = \"grey70\"),\n        # Italic, smaller, grey caption that is left-aligned\n        plot.caption = element_text(face = \"italic\", size = rel(0.7), \n                                    color = \"grey70\", hjust = 0),\n        # Bold legend titles\n        legend.title = element_text(face = \"bold\"),\n        # Bold, slightly larger facet titles that are left-aligned for the sake of repetition\n        strip.text = element_text(face = \"bold\", size = rel(1.1), hjust = 0),\n        # Bold axis titles\n        axis.title = element_text(face = \"bold\"),\n        # Add some space above the x-axis title and make it left-aligned\n        axis.title.x = element_text(margin = margin(t = 10), hjust = 0),\n        # Add some space to the right of the y-axis title and make it top-aligned\n        axis.title.y = element_text(margin = margin(r = 10), hjust = 1),\n        # Add a light grey background to the facet titles, with no borders\n        strip.background = element_rect(fill = \"grey90\", color = NA),\n        # Add a thin grey border around all the plots to tie in the facet titles\n        panel.border = element_rect(color = \"grey90\", fill = NA))\n\n\n\nCodebase_plot + \n  my_pretty_theme"
  },
  {
    "objectID": "posts/Data Visualization with R/index.html#add-annotation",
    "href": "posts/Data Visualization with R/index.html#add-annotation",
    "title": "Themes for Data Visualization with R",
    "section": "Add annotation",
    "text": "Add annotation\n\nCodemultiple_line &lt;- multiple_line + \n  geom_label(aes(x = 1980, y = 45, label = \"I'm quite a long\\nannotation over\\nthree rows\"), \n             hjust = 0, \n             vjust = 0.5, \n             lineheight = 0.8,\n             colour = \"#555555\", \n             fill = \"white\", \n             label.size = NA, \n             family=\"Helvetica\", \n             size = 6) \nmultiple_line"
  },
  {
    "objectID": "posts/Data Visualization with R/index.html#add-legend-to-lines",
    "href": "posts/Data Visualization with R/index.html#add-legend-to-lines",
    "title": "Themes for Data Visualization with R",
    "section": "Add legend to lines",
    "text": "Add legend to lines\n\nCodemultiple_line &lt;- multiple_line + \n  theme(legend.position = \"none\") + \n  xlim(c(1950, 2011)) +\n  geom_label(aes(x = 2007, y = 79, label = \"US\"), \n             hjust = 0, \n             vjust = 0.5, \n             colour = \"#1380A1\", \n             fill = \"white\", \n             label.size = NA, \n             family=\"Helvetica\", \n             size = 6) +\n  geom_label(aes(x = 2007, y = 72, label = \"China\"), \n             hjust = 0, \n             vjust = 0.5, \n             colour = \"#FAAB18\", \n             fill = \"white\", \n             label.size = NA, \n             family=\"Helvetica\", \n             size = 6)\nmultiple_line"
  },
  {
    "objectID": "posts/Data Visualization with R/index.html#add-arrow-to-address-dip-in-china",
    "href": "posts/Data Visualization with R/index.html#add-arrow-to-address-dip-in-china",
    "title": "Themes for Data Visualization with R",
    "section": "Add arrow to address dip in china",
    "text": "Add arrow to address dip in china\n\nCodemultiple_line + geom_curve(aes(x = 1979, y = 45, xend = 1965, yend = 43), \n                             colour = \"#555555\", \n                             size=0.5, \n                             curvature = -0.2,\n                             arrow = arrow(length = unit(0.03, \"npc\")))"
  },
  {
    "objectID": "posts/Data Visualization with R/index.html#dumbbell-plot",
    "href": "posts/Data Visualization with R/index.html#dumbbell-plot",
    "title": "Themes for Data Visualization with R",
    "section": "Dumbbell plot",
    "text": "Dumbbell plot\n\nCodelibrary(\"ggalt\")\nlibrary(\"tidyr\")\n\n#Prepare data\ndumbbell_df &lt;- gapminder %&gt;%\n  filter(year == 1967 | year == 2007) %&gt;%\n  select(country, year, lifeExp) %&gt;%\n  spread(year, lifeExp) %&gt;%\n  mutate(gap = `2007` - `1967`) %&gt;%\n  arrange(desc(gap)) %&gt;%\n  head(10)\n\n#Make plot\nggplot(dumbbell_df, aes(x = `1967`, xend = `2007`, y = reorder(country, gap), group = country)) + \n  geom_dumbbell(colour = \"#dddddd\",\n                size = 3,\n                colour_x = \"#FAAB18\",\n                colour_xend = \"#1380A1\") +\n  bbc_style() + \n  labs(title=\"We're living longer\",\n       subtitle=\"Biggest life expectancy rise, 1967-2007\")"
  },
  {
    "objectID": "posts/Learning_R/index.html",
    "href": "posts/Learning_R/index.html",
    "title": "Learning R with R for Data Science",
    "section": "",
    "text": "Dieses Projekt dient dazu mein Wissen über die Programmiersprache R zu vertiefen. Dazu verwende ich R for Data Science"
  },
  {
    "objectID": "posts/Learning_R/index.html#dplyrs-funktionen",
    "href": "posts/Learning_R/index.html#dplyrs-funktionen",
    "title": "Learning R with R for Data Science",
    "section": "Dplyrs Funktionen",
    "text": "Dplyrs Funktionen\n\n\nfilter() = filtert Reihen ohne ihre Reihenfolge zu ändern\n\narrange() = verändert Reihenfolge ohne zu filtern\n\nDiese Funktionen können mit logischen Statments als Argumente umgehen. %in% ist eine abgekürzte Schreibweise für | und ==.\n\nCode# A shorter way to select flights that departed in January or February\nflights |&gt; \n  filter(month %in% c(1, 2))\n\n# A tibble: 51,955 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 51,945 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nCodeflights |&gt; \n  arrange(year, month, day, dep_time)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nMit der Funktion distinct() kann man Reihne, die mehrfach wiederholt werden rausfiltern\n\nCodeflights |&gt;\n  distinct()\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nAlle einzigartige Kombination von Abflug- und Ankunftsort lassen sich folgenermassen herausfiltern:\n\nCodeflights |&gt;\n  distinct(origin,dest)\n\n# A tibble: 224 × 2\n   origin dest \n   &lt;chr&gt;  &lt;chr&gt;\n 1 EWR    IAH  \n 2 LGA    IAH  \n 3 JFK    MIA  \n 4 JFK    BQN  \n 5 LGA    ATL  \n 6 EWR    ORD  \n 7 EWR    FLL  \n 8 LGA    IAD  \n 9 JFK    MCO  \n10 LGA    ORD  \n# ℹ 214 more rows\n\n\nMit count() kann man die Anzahl Flüge dieser Kombination kalkulieren\n\nCodeflights |&gt;\n  count(origin, dest, sort = TRUE)\n\n# A tibble: 224 × 3\n   origin dest      n\n   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;\n 1 JFK    LAX   11262\n 2 LGA    ATL   10263\n 3 LGA    ORD    8857\n 4 JFK    SFO    8204\n 5 LGA    CLT    6168\n 6 EWR    ORD    6100\n 7 JFK    BOS    5898\n 8 LGA    MIA    5781\n 9 JFK    MCO    5464\n10 EWR    BOS    5327\n# ℹ 214 more rows\n\n\nWenn man mehrere Spalten selektionieren will wie z.B. “x1”, “x2” usw.:\n\nCode# dataset |&gt;\n#  select(num_range(\"x\", 1:3)) selektioniert x1, x2 and x3.\n\n\nMit ==select(new variable name = variable)== kann man die selektionierten Variablen neu benennen. Falls man alle anderen Variablen behalten will, kann man auch ==rename()== verwenden."
  },
  {
    "objectID": "posts/Learning_R/index.html#excercise",
    "href": "posts/Learning_R/index.html#excercise",
    "title": "Learning R with R for Data Science",
    "section": "Excercise",
    "text": "Excercise\nSort flights to find the flights with longest departure delays. Find the flights that left earliest in the morning.\n\nCodeflights |&gt;\n  arrange(desc(dep_delay))\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     9      641            900      1301     1242           1530\n 2  2013     6    15     1432           1935      1137     1607           2120\n 3  2013     1    10     1121           1635      1126     1239           1810\n 4  2013     9    20     1139           1845      1014     1457           2210\n 5  2013     7    22      845           1600      1005     1044           1815\n 6  2013     4    10     1100           1900       960     1342           2211\n 7  2013     3    17     2321            810       911      135           1020\n 8  2013     6    27      959           1900       899     1236           2226\n 9  2013     7    22     2257            759       898      121           1026\n10  2013    12     5      756           1700       896     1058           2020\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\nCodeflights|&gt;\n  arrange(time_hour)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            558        -4      740            728\n 6  2013     1     1      559            559         0      702            706\n 7  2013     1     1      554            600        -6      812            837\n 8  2013     1     1      555            600        -5      913            854\n 9  2013     1     1      557            600        -3      709            723\n10  2013     1     1      557            600        -3      838            846\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nSort flights to find the fastest flights. (Hint: Try including a math calculation inside of your function.)\n\nCodeflights |&gt;\n  mutate(speed = distance/air_time *60, .before = 1) |&gt;  #.before = 1 adds speed to start of coloumns, we can also use .before = year\n  arrange(desc(speed))\n\n# A tibble: 336,776 × 20\n   speed  year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1  703.  2013     5    25     1709           1700         9     1923\n 2  650.  2013     7     2     1558           1513        45     1745\n 3  648   2013     5    13     2040           2025        15     2225\n 4  641.  2013     3    23     1914           1910         4     2045\n 5  591.  2013     1    12     1559           1600        -1     1849\n 6  564   2013    11    17      650            655        -5     1059\n 7  557.  2013     2    21     2355           2358        -3      412\n 8  556.  2013    11    17      759            800        -1     1212\n 9  554.  2013    11    16     2003           1925        38       17\n10  554.  2013    11    16     2349           2359       -10      402\n# ℹ 336,766 more rows\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nWenn dem Argument .keep = “used” behält man alle Variablen die man in der Funktion mutate() verwendet hat.\n\nCodeflights |&gt;\n  mutate(speed = distance/air_time *60, .keep = \"used\") |&gt;\n  arrange(desc(speed))\n\n\nWas there a flight on every day of 2013?\n\nCodeflights |&gt;\n  filter(year == 2013) |&gt;\n  distinct(month,day) |&gt;\n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   365\n\n\nFast flight to Houston’s IAH airport\n\nCodeflights |&gt; \n  filter(dest == \"IAH\") |&gt; \n  mutate(speed = distance / air_time * 60) |&gt; \n  select(year:day, dep_time, carrier, flight, speed) |&gt; \n  arrange(desc(speed)) \n\n# A tibble: 7,198 × 7\n    year month   day dep_time carrier flight speed\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt; &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;\n 1  2013     7     9      707 UA         226  522.\n 2  2013     8    27     1850 UA        1128  521.\n 3  2013     8    28      902 UA        1711  519.\n 4  2013     8    28     2122 UA        1022  519.\n 5  2013     6    11     1628 UA        1178  515.\n 6  2013     8    27     1017 UA         333  515.\n 7  2013     8    27     1205 UA        1421  515.\n 8  2013     8    27     1758 UA         302  515.\n 9  2013     9    27      521 UA         252  515.\n10  2013     8    28      625 UA         559  515.\n# ℹ 7,188 more rows\n\n\nWhich month has most delays on average?\n\nCodeflights |&gt; \n  group_by(month) |&gt; \n  summarize(\n    delay = round(mean(dep_delay, na.rm = TRUE),2), n_rows = n()\n  ) |&gt;\n  arrange(desc(delay))\n\n# A tibble: 12 × 3\n   month delay n_rows\n   &lt;int&gt; &lt;dbl&gt;  &lt;int&gt;\n 1     7 21.7   29425\n 2     6 20.8   28243\n 3    12 16.6   28135\n 4     4 13.9   28330\n 5     3 13.2   28834\n 6     5 13.0   28796\n 7     8 12.6   29327\n 8     2 10.8   24951\n 9     1 10.0   27004\n10     9  6.72  27574\n11    10  6.24  28889\n12    11  5.44  27268\n\nCode#Alternative:\n\n#flights |&gt; \n#  summarize(\n#    delay = mean(dep_delay, na.rm = TRUE), \n#    n = n(),\n#    .by = month\n#  )\n\n\nFind most delays flight for each destination\n\nCodeflights |&gt; \n  group_by(dest) |&gt; \n  slice_max(arr_delay, n = 1) |&gt; #if two flights same delay both are kept, with_ties = FALSE if strictly only one row per destination \n  relocate(dest) |&gt;\n  arrange(desc(arr_delay))\n\n# A tibble: 108 × 19\n# Groups:   dest [105]\n   dest   year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1 HNL    2013     1     9      641            900      1301     1242\n 2 CMH    2013     6    15     1432           1935      1137     1607\n 3 ORD    2013     1    10     1121           1635      1126     1239\n 4 SFO    2013     9    20     1139           1845      1014     1457\n 5 CVG    2013     7    22      845           1600      1005     1044\n 6 TPA    2013     4    10     1100           1900       960     1342\n 7 MSP    2013     3    17     2321            810       911      135\n 8 ATL    2013     7    22     2257            759       898      121\n 9 MIA    2013    12     5      756           1700       896     1058\n10 LAS    2013     5    19      713           1700       853     1007\n# ℹ 98 more rows\n# ℹ 11 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nCodeavg_flight &lt;- flights %&gt;% \n  group_by(month, day) %&gt;% \n  summarize(n = n())\n\n`summarise()` has grouped output by 'month'. You can override using the\n`.groups` argument.\n\nCodeggplot(data = avg_flight,\n       # By default, the y-axis will have December at the top, so use fct_rev() to reverse it\n       mapping = aes(x = day, y = month, fill = n)) +\n  geom_tile() +\n  # Add viridis colors\nscale_fill_viridis_c(option = \"inferno\") + \n  # Add nice labels\n  labs(x = \"Day\", y = \"Month\",\n       title = \"Flights per day\",\n       subtitle = \"2013\",\n       fill = \"Count\") +\n\n  # Force all the tiles to have equal widths and heights\n  coord_equal() +\n  # Use a cleaner theme\n  theme_minimal()+\n  #new yticks\n  scale_y_continuous(breaks = seq(1, 12, 1), limits = c(1, 12)) \n\nWarning: Removed 62 rows containing missing values (`geom_tile()`)."
  },
  {
    "objectID": "posts/Masterarbeit/index.html",
    "href": "posts/Masterarbeit/index.html",
    "title": "Masterarbeit",
    "section": "",
    "text": "My master thesis “Investigating the Neuronal Basis of Learning Processes and Memory Formation in Children and Adolescents with Various Psychiatric Disorders” can be found on GitHub\n\nKey Takeaways\n\nTopic: Using EEG to track successful learning in children and adolescents with psychiatric disorders\nMethod: Analyzing existing data set\nStand out: Large data set (n &gt; 1000),\nMain pitfalls: Model fit, confoundation, validity of EEG\nGrade: 6\n\n\n\nWhat was is about?\nIt has been suggested that an EEG brain marker can be used to track successful learning. To investigate whether this still holds in a sample of children and adolescents with psychiatric disorders and/or learning problems, such a sample was analysed. More specific a sample of over 1’000 participants with a longitudinal data structure (i.e., data point every 2 ms during a learning task) was analysed using linear mixed-effects models. The results indicated that the same EEG marker can be used in such a population, therefore paving the way for using brain data to aid learning and help with identifying individuals with learning problems.\n\n\nWhat I’ve learned?\nA big part of this thesis consisted of preparing and analyzing a large complex data set (12 GB). This was an interesting experience in which I could learn a lot about programming and challenges that come with such a data set. Additionally, a deeper understanding of complex mixed-effects models was gained. Especially, the limits when it comes to fitting non-normal data and interpretation of the resulting coefficients was challenging. There are still questions regarding the validity of the investigated EEG marker that couldn’t be answered with my thesis. This was due to the nature of EEG data and confounding variables that couldn’t be controlled for. Nevertheless, research in this area could possible lead to promising tools that may be helpful in clinical or school settings."
  },
  {
    "objectID": "posts/Simpsons_Paradox/index.html",
    "href": "posts/Simpsons_Paradox/index.html",
    "title": "Simpson’s Paradox",
    "section": "",
    "text": "Simpson’s Paradox bezeichnet ein statistische Phänomen, in dem die Daten eine hierarchisch angeordnete Struktur aufweisen. Die Analyse der Daten kann deshalb auf unterschiedliche Ebenen dieser Sturktur gemacht werden und Einfluss auf das Resultat haben. Teilweise kann wird dadurch die Interpretation der Forschungsfrage beeinflusst je nach untersuchten Ebenen. Simpson’s Paradox wird oft verwendet um die Vorteile von Linear Mixed Effects Model im Vergleich zur einfacher linearen Regression darzustellen."
  },
  {
    "objectID": "posts/Simpsons_Paradox/index.html#simpsons-paradox",
    "href": "posts/Simpsons_Paradox/index.html#simpsons-paradox",
    "title": "Simpson’s Paradox",
    "section": "",
    "text": "Simpson’s Paradox bezeichnet ein statistische Phänomen, in dem die Daten eine hierarchisch angeordnete Struktur aufweisen. Die Analyse der Daten kann deshalb auf unterschiedliche Ebenen dieser Sturktur gemacht werden und Einfluss auf das Resultat haben. Teilweise kann wird dadurch die Interpretation der Forschungsfrage beeinflusst je nach untersuchten Ebenen. Simpson’s Paradox wird oft verwendet um die Vorteile von Linear Mixed Effects Model im Vergleich zur einfacher linearen Regression darzustellen."
  },
  {
    "objectID": "posts/Simpsons_Paradox/index.html#hierarchische-datenstruktur",
    "href": "posts/Simpsons_Paradox/index.html#hierarchische-datenstruktur",
    "title": "Simpson’s Paradox",
    "section": "Hierarchische Datenstruktur",
    "text": "Hierarchische Datenstruktur\nIm Querschnitt\nBeispiele sind:\n\nSchüler genested in Schulen (Zwei Ebenen)\nSchüler genested in Schulen genested in Kantonen (Drei Ebenen)\nMitarbeiter genested in Unternehmen (Zwei Ebenen)\nIm Längschnitt\nBeispiele sind:\n\nPrüfungsnoten genested in Schüler (Zwei Ebenen)\nKundenzufriedenheit genested Person genested in Altersgruppen (Drei Ebenen)"
  },
  {
    "objectID": "posts/Simpsons_Paradox/index.html#beispiel-pinguine",
    "href": "posts/Simpsons_Paradox/index.html#beispiel-pinguine",
    "title": "Simpson’s Paradox",
    "section": "Beispiel Pinguine",
    "text": "Beispiel Pinguine\nDaten (Gorman (2014)):\n\npalmerpenguins\n\n344 Pinguine\n3 Arten (Adéline, chinstrap und gentoo)\n\n\n\nDie Daten wurden von Dr Allison Horst, Alison Hill und Kristen Gorman zu einem R Packet palmerpenguins verarbeitet. Das folgende Projekt basiert auf dem Turtorial von Silvia Canelon.\nZuerst, R Packete installieren\n\nCodeinstall.packages(\"palmerpenguins\") #Data\ninstall.packages(\"tibble\") #Data handling\ninstall.packges(\"ggplot2\") #Plotting\ninstall.packages(\"dplyr\")\n\n\nPackete laden\n\nCodelibrary(palmerpenguins)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttache Paket: 'dplyr'\n\n\nDie folgenden Objekte sind maskiert von 'package:stats':\n\n    filter, lag\n\n\nDie folgenden Objekte sind maskiert von 'package:base':\n\n    intersect, setdiff, setequal, union\n\nCodepenguins &lt;- palmerpenguins::penguins\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA &lt;NA&gt;   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\nDaten explorieren\n\nCodeggplot(data = penguins,\n       aes(x = sex, y = body_mass_g))+\n  geom_boxplot(aes(fill = species))\n\n\n\n\nDie Boxplotte deuten darauf hin, dass bei Gentoo ein geschlechtsspezifischer Unterschiedlich im Body Mass zu sehen ist, wobei dies möglicherweise nicht der Fall ist bei Adelie und Chinstrap.\n\nCodepenguins %&gt;%\n  select(species, sex, body_mass_g) %&gt;%\n  arrange(desc(body_mass_g))\n\n# A tibble: 344 × 3\n   species sex   body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;       &lt;int&gt;\n 1 Gentoo  male         6300\n 2 Gentoo  male         6050\n 3 Gentoo  male         6000\n 4 Gentoo  male         6000\n 5 Gentoo  male         5950\n 6 Gentoo  male         5950\n 7 Gentoo  male         5850\n 8 Gentoo  male         5850\n 9 Gentoo  male         5850\n10 Gentoo  male         5800\n# … with 334 more rows\n\n\n\nCodepenguins %&gt;%\n  select(species, sex, body_mass_g) %&gt;%\n  group_by(species,sex) %&gt;%\n  summarize(mean = mean(body_mass_g))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex     mean\n  &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt;\n1 Adelie    female 3369.\n2 Adelie    male   4043.\n3 Adelie    &lt;NA&gt;     NA \n4 Chinstrap female 3527.\n5 Chinstrap male   3939.\n6 Gentoo    female 4680.\n7 Gentoo    male   5485.\n8 Gentoo    &lt;NA&gt;     NA \n\n\n\nCodepenguins %&gt;%\n  group_by(species) %&gt;%\n  mutate(n_species = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(species,sex,n_species) %&gt;%\n  summarize(n=n())\n\n`summarise()` has grouped output by 'species', 'sex'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   species, sex [8]\n  species   sex    n_species     n\n  &lt;fct&gt;     &lt;fct&gt;      &lt;int&gt; &lt;int&gt;\n1 Adelie    female       152    73\n2 Adelie    male         152    73\n3 Adelie    &lt;NA&gt;         152     6\n4 Chinstrap female        68    34\n5 Chinstrap male          68    34\n6 Gentoo    female       124    58\n7 Gentoo    male         124    61\n8 Gentoo    &lt;NA&gt;         124     5\n\n\n\nCodepenguins %&gt;%\n  count(species, sex) %&gt;%\n  add_count(species, wt = n,\n            name = \"n_species\")\n\n# A tibble: 8 × 4\n  species   sex        n n_species\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;     &lt;int&gt;\n1 Adelie    female    73       152\n2 Adelie    male      73       152\n3 Adelie    &lt;NA&gt;       6       152\n4 Chinstrap female    34        68\n5 Chinstrap male      34        68\n6 Gentoo    female    58       124\n7 Gentoo    male      61       124\n8 Gentoo    &lt;NA&gt;       5       124\n\n\n\nCodepenguins %&gt;%\n  count(species, sex) %&gt;%\n  add_count(species, wt = n,\n            name = \"n_species\") %&gt;%\n  mutate(prop = n/n_species*100)\n\n# A tibble: 8 × 5\n  species   sex        n n_species  prop\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Adelie    female    73       152 48.0 \n2 Adelie    male      73       152 48.0 \n3 Adelie    &lt;NA&gt;       6       152  3.95\n4 Chinstrap female    34        68 50   \n5 Chinstrap male      34        68 50   \n6 Gentoo    female    58       124 46.8 \n7 Gentoo    male      61       124 49.2 \n8 Gentoo    &lt;NA&gt;       5       124  4.03\n\n\n\nCodepenguins %&gt;%\n  count(species, sex) %&gt;%\n  add_count(species, wt = n,\n            name = \"n_species\") %&gt;%\n  mutate(prop = n/n_species*100) %&gt;%\n  filter(species == \"Chinstrap\")\n\n# A tibble: 2 × 5\n  species   sex        n n_species  prop\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Chinstrap female    34        68    50\n2 Chinstrap male      34        68    50\n\n\n\nCodepenguins_new &lt;-\n  penguins %&gt;%\n  mutate(year_factor = factor(year, levels = unique(year)))\npenguins_new\n\n# A tibble: 344 × 9\n   species island    bill_length_mm bill_d…¹ flipp…² body_…³ sex    year year_…⁴\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;    &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;  \n 1 Adelie  Torgersen           39.1     18.7     181    3750 male   2007 2007   \n 2 Adelie  Torgersen           39.5     17.4     186    3800 fema…  2007 2007   \n 3 Adelie  Torgersen           40.3     18       195    3250 fema…  2007 2007   \n 4 Adelie  Torgersen           NA       NA        NA      NA &lt;NA&gt;   2007 2007   \n 5 Adelie  Torgersen           36.7     19.3     193    3450 fema…  2007 2007   \n 6 Adelie  Torgersen           39.3     20.6     190    3650 male   2007 2007   \n 7 Adelie  Torgersen           38.9     17.8     181    3625 fema…  2007 2007   \n 8 Adelie  Torgersen           39.2     19.6     195    4675 male   2007 2007   \n 9 Adelie  Torgersen           34.1     18.1     193    3475 &lt;NA&gt;   2007 2007   \n10 Adelie  Torgersen           42       20.2     190    4250 &lt;NA&gt;   2007 2007   \n# … with 334 more rows, and abbreviated variable names ¹​bill_depth_mm,\n#   ²​flipper_length_mm, ³​body_mass_g, ⁴​year_factor\n\n\nVisualize Simpson’s Paradox\n\nCodeggplot(data = penguins,\n       mapping = aes(x = bill_length_mm, y = bill_depth_mm))+\n  geom_point() +\n  geom_smooth(method='lm', formula = y ~x)\n\n\n\n\n\nCodepenguins %&gt;%\n  ggplot(mapping = aes(x = bill_length_mm, y = bill_depth_mm, group = species, col = species)) + \n  geom_point() +\n  geom_smooth(method = 'lm', formula = y ~x)\n\n\n\n\nMore information on mixed-effects models and simpson’s paradox (see Hox, Moerbeek, and Van de Schoot 2017)"
  },
  {
    "objectID": "posts/variable_selection/index.html",
    "href": "posts/variable_selection/index.html",
    "title": "Accuracy of selected predictor per sample size",
    "section": "",
    "text": "With this exercise we want to inspect how lm function and step function vary when it comes to selection of correct variables depending on sample size\nOur team: Maué Pantoja , Chiara Breuer, Oliver Zingg\nCode# Load packages \nlibrary(lme4)\nlibrary(dplyr)\nlibrary(ggplot2)\n\noptions(scipen=0)\nset.seed(101)\n\n# Variable and Dataset preparation\nN &lt;- c(25, 30, 40, 60, 100 ,500, 1000)\n# create a data frame to later add which variables were selected \npredi_selection_lm &lt;- as.data.frame(matrix(NA, nrow = 19, ncol = length(N)))\npredi_selection_step &lt;- as.data.frame(matrix(NA, nrow = 19, ncol = length(N)))\n\nnames(predi_selection_lm ) &lt;- c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")\nnames(predi_selection_step ) &lt;- c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")\n\n#counter to use as index for table predi_selection (loop)\na = 1\n\n#loop over different N (sample size)\nfor(i in N){\n  # Simulation dataset\n  x1 &lt;- runif(i, 0, 10)\n  x2 &lt;- runif(i, 0, 10)\n  x3 &lt;- runif(i, 0, 10)\n  x4 &lt;- runif(i, 0, 10)\n  x5 &lt;- runif(i, 0, 10)\n  x6 &lt;- runif(i, 0, 10)\n  x7 &lt;- runif(i, 0, 10)\n  x8 &lt;- runif(i, 0, 10)\n  x9 &lt;- runif(i, 0, 10)\n  x10 &lt;- runif(i, 0, 10)\n  x11 &lt;- runif(i, 0, 10)\n  x12 &lt;- runif(i, 0, 10)\n  x13 &lt;- runif(i, 0, 10)\n  x14 &lt;- runif(i, 0, 10)\n  x15 &lt;- runif(i, 0, 10)\n  x16 &lt;- runif(i, 0, 10)\n  x17 &lt;- runif(i, 0, 10)\n  x18 &lt;- runif(i, 0, 10)\n  x19 &lt;- runif(i, 0, 10)\n  err &lt;- runif(i, 0, 5)\n  \n  data &lt;- cbind(x1,x2,x3,x4,x5,x6, x7, x8, x9 ,x10, x11, x12, x13, x14, x15, x16, x17, x18, x19,  err)\n  data &lt;- as.data.frame(data)\n \n  # generate y values.\n  y &lt;- 30 + 0.4*x1 + 1*x2 + 2.3*x3 + 0.7*x4 + 0.2*x5  + 1*x6 + 2*x7 + 3*x8 + 1.5 *x9 + 0.5*x10 + err\n  \n  # add generated y values to data frame\n  data_2 &lt;- cbind(data, y)\n  \n  # Modelling with all variables as predi.\n  mod &lt;- lm(y~ x1 + x2 + x3 + x4 + x5 + x5 +x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18 + x19, data =  data_2)\n  summary(mod)\n  \n  ########################\n  #lets try step function#\n  ########################\n \n\n  # use step function and save final model as stepwise_model\n  stepwise_model &lt;-  step(mod)\n\n  \n  # Extract p-val: lm\n  p_val_lm &lt;- (summary(mod)$coefficients[,4])\n  p_val_lm &lt;- p_val_lm[2:20] #take out intercept\n  \n  # Extract p-val: step\n  p_val_step &lt;- summary(stepwise_model)$coefficients[,4]\n  p_val_step &lt;- p_val_step[2:20] #take out intercept\n  \n  # Store value: lm \n  predi_selection_lm[a] &lt;- ifelse(p_val_lm &lt; 0.05, 1, 0)\n  \n  #store values for step different, because summary output dosen't include all variable like in lm \n  p_val_step &lt;-  na.omit(ifelse(p_val_step &lt; 0.05, 1, 0))\n \n   #remove selected pred. without 0.05\n  p_val_step &lt;- p_val_step[! p_val_step %in% 0]\n  \n  # save the names of all selected coefficients as coes\n  coes &lt;- names(p_val_step)\n  \n  # create dummy var with all possible variables \n  dummy_var &lt;- c('x1','x2','x3','x4','x5','x6','x7','x8','x9','x10','x11','x12','x13','x14','x15','x16','x17','x18','x19')\n  \n  # which out of all possible variable have been selected? \n  selected_coe &lt;- ifelse(dummy_var %in% coes, 1,0)\n  \n  predi_selection_step[a] &lt;- selected_coe\n  \n  #increase counter by one\n  a = a + 1\n}\n\n# Summary accuracy predictors selection\ntrue_pred_dummy &lt;- c(1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0) #dummy code for the existing predictor \n\n# new data frame to compare true predi. to selected predi.\npredi_accuracy_lm &lt;- data.frame(matrix(NA, nrow = 19, ncol = length(N)))\nnames(predi_accuracy_lm) &lt;- c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")\n\npredi_accuracy_step &lt;- data.frame(matrix(NA, nrow = 19, ncol = length(N)))\nnames(predi_accuracy_step) &lt;- c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")\n\n#if all correct than coloumn all 1. \nfor (j in 1:length(N)){\n  predi_accuracy_lm[j] &lt;- ifelse(predi_selection_lm[j] == true_pred_dummy, 1,0)\n  predi_accuracy_step[j] &lt;- ifelse(predi_selection_step[j] == true_pred_dummy, 1,0)\n}"
  },
  {
    "objectID": "posts/variable_selection/index.html#now-lets-visualize-these-results",
    "href": "posts/variable_selection/index.html#now-lets-visualize-these-results",
    "title": "Accuracy of selected predictor per sample size",
    "section": "Now lets visualize these results",
    "text": "Now lets visualize these results\n\nCode# final data frame used for visualization\nsample_s &lt;- c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")\nsummary_accuracy &lt;- data.frame(cbind(sample_s,rep(NA, times = length(sample_s)),rep(NA, times = length(sample_s))))\nnames(summary_accuracy) &lt;- c(\"sample_s\", \"accuracy_lm\", \"accuracy_step\")\n\n\n\nfor( i in 1: nrow(summary_accuracy)){\n  summary_accuracy[i,2] &lt;-  mean(predi_accuracy_lm[,i])\n  summary_accuracy[i,3] &lt;-  mean(predi_accuracy_step[,i])\n\n}\nsummary_accuracy$accuracy_lm &lt;- as.numeric(summary_accuracy$accuracy_lm)\nsummary_accuracy$accuracy_step &lt;- as.numeric(summary_accuracy$accuracy_step)\nggplot(data = summary_accuracy,aes(x=factor(sample_s, \n                                           level = c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")) ,\n                                            y = accuracy_lm)) + \n  geom_bar(stat= \"identity\")+\n  xlab(\"sample size\")+\n  labs(title = \"Accuracy of selected predictor per sample size\")\n\n\n\n\nThis is the accuracy of the normal lm function with all predictors used.\n\nCodeggplot(data = summary_accuracy,aes(x=factor(sample_s, \n                                           level = c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")) ,\n                                            y = accuracy_step)) + \n  geom_bar(stat= \"identity\")+\n  xlab(\"sample size\")+\n  labs(title = \"Accuracy of selected predictor per sample size\")\n\n\n\n\nThis is the accuracy of the step function default = backwards selection (starting with full model: all predictors) ."
  },
  {
    "objectID": "posts/variable_selection/index.html#summary",
    "href": "posts/variable_selection/index.html#summary",
    "title": "Accuracy of selected predictor per sample size",
    "section": "Summary:",
    "text": "Summary:\n\nespecially with low sample size N25 step function seems to perform less well than normal lm function\nwith increasing sample size both perform well and use all relevant predictors"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nThemes for Data Visualization with R\n\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2023\n\n\nOliver Zingg\n\n\n\n\n\n\n  \n\n\n\n\nLearning R with R for Data Science\n\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2023\n\n\nOliver Zingg\n\n\n\n\n\n\n  \n\n\n\n\nConfidence Intervals\n\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\nOliver Zingg\n\n\n\n\n\n\n  \n\n\n\n\nSimpson’s Paradox\n\n\n\n\n\n\n\nLinear Mixed Effects\n\n\nStatistics\n\n\nRandom Effects\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nOliver Zingg\n\n\n\n\n\n\n  \n\n\n\n\nMasterarbeit\n\n\n\n\n\n\n\nData Analysis\n\n\nMixed-Effects Model\n\n\nEEG\n\n\nPsychiatric Disorders\n\n\nMasterarbeit\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\nOliver Zingg\n\n\n\n\n\n\n  \n\n\n\n\nAccuracy of selected predictors per sample size\n\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2022\n\n\n\n\n\n\n  \n\n\n\n\nBachelorarbeit\n\n\n\n\n\n\n\nMachine Learning\n\n\nSystematic literature review\n\n\nMRI\n\n\nBachelorarbeit\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2021\n\n\nOliver Zingg\n\n\n\n\n\n\nNo matching items"
  }
]