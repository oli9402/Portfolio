[
  {
    "objectID": "wiki/Python/basics.html",
    "href": "wiki/Python/basics.html",
    "title": "Learning the Basics",
    "section": "",
    "text": "Use # %% Testto create executable blocks",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#namespace",
    "href": "wiki/Python/basics.html#namespace",
    "title": "Learning the Basics",
    "section": "Namespace",
    "text": "Namespace\n\nbuilt-in: all objects provided by python\nglobal: programmer defined objects across whole script\nlocal: programmer defined objects in function blocks\n\nObject name must only be unique within a given namespace:\n\ndef my_function(x):\n  return x+10\n\ndef test_function(x):\n  def my_function(y):  # my_function: this is possible since different namespace \n    return y + 20\n  x = my_function(2)\n  return x \n\ntest_function()(y=10)\n\nInside a function you can access a global object via the keyword global. You can also increase a global var inside a function for example. But this should be avoided global access.\nThe keyword nonlocal will access a variable in the next higher namespace.",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#scope",
    "href": "wiki/Python/basics.html#scope",
    "title": "Learning the Basics",
    "section": "Scope",
    "text": "Scope\nScope of an object refers to the code section from which an object is accessible.",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#indexing",
    "href": "wiki/Python/basics.html#indexing",
    "title": "Learning the Basics",
    "section": "Indexing",
    "text": "Indexing\nLast index is not included:\n\na = [1,2,3,4,5]\n\n# Full list\na[:5]\n\na[-1:] #equal to a[-1:5]\n\na[-1:1:-1] #2 is index 2 but not included\n\n#full list in reverse\na[-1::-1]   #equal to a[::-1]\na[:-2] #-2 is index of 4 but not included\n\na[:-2:-1]\n\n# Print last string of language\nL = [\"Mojo\", \"is\", \"a\", \"language\"]\nprint(L[3][-1:])\n\n# Print up to last string in last element\nL = [\"Mojo\", \"is\", \"a\", \"language\"]\nprint(L[3][:-1])\n\n2D Lists return first line\n\ndata = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n[row[0] for row in data]\n\nModify index\n\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nnumbers[1:4] = [10, 20, 30]\n\nFor loops\n\nnumbers = [12, 32, 34, 4,25, 6, 72, 8, 19] #9 elements\n\nlen(numbers) #returns 9\nprint('')\n\ni = range(6) #returns 0-5\nfor idx in i: \n  print(idx, end= ' ')\n  \nprint('\\n')\ni = range(len(numbers)) #0-8 -&gt; 9 elements\nfor idx in i: \n  print(idx, end= ' ')\n\nprint('\\n')\ni = range(1,len(numbers)) #1-8 -&gt; 8 elements\nfor idx in i: \n  print(idx, end= ' ')\n\nEnumerate returns index and value\n\nnumbers = [12, 32, 34, 4,25, 6, 72, 8, 19] #9 elements\n\nfor i in enumerate(numbers):\n  print(i, end = ', ')\n  print(i[0], end = ', ')\n  print(i[1])\n\nprint('----')\n# we can also save both values individuel\nfor idx, value in enumerate(numbers):\n  print(idx , value)\n\nUsing range(start, stop, step) in for loop we can reverse the steps with -1.\n\nfor i in range(5,0,-1):\n  print(i)",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#decorator-not-in-exam-22000",
    "href": "wiki/Python/basics.html#decorator-not-in-exam-22000",
    "title": "Learning the Basics",
    "section": "Decorator (not in exam) 2:20:00",
    "text": "Decorator (not in exam) 2:20:00",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "posts/confidence_intervals/index.html",
    "href": "posts/confidence_intervals/index.html",
    "title": "Confidence Intervals",
    "section": "",
    "text": "Full credit goes to: https://www.kaggle.com/code/hamelg/intro-to-r-part-23-confidence-intervals/notebook\nMy contribution is only adding comments to learn from this code as much as possible\nMore on confidence intervals can be found here Hoekstra et al. (2014)"
  },
  {
    "objectID": "posts/confidence_intervals/index.html#generating-data",
    "href": "posts/confidence_intervals/index.html#generating-data",
    "title": "Confidence Intervals",
    "section": "Generating data",
    "text": "Generating data\nThe following example simulates the process of finding the mean age value of a population of voters.\n\nFirst we generate a population with their age values\n\n1 Million values a generated from an exponential distribution and 1.5 Million values from a poisson distribution\n18 is added to make sure that no age value is below 18 years old.\n\nFor numbers higher than 100 the modulo (%%) is taken and 18 is added to make sure we don’t end up with too large numbers.\n\n\n\nCode\nset.seed(12)\n\n# Generate a population\npopulation_ages &lt;- c(rexp(1000000,0.015)+18,   \n                    rpois(500000,20)+18,\n                    rpois(500000,32.5)+18,\n                    rpois(500000,45)+18)\n\npopulation_ages &lt;- ifelse(population_ages&lt;100, \n                          population_ages, population_ages%%100+18)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntrue_mean &lt;- mean(population_ages)   # Check the population mean\n\ntrue_mean"
  },
  {
    "objectID": "posts/confidence_intervals/index.html#drawing-different-sample-from-the-population-and-calculating-the-mean",
    "href": "posts/confidence_intervals/index.html#drawing-different-sample-from-the-population-and-calculating-the-mean",
    "title": "Confidence Intervals",
    "section": "Drawing different sample from the population and calculating the mean",
    "text": "Drawing different sample from the population and calculating the mean\n\nThe central limit theorem states that the distribution of the mean values are normally distributed no matter what the distribution is of the population.\n\n\nLet’s visualize the distribution of mean values. A skewness of 0 would be a normal distribution.\n\n\n\n\n\n\n\n\n\n[1] -0.2609927"
  },
  {
    "objectID": "posts/confidence_intervals/index.html#confidence-intervals-for-the-mean-values",
    "href": "posts/confidence_intervals/index.html#confidence-intervals-for-the-mean-values",
    "title": "Confidence Intervals",
    "section": "Confidence Intervals for the mean values",
    "text": "Confidence Intervals for the mean values\n\nFirst the sample size is determined then an empty vector is created that will store the boundaries of all confidence intervals\nThe for loop is set to 25, meaning we draw a sample of size 1000 25 times.\nWith qnorm we can get the z value that is used to calculate the boundaries of the confidence intervals\n\n\n\nCode\nset.seed(12)\nsample_size &lt;- 1000\n\nintervals &lt;- c()  # Create and store 25 intervals\n \nfor (sample in 1:25){\nsample_ages &lt;- sample(population_ages, size=sample_size)  # Take a sample of 1000 ages\n\nsample_mean &lt;- mean(sample_ages)  # Get the sample mean\n\nz_critical &lt;- qnorm(0.975)        # Get the z-critical value*\n\npop_stdev &lt;- sd(population_ages)  # Get the population standard deviation\n\nmargin_of_error &lt;- z_critical * (pop_stdev / sqrt(sample_size)) # Calculate margin of error\n\nconfidence_interval  &lt;- c(sample_mean - margin_of_error,  # Calculate the the interval\n                          sample_mean + margin_of_error)  \n\nintervals &lt;- c(intervals, confidence_interval)    \n}\n\ninterval_df &lt;- data.frame(t(matrix(intervals,2,25)))  # Store intervals as data frame"
  },
  {
    "objectID": "posts/confidence_intervals/index.html#plotting-the-25-confidence-intervals",
    "href": "posts/confidence_intervals/index.html#plotting-the-25-confidence-intervals",
    "title": "Confidence Intervals",
    "section": "Plotting the 25 confidence intervals",
    "text": "Plotting the 25 confidence intervals\nThe red line shows the true mean. We see the first confidence interval not including the true mean. (1/25 = 0.04)"
  },
  {
    "objectID": "posts/confidence_intervals/index.html#summary",
    "href": "posts/confidence_intervals/index.html#summary",
    "title": "Confidence Intervals",
    "section": "Summary",
    "text": "Summary\n\nSimulating confidence interval shows that some will not include the true value."
  },
  {
    "objectID": "posts/Data Visualization with R/index.html",
    "href": "posts/Data Visualization with R/index.html",
    "title": "Themes for Data Visualization with R",
    "section": "",
    "text": "https://datavizs23.classes.andrewheiss.com/example/05-example.html\n\n\n\nCode\nlibrary(tidyverse)  # For ggplot, dplyr, and friends\nlibrary(gapminder)  # For gapminder data\nlibrary(scales)     # For nice axis labels\n\ngapminder_filtered &lt;- gapminder %&gt;% \n  filter(year &gt; 2000)\n\nbase_plot &lt;- ggplot(data = gapminder_filtered,\n                    mapping = aes(x = gdpPercap, y = lifeExp, \n                                  color = continent, size = pop)) +\n  geom_point() +\n  # Use dollars, and get rid of the cents part (i.e. $300 instead of $300.00)\n  scale_x_log10(labels = dollar_format(accuracy = 1)) +\n  # Format with commas\n  scale_size_continuous(labels = comma) +\n  # Use viridis\n  scale_color_viridis_d(option = \"plasma\", end = 0.9) +\n  labs(x = \"GDP per capita\", y = \"Life expectancy\",\n       color = \"Continent\", size = \"Population\",\n       title = \"Here's a cool title\",\n       subtitle = \"And here's a neat subtitle\",\n       caption = \"Source: The Gapminder Project\") +\n  facet_wrap(vars(year))\n\n\n\n\n\n\nCode\nmy_pretty_theme &lt;- theme_minimal(base_family = \"Roboto Condensed\", base_size = 12) +\n  theme(panel.grid.minor = element_blank(),\n        # Bold, bigger title\n        plot.title = element_text(face = \"bold\", size = rel(1.7)),\n        # Plain, slightly bigger subtitle that is grey\n        plot.subtitle = element_text(face = \"plain\", size = rel(1.3), color = \"grey70\"),\n        # Italic, smaller, grey caption that is left-aligned\n        plot.caption = element_text(face = \"italic\", size = rel(0.7), \n                                    color = \"grey70\", hjust = 0),\n        # Bold legend titles\n        legend.title = element_text(face = \"bold\"),\n        # Bold, slightly larger facet titles that are left-aligned for the sake of repetition\n        strip.text = element_text(face = \"bold\", size = rel(1.1), hjust = 0),\n        # Bold axis titles\n        axis.title = element_text(face = \"bold\"),\n        # Add some space above the x-axis title and make it left-aligned\n        axis.title.x = element_text(margin = margin(t = 10), hjust = 0),\n        # Add some space to the right of the y-axis title and make it top-aligned\n        axis.title.y = element_text(margin = margin(r = 10), hjust = 1),\n        # Add a light grey background to the facet titles, with no borders\n        strip.background = element_rect(fill = \"grey90\", color = NA),\n        # Add a thin grey border around all the plots to tie in the facet titles\n        panel.border = element_rect(color = \"grey90\", fill = NA))\n\n\n\n\nCode\nbase_plot + \n  my_pretty_theme\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(gapminder)\nlibrary(scales)\nlibrary(ggtext)\nlibrary(thematic)\n\ngapminder_filtered &lt;- gapminder %&gt;% \n  filter(year &gt; 2000)\n\ncolors &lt;- thematic::okabe_ito(5)\ntitle_text &lt;- glue::glue(\n  ' Life Expectancy per GDP for &lt;span style = \"color:{colors[1]}\"&gt;**Africa**&lt;/span&gt;, &lt;span style = \"color:{colors[2]}\"&gt;**America**&lt;/span&gt;, &lt;span style = \"color:{colors[3]}\"&gt;**Asia**&lt;/span&gt;, &lt;br&gt; &lt;span style = \"color:{colors[4]}\"&gt;**Europa**&lt;/span&gt; and &lt;span style = \"color:{colors[5]}\"&gt;**Oceania**&lt;/span&gt;'\n)\n\nplot_nice &lt;- ggplot(data = gapminder_filtered,\n                    mapping = aes(x = gdpPercap, y = lifeExp, \n                                  color = continent, size = pop)) +\n  geom_point() + \n  # Use dollars, and get rid of the cents part (i.e. $300 instead of $300.00)\n  #log10 scale = abstände nicht gleich(nicht-linear): x-1*x = x+1\n  scale_x_log10(labels = dollar_format(accuracy = 1)) +\n  # Format with commas (anstatt wissenschaftliche Notation für Population)\n  scale_size_continuous(labels = comma) +\n  # Use viridis color\n  scale_color_viridis_d(option = \"plasma\", end = 0.9) +\n  labs(x = \"GDP per capita\", y = \"Life expectancy\",\n       color = \"Continent\", size = \"Population\",\n       title = title_text,\n       subtitle = \"Visible Correlation between GDP (per capita) and Life expecanty\",\n       caption = \"Source: The Gapminder Project\") +\n  facet_wrap(vars(year))+\n\n  theme_minimal(base_family = 'Source Sans Pro', base_size = 12) + \n  #guide = none, legend wird ausgelassen\n  scale_color_manual(values = thematic::okabe_ito(5), guide = \"none\")+\n  #grauere Hintergrund verschwindet+\n  theme(panel.grid.minor = element_blank(),\n  # Plain, slightly bigger subtitle that is grey\n  plot.subtitle = element_text(face = \"plain\", size = rel(1.3), color = \"grey70\"),\n  # Italic, smaller, grey caption that is left-aligned\n  plot.caption = element_text(face = \"italic\", size = rel(0.7), \n                              color = \"grey70\", hjust = 0),\n  #Bold legend titles\n  legend.title = element_text(face = \"bold\"),\n  # Bold, slightly larger facet titles that are left-aligned for the sake of repetition\n  strip.text = element_text(face = \"bold\", size = rel(1.1), hjust = 0, color = \"white\"),\n  plot.title = ggtext::element_markdown(\n    color = 'grey20', family = 'Merriweather', size = rel(1.7),face = \"bold\"\n  ),\n  plot.title.position = \"plot\",\n  # Bold axis titles\n  axis.title = element_text(face = \"bold\"),\n  # Add some space above the x-axis title and make it left-aligned\n  axis.title.x = element_text(margin = margin(t = 10), hjust = 0),\n  # Add some space to the right of the y-axis title and make it top-aligned\n  axis.title.y = element_text(margin = margin(r = 10), hjust = 1, margin (0,4,0,0)),\n  # Add a light grey background to the facet titles, with no borders\n  strip.background = element_rect(fill = \"grey20\", color = NA),\n  # Add a thin grey border around all the plots to tie in the facet titles\n  panel.border = element_rect(color = \"grey20\", fill = NA),\n  legend.position = \"top\",\n  legend.justification = 1)\n\nplot_nice\n\n\n\n\n\n\n\n\n\n\n\n\nAnnotionations can be great to highlight information that gets lost otherwise. For example we see that South Africa increases its GDP per capita from 2002 to 2007 but life expectancy decreases by 4 years.\n\n\nCode\n#find african country with dip in lifeexp: South Africa(2002: x = 7711 ,y= 53.4; 2007: x = 9270, y = 49.3)\ngapminder_filtered |&gt; \n  filter(continent == \"Africa\" & between(gdpPercap,3000,10000) & between(lifeExp,48,55)) |&gt; arrange(desc(pop))\n\n\n# A tibble: 6 × 6\n  country           continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;             &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 South Africa      Africa     2002    53.4 44433622     7711.\n2 South Africa      Africa     2007    49.3 43997828     9270.\n3 Congo, Rep.       Africa     2002    53.0  3328795     3484.\n4 Namibia           Africa     2007    52.9  2055080     4811.\n5 Namibia           Africa     2002    51.5  1972153     4072.\n6 Equatorial Guinea Africa     2002    49.3   495627     7703.\n\n\nCode\n# creat df for different arrow coordinations\ndf_2002 &lt;-  gapminder_filtered %&gt;%\n  filter(year == 2002) %&gt;% \n  mutate(\n    start_x = 12000,\n    end_x = 9000,\n    start_y = 55,\n    end_y = 53.4\n  )\n\n# creat df for different arrow coordinations\ndf_2007 &lt;-  gapminder_filtered %&gt;%\n  filter(year == 2007) %&gt;% \n  mutate(\n    start_x = 12000,\n    end_x = 10000,\n    start_y = 55,\n    end_y = 49.3\n  )\n\n\n\nplot_nice_anno &lt;- plot_nice + geom_label(aes(x = 12000, y = 55, label = \"South Africa\"), \n                  hjust = 0, \n                  vjust = 0.5, \n                  lineheight = 0.8,\n                  colour = colors[1], \n                  fill = \"white\", \n                  label.size = NA, \n                  size = 5)   +\n  geom_curve(data = df_2002, aes(x = start_x, y = start_y, xend = end_x, yend = end_y), \n             colour = \"grey20\", \n             size=0.8, \n             curvature = -0.2,\n             arrow = arrow(length = unit(0.03, \"npc\"))) + \n  geom_curve(data = df_2007, aes(x = start_x, y = start_y, xend = end_x, yend = end_y), \n             colour = \"grey20\", \n             size=0.8, \n             curvature = -0.2,\n             arrow = arrow(length = unit(0.03, \"npc\")))\nplot_nice_anno\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_nice_anno +\n  scale_y_continuous(\n    breaks = c(4:7*10, seq(70, 80, by = 2))\n    ) + coord_cartesian(clip = \"off\")"
  },
  {
    "objectID": "posts/Data Visualization with R/index.html#create-and-save-theme-my_pretty_theme",
    "href": "posts/Data Visualization with R/index.html#create-and-save-theme-my_pretty_theme",
    "title": "Themes for Data Visualization with R",
    "section": "",
    "text": "Code\nmy_pretty_theme &lt;- theme_minimal(base_family = \"Roboto Condensed\", base_size = 12) +\n  theme(panel.grid.minor = element_blank(),\n        # Bold, bigger title\n        plot.title = element_text(face = \"bold\", size = rel(1.7)),\n        # Plain, slightly bigger subtitle that is grey\n        plot.subtitle = element_text(face = \"plain\", size = rel(1.3), color = \"grey70\"),\n        # Italic, smaller, grey caption that is left-aligned\n        plot.caption = element_text(face = \"italic\", size = rel(0.7), \n                                    color = \"grey70\", hjust = 0),\n        # Bold legend titles\n        legend.title = element_text(face = \"bold\"),\n        # Bold, slightly larger facet titles that are left-aligned for the sake of repetition\n        strip.text = element_text(face = \"bold\", size = rel(1.1), hjust = 0),\n        # Bold axis titles\n        axis.title = element_text(face = \"bold\"),\n        # Add some space above the x-axis title and make it left-aligned\n        axis.title.x = element_text(margin = margin(t = 10), hjust = 0),\n        # Add some space to the right of the y-axis title and make it top-aligned\n        axis.title.y = element_text(margin = margin(r = 10), hjust = 1),\n        # Add a light grey background to the facet titles, with no borders\n        strip.background = element_rect(fill = \"grey90\", color = NA),\n        # Add a thin grey border around all the plots to tie in the facet titles\n        panel.border = element_rect(color = \"grey90\", fill = NA))\n\n\n\n\nCode\nbase_plot + \n  my_pretty_theme"
  },
  {
    "objectID": "posts/Data Visualization with R/index.html#lets-make-this-plot-even-better-d",
    "href": "posts/Data Visualization with R/index.html#lets-make-this-plot-even-better-d",
    "title": "Themes for Data Visualization with R",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(gapminder)\nlibrary(scales)\nlibrary(ggtext)\nlibrary(thematic)\n\ngapminder_filtered &lt;- gapminder %&gt;% \n  filter(year &gt; 2000)\n\ncolors &lt;- thematic::okabe_ito(5)\ntitle_text &lt;- glue::glue(\n  ' Life Expectancy per GDP for &lt;span style = \"color:{colors[1]}\"&gt;**Africa**&lt;/span&gt;, &lt;span style = \"color:{colors[2]}\"&gt;**America**&lt;/span&gt;, &lt;span style = \"color:{colors[3]}\"&gt;**Asia**&lt;/span&gt;, &lt;br&gt; &lt;span style = \"color:{colors[4]}\"&gt;**Europa**&lt;/span&gt; and &lt;span style = \"color:{colors[5]}\"&gt;**Oceania**&lt;/span&gt;'\n)\n\nplot_nice &lt;- ggplot(data = gapminder_filtered,\n                    mapping = aes(x = gdpPercap, y = lifeExp, \n                                  color = continent, size = pop)) +\n  geom_point() + \n  # Use dollars, and get rid of the cents part (i.e. $300 instead of $300.00)\n  #log10 scale = abstände nicht gleich(nicht-linear): x-1*x = x+1\n  scale_x_log10(labels = dollar_format(accuracy = 1)) +\n  # Format with commas (anstatt wissenschaftliche Notation für Population)\n  scale_size_continuous(labels = comma) +\n  # Use viridis color\n  scale_color_viridis_d(option = \"plasma\", end = 0.9) +\n  labs(x = \"GDP per capita\", y = \"Life expectancy\",\n       color = \"Continent\", size = \"Population\",\n       title = title_text,\n       subtitle = \"Visible Correlation between GDP (per capita) and Life expecanty\",\n       caption = \"Source: The Gapminder Project\") +\n  facet_wrap(vars(year))+\n\n  theme_minimal(base_family = 'Source Sans Pro', base_size = 12) + \n  #guide = none, legend wird ausgelassen\n  scale_color_manual(values = thematic::okabe_ito(5), guide = \"none\")+\n  #grauere Hintergrund verschwindet+\n  theme(panel.grid.minor = element_blank(),\n  # Plain, slightly bigger subtitle that is grey\n  plot.subtitle = element_text(face = \"plain\", size = rel(1.3), color = \"grey70\"),\n  # Italic, smaller, grey caption that is left-aligned\n  plot.caption = element_text(face = \"italic\", size = rel(0.7), \n                              color = \"grey70\", hjust = 0),\n  #Bold legend titles\n  legend.title = element_text(face = \"bold\"),\n  # Bold, slightly larger facet titles that are left-aligned for the sake of repetition\n  strip.text = element_text(face = \"bold\", size = rel(1.1), hjust = 0, color = \"white\"),\n  plot.title = ggtext::element_markdown(\n    color = 'grey20', family = 'Merriweather', size = rel(1.7),face = \"bold\"\n  ),\n  plot.title.position = \"plot\",\n  # Bold axis titles\n  axis.title = element_text(face = \"bold\"),\n  # Add some space above the x-axis title and make it left-aligned\n  axis.title.x = element_text(margin = margin(t = 10), hjust = 0),\n  # Add some space to the right of the y-axis title and make it top-aligned\n  axis.title.y = element_text(margin = margin(r = 10), hjust = 1, margin (0,4,0,0)),\n  # Add a light grey background to the facet titles, with no borders\n  strip.background = element_rect(fill = \"grey20\", color = NA),\n  # Add a thin grey border around all the plots to tie in the facet titles\n  panel.border = element_rect(color = \"grey20\", fill = NA),\n  legend.position = \"top\",\n  legend.justification = 1)\n\nplot_nice"
  },
  {
    "objectID": "posts/Data Visualization with R/index.html#we-can-add-some-annotations",
    "href": "posts/Data Visualization with R/index.html#we-can-add-some-annotations",
    "title": "Themes for Data Visualization with R",
    "section": "",
    "text": "Annotionations can be great to highlight information that gets lost otherwise. For example we see that South Africa increases its GDP per capita from 2002 to 2007 but life expectancy decreases by 4 years.\n\n\nCode\n#find african country with dip in lifeexp: South Africa(2002: x = 7711 ,y= 53.4; 2007: x = 9270, y = 49.3)\ngapminder_filtered |&gt; \n  filter(continent == \"Africa\" & between(gdpPercap,3000,10000) & between(lifeExp,48,55)) |&gt; arrange(desc(pop))\n\n\n# A tibble: 6 × 6\n  country           continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;             &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 South Africa      Africa     2002    53.4 44433622     7711.\n2 South Africa      Africa     2007    49.3 43997828     9270.\n3 Congo, Rep.       Africa     2002    53.0  3328795     3484.\n4 Namibia           Africa     2007    52.9  2055080     4811.\n5 Namibia           Africa     2002    51.5  1972153     4072.\n6 Equatorial Guinea Africa     2002    49.3   495627     7703.\n\n\nCode\n# creat df for different arrow coordinations\ndf_2002 &lt;-  gapminder_filtered %&gt;%\n  filter(year == 2002) %&gt;% \n  mutate(\n    start_x = 12000,\n    end_x = 9000,\n    start_y = 55,\n    end_y = 53.4\n  )\n\n# creat df for different arrow coordinations\ndf_2007 &lt;-  gapminder_filtered %&gt;%\n  filter(year == 2007) %&gt;% \n  mutate(\n    start_x = 12000,\n    end_x = 10000,\n    start_y = 55,\n    end_y = 49.3\n  )\n\n\n\nplot_nice_anno &lt;- plot_nice + geom_label(aes(x = 12000, y = 55, label = \"South Africa\"), \n                  hjust = 0, \n                  vjust = 0.5, \n                  lineheight = 0.8,\n                  colour = colors[1], \n                  fill = \"white\", \n                  label.size = NA, \n                  size = 5)   +\n  geom_curve(data = df_2002, aes(x = start_x, y = start_y, xend = end_x, yend = end_y), \n             colour = \"grey20\", \n             size=0.8, \n             curvature = -0.2,\n             arrow = arrow(length = unit(0.03, \"npc\"))) + \n  geom_curve(data = df_2007, aes(x = start_x, y = start_y, xend = end_x, yend = end_y), \n             colour = \"grey20\", \n             size=0.8, \n             curvature = -0.2,\n             arrow = arrow(length = unit(0.03, \"npc\")))\nplot_nice_anno"
  },
  {
    "objectID": "posts/Data Visualization with R/index.html#lets-keep-playing-around",
    "href": "posts/Data Visualization with R/index.html#lets-keep-playing-around",
    "title": "Themes for Data Visualization with R",
    "section": "",
    "text": "Code\nplot_nice_anno +\n  scale_y_continuous(\n    breaks = c(4:7*10, seq(70, 80, by = 2))\n    ) + coord_cartesian(clip = \"off\")"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "CV",
    "section": "",
    "text": "Aktuell bin ich auf der Suche nach einem Praktikum im Bereich Marketing, Meinungsforschung oder User Experience.\n\nAusbildung\n\n\nUniversität Zürich\nM.Sc. in Psychologie (Praktikum noch offen)\n\n\nZürich\nVoraussichtlicher Abschluss Sommer 2024\n\n\n\nMasterarbeit: Investigating the Neuronal Basis of Learning Processes and Memory Formation in Children and Adolescents with Various Psychiatric Disorders\nNote: 6\n\n\n\nUniversität Zürich\nB.Sc. in Psychologie\n\n\nZürich 2018 - 2021\n\n\n\nBachelorarbeit: Comparison between Support Vector Machine and Convolutional Neural Network for Alzheimer’s Disease Classification\nNote: 6\n\n\n\nUniversität Zürich\nAssessmentjahr Wirtschaftsinformatik\n\n\nZürich\n2017 - 2018\n\n\n\nBerufserfahrungSkillsSprachkenntnisse\n\n\nKabinenreiniger (Flugzeuge) | Juli 2019 - November 2019 | Vebego Airport AG | Zürich\nMitarbeiter Wäscherei Bodensee | November 2015 - Juli 2016 | Wäscherei Bodensee AG | Münsterlingen (TG)\nKommissionierer | August 2015 - September 2015 | Lidl Schweiz | Weinfelden (TG)\n\n\nProgramming | R | Matlab | C | Python\nProgramme | RStudio | SPSS | LaTex | MS-Office | Abelton Live | Markdown\n\n\nEnglisch | Bilingual\nDeutsch | Bilingual"
  },
  {
    "objectID": "wiki.html",
    "href": "wiki.html",
    "title": "Introduction to My Personal Knowledge Base",
    "section": "",
    "text": "Welcome to my Personal Knowledge Base (PKB), a curated repository designed to organize, preserve, and access the wealth of information I encounter in my personal and professional life. This knowledge base serves as a digital extension of my mind, empowering me to think, create, and make decisions more effectively.",
    "crumbs": [
      "Wiki",
      "Personal Wiki"
    ]
  },
  {
    "objectID": "wiki.html#purpose-and-vision",
    "href": "wiki.html#purpose-and-vision",
    "title": "Introduction to My Personal Knowledge Base",
    "section": "Purpose and Vision",
    "text": "Purpose and Vision\nThe primary goal of this PKB is to centralize diverse information—insights from books, articles, conversations, projects, and experiences—in a structured and meaningful way. It reflects my evolving interests, skills, and understanding of the world. Whether exploring new ideas or revisiting past learnings, this knowledge base is a tool for lifelong learning and problem-solving.\n\n\n\n\n\n\nNote\n\n\n\nIntroduction text is written with ChatGPT but it generally reflects my use case for this site",
    "crumbs": [
      "Wiki",
      "Personal Wiki"
    ]
  },
  {
    "objectID": "wiki.html#why-a-quarto-website",
    "href": "wiki.html#why-a-quarto-website",
    "title": "Introduction to My Personal Knowledge Base",
    "section": "Why a Quarto Website?",
    "text": "Why a Quarto Website?\nBrowsing Youtube Video for tutorials of creating a personal knowledge will most likely lead you to a video of programs like Notion, Obsidian or Remnote. These Programs are great for personal knowledge bases. So why would I end up creating a personal website for such a purpose?\nMy reasons:\n\nOpen Source\nCan be easily viewed in any browser on any device\nPortfolio purposes\nQuarto documents have executable code\nSimultaneously working on projects and creating blog post\nLearning purposes (HTTP, CSS, SCSS)\nFeeling of “owning” the product\nRStudio’s integration with GitHub",
    "crumbs": [
      "Wiki",
      "Personal Wiki"
    ]
  },
  {
    "objectID": "wiki/LinAlg/norm.html",
    "href": "wiki/LinAlg/norm.html",
    "title": "Norm",
    "section": "",
    "text": "Used in matrix multiplication. For two vectors use 1D array in python\n\nimport numpy as np\nv = np.array([[1],[2],[3]])\ng = np.array([[3],[2],[1]])\n\nprint((v.T@g)[0,0])\n#or\nprint(np.dot(v.T,g)[0,0])\n\nDefinition:\n\\[\n\\mathbf{x} \\bullet \\mathbf{y} =\n\\begin{bmatrix}\nx_1 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n\\bullet\n\\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n= x_1 y_1 + \\cdots + x_n y_n = \\sum_{i=1}^n x_i y_i\n\\]\n\n\n\n\n\n\n\n\nExpand \\((x+y) \\bullet (x+y)\\) (can we express with norm?)\n\n\n\n\n\n\\((x+y) \\bullet (x+y) = x \\bullet x + 2(x\\bullet y)+y \\bullet y\\)\nExpress with norm if x and y are orthogonal because dot product is zero:\n\\(\\left\\|x\\right\\|^2 +\\left\\|y\\right\\|^2\\)\n\n\n\n\n\n\n\n\n\nExpand \\((x+y) \\bullet (x-y)\\) (can we express with norm?)\n\n\n\n\n\n\\((x+y) \\bullet (x-y) = x \\bullet x - y \\bullet y\\)\nWe can express with norm\n\\(\\left\\|x\\right\\|^2 - \\left\\|y\\right\\|^2\\)",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Orthonormal Basis"
    ]
  },
  {
    "objectID": "wiki/LinAlg/norm.html#exercise-6.1.",
    "href": "wiki/LinAlg/norm.html#exercise-6.1.",
    "title": "Norm",
    "section": "",
    "text": "Expand \\((x+y) \\bullet (x+y)\\) (can we express with norm?)\n\n\n\n\n\n\\((x+y) \\bullet (x+y) = x \\bullet x + 2(x\\bullet y)+y \\bullet y\\)\nExpress with norm if x and y are orthogonal because dot product is zero:\n\\(\\left\\|x\\right\\|^2 +\\left\\|y\\right\\|^2\\)\n\n\n\n\n\n\n\n\n\nExpand \\((x+y) \\bullet (x-y)\\) (can we express with norm?)\n\n\n\n\n\n\\((x+y) \\bullet (x-y) = x \\bullet x - y \\bullet y\\)\nWe can express with norm\n\\(\\left\\|x\\right\\|^2 - \\left\\|y\\right\\|^2\\)",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Orthonormal Basis"
    ]
  },
  {
    "objectID": "wiki/LinAlg/norm.html#orthogonal-projection",
    "href": "wiki/LinAlg/norm.html#orthogonal-projection",
    "title": "Norm",
    "section": "Orthogonal Projection",
    "text": "Orthogonal Projection\n Orthogonal Projection onto a line spanned by vector v.\n\\[\nprojection = \\frac{u \\bullet v}{\\left\\|v\\right\\|^2}*v\n\\] Find orthogonal vector w:\n\\[\nw = u-p\n\\] we can write w as \\(u = p + w\\) and yields a decomposition of u as sum of two vectors which are orthogonal.",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Orthonormal Basis"
    ]
  },
  {
    "objectID": "wiki/LinAlg/norm.html#orthonormal",
    "href": "wiki/LinAlg/norm.html#orthonormal",
    "title": "Norm",
    "section": "Orthonormal",
    "text": "Orthonormal\nA set is orthogonal if any two vectors in that set are orthogonal (i.e., linear independent: \\(dot \\space product = 0\\) or \\(u\\bullet v = 0\\)).\nA set is orthonormal if any two vectors in that set are orthogonal and each vector is normalized (i.e., \\(norm = 1\\) or \\(u\\bullet u = 1^2\\)).\n\nAn orthonormal set is also called orthonormal system (ONS for short).\nNormalize vectors by diving each components with the norm (\\(\\left\\|vector\\right\\|\\)) of vector.\nOrthonormal basis span a vector space:\n\neach vector \\(v\\) in that space can be formulated as linear transformation of those basis \\(b_1,b_2\\).\nFinding the coordinates \\(a_1,a_2\\) can be done by \\(a_1 = v\\bullet b_1\\) and \\(a_2=v \\bullet b_2\\).\n\n\n\\[\nv = (v\\bullet b_1)*b_1 + (v\\bullet b_2)*b_2\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\\((v\\bullet b_1)*b_1\\) is the orthogonal projection of v onto the line span(\\(b_i\\)).\n\n\n\nWhats the use of orthonormal basis (ONB)?\nU linear subspace of \\(\\mathbb{R}^2\\) and \\(b_1,...b_k\\) is orthonormal basis of U. Then, for each \\(vector \\in U\\) we have:\n\\[\nvector = a_1*b_1 +a_2*b_2 +...a_k*b_k\n\\] so we can decompose each vector into a linear combination of the orthonormal basis (think in geometrical terms where we scale and add x and y basis to display a vector). \\(a_1,..\\) are the coordinates with respect to the basis and we normally have to solve a linear system with gaussian elimination to find these coordinates (where coordinates are the unknown: \\(s_1,s_2,..\\)).\nBut with ONB we can directly calculate these coordinates with dot product:\n\\[\nvector = (v\\bullet b_1)b1 + ...\n\\]\nIn \\(\\mathbb{R}^2\\) with \\(î\\) and \\(\\hat j\\):\n\\[\n[5 \\space 6] = ([5 \\space 6]^t \\bullet [1 \\space 0]^t) * [1 \\space 0]^t+ ([5\\space  6]^t \\bullet [0 \\space 1]^t) * [0 \\space 1]^t\n\\]\nThe coordinate vector of \\([5 \\space 6]\\) is \\([5 \\space 6]\\) with respect to basis \\(i\\) and \\(j\\) but these are normally not the same.\n\n\n\n\n\n\nNote\n\n\n\nIf possible you can adjuste the vectors before taking dot product:\n\\[\nv \\bullet b_1 = \\begin{pmatrix} 2 \\\\ 5 \\\\ -7 \\\\ 3\\end{pmatrix} * \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1\\end{pmatrix} = \\frac{1}{2}(2*1 + 5*1 + (-7)*1 + 3 * 1). =\\frac{3}{2}\n\\] Instead of using:\n\\[\n\\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{2}\\end{pmatrix}\n\\]",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Orthonormal Basis"
    ]
  },
  {
    "objectID": "wiki/LinAlg/svd.html",
    "href": "wiki/LinAlg/svd.html",
    "title": "Singluar Value Decomposition",
    "section": "",
    "text": "Bringing it all together"
  },
  {
    "objectID": "wiki/LinAlg/sys.html",
    "href": "wiki/LinAlg/sys.html",
    "title": "Systems of Linear Equations",
    "section": "",
    "text": "flowchart TD\n    A[Linear Equation System] --&gt; B(Solving for unknown &lt;br/&gt;variables)\n    B --&gt; |Echelon Form| C[3 different types of EF&lt;br/&gt; consistent vs. inconsistent?]\n    C --&gt;|consistent if| D[Rank Criterion &lt;br/&gt; rkA = rk A,b &lt;br/&gt; for Ax=b]\n    D --&gt; K[Rank = #Pivots &lt;br/&gt; Nullity = #FREEparameter]\n    A --&gt; E[Inhomogeneous Linear System &lt;br/&gt; Ax=b]\n    E --&gt; F[Solution Set: Particular + &lt;br/&gt; Homogenous System]\n    A --&gt; G[Matrix Form &lt;br/&gt; Vector Form]\n    A --&gt; H[Square System]\n    H --&gt; |check with &lt;br/&gt; determinants| HA[invertible,regular &lt;br/&gt; non-singular &lt;br/&gt; if AX=I]\n    HA --&gt; |if no inverse|HB[singular &lt;br/&gt; non-invertible]\n    HA --&gt; |double-augmented &lt;br/&gt; matirx|HC[calculate with LSE]\n    HA --&gt; |Corollary|HE[if A and B invertible &lt;br/&gt; AB invertible]\n    HC --&gt; HD[if no pivot]\n    HD --&gt; HB\n    H --&gt; I[Unique Solution. &lt;br/&gt; Hom.S. only trivial sol]\n    I --&gt; J[except if any zero rows]\n    A --&gt; L[vectors linear independent?]\n    A --&gt; M[Linear transformation &lt;br/&gt; with non-standard basis &lt;br/&gt; and mapping rule L]\n    M --&gt; |to find:|MA[Coordinates of &lt;br/&gt;images vector L: v_j]\n    MA --&gt; |each image basis &lt;br/&gt;has own LSE|MAA[find linear combination of &lt;br/&gt; basis that form &lt;br/&gt; image of basis. &lt;br/&gt; Coefficients = coordinates]\n\n\n    \n\n\n\n\n\n\nFigure 1: A flow chart displaying for what to use linear system equations and\ngaussian elimination for solving the system. Only lecture relevant use cases are mentioned.",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Systems of Linear Equations"
    ]
  },
  {
    "objectID": "wiki/LinAlg/sys.html#overview",
    "href": "wiki/LinAlg/sys.html#overview",
    "title": "Systems of Linear Equations",
    "section": "",
    "text": "flowchart TD\n    A[Linear Equation System] --&gt; B(Solving for unknown &lt;br/&gt;variables)\n    B --&gt; |Echelon Form| C[3 different types of EF&lt;br/&gt; consistent vs. inconsistent?]\n    C --&gt;|consistent if| D[Rank Criterion &lt;br/&gt; rkA = rk A,b &lt;br/&gt; for Ax=b]\n    D --&gt; K[Rank = #Pivots &lt;br/&gt; Nullity = #FREEparameter]\n    A --&gt; E[Inhomogeneous Linear System &lt;br/&gt; Ax=b]\n    E --&gt; F[Solution Set: Particular + &lt;br/&gt; Homogenous System]\n    A --&gt; G[Matrix Form &lt;br/&gt; Vector Form]\n    A --&gt; H[Square System]\n    H --&gt; |check with &lt;br/&gt; determinants| HA[invertible,regular &lt;br/&gt; non-singular &lt;br/&gt; if AX=I]\n    HA --&gt; |if no inverse|HB[singular &lt;br/&gt; non-invertible]\n    HA --&gt; |double-augmented &lt;br/&gt; matirx|HC[calculate with LSE]\n    HA --&gt; |Corollary|HE[if A and B invertible &lt;br/&gt; AB invertible]\n    HC --&gt; HD[if no pivot]\n    HD --&gt; HB\n    H --&gt; I[Unique Solution. &lt;br/&gt; Hom.S. only trivial sol]\n    I --&gt; J[except if any zero rows]\n    A --&gt; L[vectors linear independent?]\n    A --&gt; M[Linear transformation &lt;br/&gt; with non-standard basis &lt;br/&gt; and mapping rule L]\n    M --&gt; |to find:|MA[Coordinates of &lt;br/&gt;images vector L: v_j]\n    MA --&gt; |each image basis &lt;br/&gt;has own LSE|MAA[find linear combination of &lt;br/&gt; basis that form &lt;br/&gt; image of basis. &lt;br/&gt; Coefficients = coordinates]\n\n\n    \n\n\n\n\n\n\nFigure 1: A flow chart displaying for what to use linear system equations and\ngaussian elimination for solving the system. Only lecture relevant use cases are mentioned.",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Systems of Linear Equations"
    ]
  },
  {
    "objectID": "wiki/LinAlg/sys.html#connection-to-linear-maps",
    "href": "wiki/LinAlg/sys.html#connection-to-linear-maps",
    "title": "Systems of Linear Equations",
    "section": "Connection to linear maps",
    "text": "Connection to linear maps\n\nLinear systems of equations can be described by matrices\nMatrices can be interpreted as linear maps",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Systems of Linear Equations"
    ]
  },
  {
    "objectID": "wiki/LinAlg/sys.html#three-types-of-echelon-form-ef",
    "href": "wiki/LinAlg/sys.html#three-types-of-echelon-form-ef",
    "title": "Systems of Linear Equations",
    "section": "Three Types of Echelon Form (EF)",
    "text": "Three Types of Echelon Form (EF)\n\nPivot element in every row and column.\n\nSquared form only possible when \\(r=c\\)\nLast pivot is isolated and can be substituted\nunique solution\n\nPivot in every row but not in every column.\n\nmore unknown than equations: \\(r&lt;c\\)\nfree parameter (the column without pivot)\ninfinitive solutions\n\nRows without pivot.\n\n\\(0=b\\)\n\nif \\(b \\not= 0\\) no solution since not consistent\nif \\(b = 0\\) no information. We can end up with form 1 or 2.",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Systems of Linear Equations"
    ]
  },
  {
    "objectID": "wiki/LinAlg/sys.html#general-solution",
    "href": "wiki/LinAlg/sys.html#general-solution",
    "title": "Systems of Linear Equations",
    "section": "General Solution",
    "text": "General Solution\nTypically you have some pivots missing (in EF) and some rows with 0 = 0.\n\\(\\begin{align} -x -y + 3z &= 3\\\\ -y + 4z&=6 \\\\ 0&=0\\end{align}\\)\nYou express the pivots columns with the missing pivot column (here z).\n\nFirst we use substitution to formulate the solution for the pivot columns (x and y):\n\n\\(\\begin{align} y &= -6+4z \\\\ x&= 3 -z \\\\ z&=z\\end{align}\\)\n\nNow lets determine the solution set:\n\n\\[\n\\begin{pmatrix} x\\\\y\\\\z \\end{pmatrix} =\n\\begin{pmatrix}3-z \\\\ -6+4z\\\\ z\\end{pmatrix} =\n\\begin{pmatrix}3\\\\-6\\\\0  \\end{pmatrix} +\n\\begin{pmatrix}-z \\\\ 4z\\\\1z\\end{pmatrix} =\n\\begin{pmatrix}3\\\\-6\\\\0  \\end{pmatrix} + z\n\\begin{pmatrix}-1\\\\4\\\\1 \\end{pmatrix}\n\\] The free parameters are used to describe the kernal of a matrix using homogenous solution.",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Systems of Linear Equations"
    ]
  },
  {
    "objectID": "wiki/LinAlg/sys.html#applied-problems-process-optimization",
    "href": "wiki/LinAlg/sys.html#applied-problems-process-optimization",
    "title": "Systems of Linear Equations",
    "section": "Applied Problems: Process optimization",
    "text": "Applied Problems: Process optimization\nMany problem can be formulated as system of linear equations:\n\n\n\n\n\n\nExample: Chemieproduktion\n\n\n\n\nChemikalien: A,B,C\nMaschinen: \\(M_1,M_2\\)\nMenge an A,B,C in Tonnen: \\(x_A,x_B,x_C\\)\nZeitbudget in Stunden: \\(M_1 = 65, M_2 = 110\\)\nVerhältnis in Stunden wie lang werden welche Chemikalien in den Maschinen verarbeitet werden\n\n\n\n\\[\\begin{pmatrix}\n&|& A&B&C&| &Zeitbudget/h \\\\\n\nM_1&|&2&1&1&|&65\\\\\n\nM_2&|&2&3&4&|&110\n\\end{pmatrix}\\]\n\nForm linear system of equations\n\\[\\begin{align*}\n2*x_A + 1*x_B + 1*x_C = 65\\\\\n2*x_A + 3*x_B + 4*x_C = 110\n\n\\end{align*}\\]\nLooking at the equation we make following observations:\n\nwe want to find \\(x_A,x_B,x_C\\) that fits the equations as a solution.\nwe add the terms because the individuel maschines can’t process the chemicals in parallel so we have to add the time\nthe coefficients of the unknown show a relationship of time per ton\n\\(\\frac{time}{tone}*time = time\\)\nwe have three unknown but only 2 equations: 1 free parameter -&gt; infinitive solutions\n\\(x_A,x_B,x_C\\) can only be positive since you can’t produce negative tones.\n\n\n\nSolving the system\nSolving the linear system will end up with a general solution:\n\\[\\begin{align*}\nx_A = \\frac{85}{4} + \\frac{1}{4}x_C\\\\\nx_B =\\frac{45}{2} - \\frac{3}{2}x_C \\\\\n\\end{align*}\\]\nand since the unknown can’t be negative we have to see for which range of \\(x_C\\) \\(x_A\\) and \\(x_B\\) are not negative:\n\\[\\begin{align*}\nx_B ≥ 0 \\\\\n\\frac{45}{2}-\\frac{3}{2}x_C ≥0\\\\\n15≥ x_C\\\\\n\\\\\n0≤x_C≤15\n\\end{align*}\\]\nwe can chose any tonne of \\(x_C\\) between 0 and 15.",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Systems of Linear Equations"
    ]
  },
  {
    "objectID": "wiki/LinAlg/sys.html#determinats",
    "href": "wiki/LinAlg/sys.html#determinats",
    "title": "Systems of Linear Equations",
    "section": "Determinats",
    "text": "Determinats\nFor 2x2 squared matrices:\n\\[\\begin{pmatrix}\na&b \\\\\nc&d\n\\end{pmatrix}\\]\nad - bc\nFor 3x3 squared matrices:\nRule of Sarrus\nOther n x n cases\nbring to upper triangular matrix form by row operations and switching:\n\nfor each switch sign of det changes!\nif one column with no pivot element: \\(det(0)\\)\nif upper triangular: determinant is product of main diagonal elements",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Systems of Linear Equations"
    ]
  },
  {
    "objectID": "wiki/LinAlg/ttr.html",
    "href": "wiki/LinAlg/ttr.html",
    "title": "Things to Remember",
    "section": "",
    "text": "Implicit representation of Linear system Solution?\n\n\n\n\n\nThis is an example of a ‘folded’ caution callout that can be expanded by the user. You can use collapse=\"true\" to collapse it by default or collapse=\"false\" to make a collapsible callout that is expanded by default.\n\n\n\n\n\n\n\n\n\nExplicit Representation of Linear system Solution?\n\n\n\n\n\nThis is an example of a ‘folded’ caution callout that can be expanded by the user. You can use collapse=\"true\" to collapse it by default or collapse=\"false\" to make a collapsible callout that is expanded by default.",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Things to remember"
    ]
  },
  {
    "objectID": "wiki/Statistics/ci.html#interpretation-of-ci",
    "href": "wiki/Statistics/ci.html#interpretation-of-ci",
    "title": "Confidence Intervals",
    "section": "Interpretation of CI",
    "text": "Interpretation of CI\nCI are notoriously being misinterpreted. Altough in my opinion these interpretation are not too harmful but rather ignore some uncertainty that still exists after calculating CI’s. Once again in statistics it’s a good idea to not fully really on one criteria for making a test decision. Furthermore, this shows that at the end of every analysis there must be a human that makes the final decision. I would argue that statistical inference is about reduction of uncertainty in light of a decision rather than fully eliminating uncertainty and making a decision itself (this task must be made by a human).",
    "crumbs": [
      "Wiki",
      "Statistics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "wiki/Statistics/ci.html#simulation",
    "href": "wiki/Statistics/ci.html#simulation",
    "title": "Confidence Intervals",
    "section": "Simulation",
    "text": "Simulation\nThis can be simulated using R.",
    "crumbs": [
      "Wiki",
      "Statistics",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "posts/variable_selection/index.html",
    "href": "posts/variable_selection/index.html",
    "title": "Accuracy of selected predictors per sample size",
    "section": "",
    "text": "With this exercise we want to inspect how lm function and step function vary when it comes to selection of correct variables depending on sample size.\nFoto von Jackson Jost auf Unsplash"
  },
  {
    "objectID": "posts/variable_selection/index.html#generate-data",
    "href": "posts/variable_selection/index.html#generate-data",
    "title": "Accuracy of selected predictors per sample size",
    "section": "Generate Data",
    "text": "Generate Data\n\n\nCode\n# Load packages \nlibrary(lme4)\nlibrary(dplyr)\nlibrary(ggplot2)\n\noptions(scipen=0)\nset.seed(101)\n\n# Variable and Dataset preparation\nN &lt;- c(25, 30, 40, 60, 100 ,500, 1000)\n# create a data frame to later add which variables were selected \npredi_selection_lm &lt;- as.data.frame(matrix(NA, nrow = 19, ncol = length(N)))\npredi_selection_step &lt;- as.data.frame(matrix(NA, nrow = 19, ncol = length(N)))\n\nnames(predi_selection_lm ) &lt;- c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")\nnames(predi_selection_step ) &lt;- c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")\n\n#counter to use as index for table predi_selection (loop)\na = 1\n\n#loop over different N (sample size)\nfor(i in N){\n  # Simulation dataset\n  x1 &lt;- runif(i, 0, 10)\n  x2 &lt;- runif(i, 0, 10)\n  x3 &lt;- runif(i, 0, 10)\n  x4 &lt;- runif(i, 0, 10)\n  x5 &lt;- runif(i, 0, 10)\n  x6 &lt;- runif(i, 0, 10)\n  x7 &lt;- runif(i, 0, 10)\n  x8 &lt;- runif(i, 0, 10)\n  x9 &lt;- runif(i, 0, 10)\n  x10 &lt;- runif(i, 0, 10)\n  x11 &lt;- runif(i, 0, 10)\n  x12 &lt;- runif(i, 0, 10)\n  x13 &lt;- runif(i, 0, 10)\n  x14 &lt;- runif(i, 0, 10)\n  x15 &lt;- runif(i, 0, 10)\n  x16 &lt;- runif(i, 0, 10)\n  x17 &lt;- runif(i, 0, 10)\n  x18 &lt;- runif(i, 0, 10)\n  x19 &lt;- runif(i, 0, 10)\n  err &lt;- runif(i, 0, 5)\n  \n  data &lt;- cbind(x1,x2,x3,x4,x5,x6, x7, x8, x9 ,x10, x11, x12, x13, x14, x15, x16, x17, x18, x19,  err)\n  data &lt;- as.data.frame(data)\n \n  # generate y values.\n  y &lt;- 30 + 0.4*x1 + 1*x2 + 2.3*x3 + 0.7*x4 + 0.2*x5  + 1*x6 + 2*x7 + 3*x8 + 1.5 *x9 + 0.5*x10 + err\n  \n  # add generated y values to data frame\n  data_2 &lt;- cbind(data, y)\n  \n  # Modelling with all variables as predi.\n  mod &lt;- lm(y~ x1 + x2 + x3 + x4 + x5 + x5 +x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18 + x19, data =  data_2)\n  summary(mod)\n  \n  ########################\n  #lets try step function#\n  ########################\n \n\n  # use step function and save final model as stepwise_model\n  stepwise_model &lt;-  step(mod)\n\n  \n  # Extract p-val: lm\n  p_val_lm &lt;- (summary(mod)$coefficients[,4])\n  p_val_lm &lt;- p_val_lm[2:20] #take out intercept\n  \n  # Extract p-val: step\n  p_val_step &lt;- summary(stepwise_model)$coefficients[,4]\n  p_val_step &lt;- p_val_step[2:20] #take out intercept\n  \n  # Store value: lm \n  predi_selection_lm[a] &lt;- ifelse(p_val_lm &lt; 0.05, 1, 0)\n  \n  #store values for step different, because summary output dosen't include all variable like in lm \n  p_val_step &lt;-  na.omit(ifelse(p_val_step &lt; 0.05, 1, 0))\n \n   #remove selected pred. without 0.05\n  p_val_step &lt;- p_val_step[! p_val_step %in% 0]\n  \n  # save the names of all selected coefficients as coes\n  coes &lt;- names(p_val_step)\n  \n  # create dummy var with all possible variables \n  dummy_var &lt;- c('x1','x2','x3','x4','x5','x6','x7','x8','x9','x10','x11','x12','x13','x14','x15','x16','x17','x18','x19')\n  \n  # which out of all possible variable have been selected? \n  selected_coe &lt;- ifelse(dummy_var %in% coes, 1,0)\n  \n  predi_selection_step[a] &lt;- selected_coe\n  \n  #increase counter by one\n  a = a + 1\n}\n\n# Summary accuracy predictors selection\ntrue_pred_dummy &lt;- c(1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0) #dummy code for the existing predictor \n\n# new data frame to compare true predi. to selected predi.\npredi_accuracy_lm &lt;- data.frame(matrix(NA, nrow = 19, ncol = length(N)))\nnames(predi_accuracy_lm) &lt;- c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")\n\npredi_accuracy_step &lt;- data.frame(matrix(NA, nrow = 19, ncol = length(N)))\nnames(predi_accuracy_step) &lt;- c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")\n\n#if all correct than coloumn all 1. \nfor (j in 1:length(N)){\n  predi_accuracy_lm[j] &lt;- ifelse(predi_selection_lm[j] == true_pred_dummy, 1,0)\n  predi_accuracy_step[j] &lt;- ifelse(predi_selection_step[j] == true_pred_dummy, 1,0)\n}"
  },
  {
    "objectID": "posts/variable_selection/index.html#now-lets-visualize-these-results",
    "href": "posts/variable_selection/index.html#now-lets-visualize-these-results",
    "title": "Accuracy of selected predictors per sample size",
    "section": "Now lets visualize these results",
    "text": "Now lets visualize these results\n\n\nCode\n# final data frame used for visualization\nsample_s &lt;- c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")\nsummary_accuracy &lt;- data.frame(cbind(sample_s,rep(NA, times = length(sample_s)),rep(NA, times = length(sample_s))))\nnames(summary_accuracy) &lt;- c(\"sample_s\", \"accuracy_lm\", \"accuracy_step\")\n\n\n\nfor( i in 1: nrow(summary_accuracy)){\n  summary_accuracy[i,2] &lt;-  mean(predi_accuracy_lm[,i])\n  summary_accuracy[i,3] &lt;-  mean(predi_accuracy_step[,i])\n\n}\nsummary_accuracy$accuracy_lm &lt;- as.numeric(summary_accuracy$accuracy_lm)\nsummary_accuracy$accuracy_step &lt;- as.numeric(summary_accuracy$accuracy_step)\nggplot(data = summary_accuracy,aes(x=factor(sample_s, \n                                           level = c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")) ,\n                                            y = accuracy_lm)) + \n  geom_bar(stat= \"identity\")+\n  xlab(\"sample size\")+\n  labs(title = \"Accuracy of selected predictor per sample size\")\n\n\n\n\n\n\n\n\n\nThis is the accuracy of the normal lm function with all predictors used.\n\n\nCode\nggplot(data = summary_accuracy,aes(x=factor(sample_s, \n                                           level = c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")) ,\n                                            y = accuracy_step)) + \n  geom_bar(stat= \"identity\")+\n  xlab(\"sample size\")+\n  labs(title = \"Accuracy of selected predictor per sample size\")\n\n\n\n\n\n\n\n\n\nThis is the accuracy of the step function default = backwards selection (starting with full model: all predictors) ."
  },
  {
    "objectID": "posts/variable_selection/index.html#summary",
    "href": "posts/variable_selection/index.html#summary",
    "title": "Accuracy of selected predictors per sample size",
    "section": "Summary:",
    "text": "Summary:\n\nespecially with low sample size N25 step function seems to perform less well than normal lm function\nwith increasing sample size both perform well and use all relevant predictors"
  },
  {
    "objectID": "posts/Simpsons_Paradox/index.html",
    "href": "posts/Simpsons_Paradox/index.html",
    "title": "Simpson’s Paradox",
    "section": "",
    "text": "Simpson’s Paradox bezeichnet ein statistische Phänomen, in dem die Daten eine hierarchisch angeordnete Struktur aufweisen. Die Analyse der Daten kann deshalb auf unterschiedliche Ebenen dieser Sturktur gemacht werden und Einfluss auf das Resultat haben. Teilweise kann wird dadurch die Interpretation der Forschungsfrage beeinflusst je nach untersuchten Ebenen. Simpson’s Paradox wird oft verwendet um die Vorteile von Linear Mixed Effects Model im Vergleich zur einfacher linearen Regression darzustellen."
  },
  {
    "objectID": "posts/Simpsons_Paradox/index.html#simpsons-paradox",
    "href": "posts/Simpsons_Paradox/index.html#simpsons-paradox",
    "title": "Simpson’s Paradox",
    "section": "",
    "text": "Simpson’s Paradox bezeichnet ein statistische Phänomen, in dem die Daten eine hierarchisch angeordnete Struktur aufweisen. Die Analyse der Daten kann deshalb auf unterschiedliche Ebenen dieser Sturktur gemacht werden und Einfluss auf das Resultat haben. Teilweise kann wird dadurch die Interpretation der Forschungsfrage beeinflusst je nach untersuchten Ebenen. Simpson’s Paradox wird oft verwendet um die Vorteile von Linear Mixed Effects Model im Vergleich zur einfacher linearen Regression darzustellen."
  },
  {
    "objectID": "posts/Simpsons_Paradox/index.html#hierarchische-datenstruktur",
    "href": "posts/Simpsons_Paradox/index.html#hierarchische-datenstruktur",
    "title": "Simpson’s Paradox",
    "section": "Hierarchische Datenstruktur",
    "text": "Hierarchische Datenstruktur\n\nIm Querschnitt\nBeispiele sind:\n\nSchüler genested in Schulen (Zwei Ebenen)\nSchüler genested in Schulen genested in Kantonen (Drei Ebenen)\nMitarbeiter genested in Unternehmen (Zwei Ebenen)\n\n\n\nIm Längschnitt\nBeispiele sind:\n\nPrüfungsnoten genested in Schüler (Zwei Ebenen)\nKundenzufriedenheit genested Person genested in Altersgruppen (Drei Ebenen)"
  },
  {
    "objectID": "posts/Simpsons_Paradox/index.html#beispiel-pinguine",
    "href": "posts/Simpsons_Paradox/index.html#beispiel-pinguine",
    "title": "Simpson’s Paradox",
    "section": "Beispiel Pinguine",
    "text": "Beispiel Pinguine\nDaten (Gorman (2014)):\n\npalmerpenguins\n\n344 Pinguine\n3 Arten (Adéline, chinstrap und gentoo)\n\n\nDie Daten wurden von Dr Allison Horst, Alison Hill und Kristen Gorman zu einem R Packet palmerpenguins verarbeitet. Das folgende Projekt basiert auf dem Turtorial von Silvia Canelon.\nZuerst, R Packete installieren\n\n\nCode\ninstall.packages(\"palmerpenguins\") #Data\ninstall.packages(\"tibble\") #Data handling\ninstall.packges(\"ggplot2\") #Plotting\ninstall.packages(\"dplyr\")\n\n\nPackete laden\n\n\nCode\nlibrary(palmerpenguins)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(dplyr)\npenguins &lt;- palmerpenguins::penguins\nhead(penguins)\n\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nDaten explorieren\n\n\nCode\nggplot(data = penguins,\n       aes(x = sex, y = body_mass_g))+\n  geom_boxplot(aes(fill = species))\n\n\n\n\n\n\n\n\n\nDie Boxplotte deuten darauf hin, dass bei Gentoo ein geschlechtsspezifischer Unterschiedlich im Body Mass zu sehen ist, wobei dies möglicherweise nicht der Fall ist bei Adelie und Chinstrap.\n\n\nCode\npenguins %&gt;%\n  select(species, sex, body_mass_g) %&gt;%\n  arrange(desc(body_mass_g))\n\n\n# A tibble: 344 × 3\n   species sex   body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;       &lt;int&gt;\n 1 Gentoo  male         6300\n 2 Gentoo  male         6050\n 3 Gentoo  male         6000\n 4 Gentoo  male         6000\n 5 Gentoo  male         5950\n 6 Gentoo  male         5950\n 7 Gentoo  male         5850\n 8 Gentoo  male         5850\n 9 Gentoo  male         5850\n10 Gentoo  male         5800\n# ℹ 334 more rows\n\n\n\n\nCode\npenguins %&gt;%\n  select(species, sex, body_mass_g) %&gt;%\n  group_by(species,sex) %&gt;%\n  summarize(mean = mean(body_mass_g))\n\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex     mean\n  &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt;\n1 Adelie    female 3369.\n2 Adelie    male   4043.\n3 Adelie    &lt;NA&gt;     NA \n4 Chinstrap female 3527.\n5 Chinstrap male   3939.\n6 Gentoo    female 4680.\n7 Gentoo    male   5485.\n8 Gentoo    &lt;NA&gt;     NA \n\n\n\n\nCode\npenguins %&gt;%\n  group_by(species) %&gt;%\n  mutate(n_species = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(species,sex,n_species) %&gt;%\n  summarize(n=n())\n\n\n`summarise()` has grouped output by 'species', 'sex'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   species, sex [8]\n  species   sex    n_species     n\n  &lt;fct&gt;     &lt;fct&gt;      &lt;int&gt; &lt;int&gt;\n1 Adelie    female       152    73\n2 Adelie    male         152    73\n3 Adelie    &lt;NA&gt;         152     6\n4 Chinstrap female        68    34\n5 Chinstrap male          68    34\n6 Gentoo    female       124    58\n7 Gentoo    male         124    61\n8 Gentoo    &lt;NA&gt;         124     5\n\n\n\n\nCode\npenguins %&gt;%\n  count(species, sex) %&gt;%\n  add_count(species, wt = n,\n            name = \"n_species\")\n\n\n# A tibble: 8 × 4\n  species   sex        n n_species\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;     &lt;int&gt;\n1 Adelie    female    73       152\n2 Adelie    male      73       152\n3 Adelie    &lt;NA&gt;       6       152\n4 Chinstrap female    34        68\n5 Chinstrap male      34        68\n6 Gentoo    female    58       124\n7 Gentoo    male      61       124\n8 Gentoo    &lt;NA&gt;       5       124\n\n\n\n\nCode\npenguins %&gt;%\n  count(species, sex) %&gt;%\n  add_count(species, wt = n,\n            name = \"n_species\") %&gt;%\n  mutate(prop = n/n_species*100)\n\n\n# A tibble: 8 × 5\n  species   sex        n n_species  prop\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Adelie    female    73       152 48.0 \n2 Adelie    male      73       152 48.0 \n3 Adelie    &lt;NA&gt;       6       152  3.95\n4 Chinstrap female    34        68 50   \n5 Chinstrap male      34        68 50   \n6 Gentoo    female    58       124 46.8 \n7 Gentoo    male      61       124 49.2 \n8 Gentoo    &lt;NA&gt;       5       124  4.03\n\n\n\n\nCode\npenguins %&gt;%\n  count(species, sex) %&gt;%\n  add_count(species, wt = n,\n            name = \"n_species\") %&gt;%\n  mutate(prop = n/n_species*100) %&gt;%\n  filter(species == \"Chinstrap\")\n\n\n# A tibble: 2 × 5\n  species   sex        n n_species  prop\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Chinstrap female    34        68    50\n2 Chinstrap male      34        68    50\n\n\n\n\nCode\npenguins_new &lt;-\n  penguins %&gt;%\n  mutate(year_factor = factor(year, levels = unique(year)))\npenguins_new\n\n\n# A tibble: 344 × 9\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 3 more variables: sex &lt;fct&gt;, year &lt;int&gt;, year_factor &lt;fct&gt;\n\n\n\n\nVisualize Simpson’s Paradox\n\n\nCode\nggplot(data = penguins,\n       mapping = aes(x = bill_length_mm, y = bill_depth_mm))+\n  geom_point() +\n  geom_smooth(method='lm', formula = y ~x)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_simps &lt;- penguins %&gt;%\n  ggplot(mapping = aes(x = bill_length_mm, y = bill_depth_mm, group = species, col = species)) + \n  geom_point() +\n  geom_smooth(method = 'lm', formula = y ~x)\nplot_simps"
  },
  {
    "objectID": "posts/Simpsons_Paradox/index.html#interpretation",
    "href": "posts/Simpsons_Paradox/index.html#interpretation",
    "title": "Simpson’s Paradox",
    "section": "Interpretation",
    "text": "Interpretation\nWir sehen auf den letzten beiden Plots, dass wenn wir die Art der Penguine ingnorieren wir falsche Schlüsse ziehen über den Zusammenhang zwischen Schnabellänge und -breite.\nMore information on mixed-effects models and simpson’s paradox (see Hox, Moerbeek, and Van de Schoot 2017)"
  },
  {
    "objectID": "posts/Simpsons_Paradox/index.html#lets-make-a-nice-plot",
    "href": "posts/Simpsons_Paradox/index.html#lets-make-a-nice-plot",
    "title": "Simpson’s Paradox",
    "section": "Lets make a nice plot",
    "text": "Lets make a nice plot\n\n\nCode\nlibrary(thematic)\nlibrary(ggtext)\ncolors &lt;- thematic::okabe_ito(3)\ntitle_text &lt;- glue::glue(\n  'Bill Depth per Bill Length in mm for &lt;span style = \"color:{colors[1]}\"&gt;**Adelie**&lt;/span&gt;, &lt;span style = \"color:{colors[2]}\"&gt;**Chinstrap**&lt;/span&gt;, &lt;span style = \"color:{colors[3]}\"&gt;**Gentoo**&lt;/span&gt;')\n\nplot_simps &lt;- plot_simps  + \n  labs(x = \"Bill Lenght [mm]\", y = \"Bill Depth [mm]\",\n       color = \"Species\",\n       title = title_text,\n       caption = \"Source: Penguins Data\") + \n  scale_y_continuous(\n    breaks = 13:22\n  )+ \n  scale_x_continuous(\n    breaks = seq(30,60,by = 5)\n    )+ \n  #guide = none, legend wird ausgelassen\n  scale_color_manual(values = thematic::okabe_ito(3), guide = \"none\") +\n  theme_minimal(base_family = 'Source Sans Pro', base_size = 12)+\n  theme(plot.title = ggtext::element_markdown(color = 'grey20', size = rel(1.7),face= \"bold\"),\n        plot.title.position = \"plot\",\n  # Bold axis titles\n  axis.title = element_text(face = \"bold\"),\n  # Add some space above the x-axis title and make it left-aligned\n  axis.title.x = element_text(margin = margin(t = 10), hjust = 0),\n  # Add some space to the right of the y-axis title and make it top-aligned\n  axis.title.y = element_text(margin = margin(r = 10), hjust = 1, margin (0,4,0,0)),) \nplot_simps"
  },
  {
    "objectID": "posts/Simpsons_Paradox/index.html#save-plot",
    "href": "posts/Simpsons_Paradox/index.html#save-plot",
    "title": "Simpson’s Paradox",
    "section": "Save plot",
    "text": "Save plot\n\n\nCode\nlibrary(here)\n\n\nhere() starts at /Users/oliverzingg/Portfolio\n\n\nCode\np &lt;- here(\"my_plot.png\")\n#ggsave(p,plot_simps, width=45, height=30, units = \"cm\", dpi=500)"
  },
  {
    "objectID": "wiki/Statistics/anova.html",
    "href": "wiki/Statistics/anova.html",
    "title": "Anova - Analysis of Variance",
    "section": "",
    "text": "F-test",
    "crumbs": [
      "Wiki",
      "Statistics",
      "Anova"
    ]
  },
  {
    "objectID": "wiki/Statistics/Glossary.html",
    "href": "wiki/Statistics/Glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "test",
    "crumbs": [
      "Wiki",
      "Statistics",
      "Glossary"
    ]
  },
  {
    "objectID": "wiki/Statistics/Glossary.html#f-test",
    "href": "wiki/Statistics/Glossary.html#f-test",
    "title": "Glossary",
    "section": "",
    "text": "test",
    "crumbs": [
      "Wiki",
      "Statistics",
      "Glossary"
    ]
  },
  {
    "objectID": "wiki/LinAlg/eigen.html",
    "href": "wiki/LinAlg/eigen.html",
    "title": "Eigenvalues and Eigenvectors",
    "section": "",
    "text": "Some vectors remain on their span after transformation. Because of linear transformation any vector on that same span remains on that span. These vectors are the eigenvalues with corresponding eigenvalues (scalar).\nSince linear transformation of an eigenvector \\(\\overline v\\) just scales it, it follows:\n\\[\nA \\overline v = \\lambda \\overline v\n= (\\lambda I) \\overline v\n= (A-\\lambda I)\\overline v = \\overline 0\n\\] This expression is always true for \\(\\overline v = 0\\) but this is not allowed. So the only way a multiplication of a matrix with a non-zero vector to be zero is with \\(det(A-\\lambda I)=0\\). Transformation associated with that matrix squishes space into a lower dimension? (chapter 5/6)\nExmaple\n\\[\n\\begin{pmatrix}\n3-\\lambda & 1 \\\\\n0 & 2-\\lambda\n\\end{pmatrix}\n= (3-\\lambda)(2-\\lambda) = 0\n\\] After finding eigenvalue, we need eigenvectors associated with eigenvalue (\\(\\lambda = 2 \\space or \\space \\lambda = 3\\))",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Eigenvalues and -vectors"
    ]
  },
  {
    "objectID": "wiki/LinAlg/eigen.html#diagonalizable",
    "href": "wiki/LinAlg/eigen.html#diagonalizable",
    "title": "Eigenvalues and Eigenvectors",
    "section": "Diagonalizable",
    "text": "Diagonalizable\nDiagonal matrices have nice properties so it’s convenient to describe a linear mapping by a diagonal matrix.\n\\[\n\\begin{pmatrix}\na & 0 \\\\\n0 & d\n\\end{pmatrix}\n\\]\nDesribes the scaling by factor a in \\(x_1\\) direction and by factor d in \\(x_2\\)-direction. If a or d is negative then this reflects a reflection.\n\n\n\n\n\n\nNote\n\n\n\nWe want to find new basis (not zero-vector) for which the new linear mapping matrix D is diagonal. Thus reflecting a simple scalar multiple of the new basis. These new basis are call eigenvectors and the scalars (can be zero) are called eigenvalues of the original matrix L.\nOnly for (n x n) matrices: domain and codomain coincide\n\n\nLinear Mapping that use rotations don’t have eigenvectors",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Eigenvalues and -vectors"
    ]
  },
  {
    "objectID": "wiki/LinAlg/eigen.html#eigenspace",
    "href": "wiki/LinAlg/eigen.html#eigenspace",
    "title": "Eigenvalues and Eigenvectors",
    "section": "Eigenspace",
    "text": "Eigenspace\nEigenspace L is a subspace of \\(R^n\\) associated to the eigenvalue. Since an eigenvalue has infinite eigenvectors associated with the same eigenvalue.",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Eigenvalues and -vectors"
    ]
  },
  {
    "objectID": "wiki/LinAlg/eigen.html#determination-of-eigenvalues-and-eigenvectors",
    "href": "wiki/LinAlg/eigen.html#determination-of-eigenvalues-and-eigenvectors",
    "title": "Eigenvalues and Eigenvectors",
    "section": "Determination of Eigenvalues and Eigenvectors",
    "text": "Determination of Eigenvalues and Eigenvectors\n\\[\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nThe eigenvalues of a square matrix A are the zeros of the characteristic polynomial \\(p_A(\\lambda)\\). (n x n)-matrix has at most n eigenvalues",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Eigenvalues and -vectors"
    ]
  },
  {
    "objectID": "wiki/LinAlg/eigen.html#eigendecompostion",
    "href": "wiki/LinAlg/eigen.html#eigendecompostion",
    "title": "Eigenvalues and Eigenvectors",
    "section": "Eigendecompostion",
    "text": "Eigendecompostion\nSometimes it is very useful to write a matrix 𝐴 as a product of other matrices, say\n\\(A=PDP^{-1}\\)\nsuch a matrix is called a matrix dececomposition of matrix A. This holds for a linear map described by A and D w.r.t different bases. Goal is to find basis for which matrix D is a diagonal matrix. The matrix \\(P\\) is called transformation matrix. The process of obtaining \\(P\\) and \\(D\\) is called diagonalization.\n\n\\(D\\) is composed of eigenvalues of \\(A\\)\n\\(P\\) is composed of eigenvectors of \\(A\\) and are not unique",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Eigenvalues and -vectors"
    ]
  },
  {
    "objectID": "wiki/LinAlg/basis.html",
    "href": "wiki/LinAlg/basis.html",
    "title": "Basis, Span, Dimensions",
    "section": "",
    "text": "flowchart TD\n    A(Basis, Span &lt;br/&gt; Dimensions) --&gt; B[Matrix column span]\n    B--&gt;|echelon form: &lt;br/&gt;columns with pivots|C[ linearly independent. &lt;br/&gt;Original columns basis&lt;br/&gt;span the column space]\n    \n\n\n\n\n\n\n\nFigure 1: A flow chart of concepts in Basis and Span",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Basis"
    ]
  },
  {
    "objectID": "wiki/LinAlg/basis.html#overview",
    "href": "wiki/LinAlg/basis.html#overview",
    "title": "Basis, Span, Dimensions",
    "section": "",
    "text": "flowchart TD\n    A(Basis, Span &lt;br/&gt; Dimensions) --&gt; B[Matrix column span]\n    B--&gt;|echelon form: &lt;br/&gt;columns with pivots|C[ linearly independent. &lt;br/&gt;Original columns basis&lt;br/&gt;span the column space]\n    \n\n\n\n\n\n\n\nFigure 1: A flow chart of concepts in Basis and Span",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Basis"
    ]
  },
  {
    "objectID": "wiki/LinAlg/basis.html#span",
    "href": "wiki/LinAlg/basis.html#span",
    "title": "Basis, Span, Dimensions",
    "section": "Span",
    "text": "Span\nThe span of two vectors \\(\\overline v\\) and \\(\\overline w\\) is the set of all linear combinations with \\(a*\\overline v + b * \\overline w\\) with \\(a,b \\in R\\). So two operations are used: vector scaling and vector addition.\n\n\n\n\n\n\nGiven two vectors: When can’t you reach all vectors in a plane?\n\n\n\n\n\nIf the vectors are colinear/parallel or consists of points",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Basis"
    ]
  },
  {
    "objectID": "wiki/LinAlg/basis.html#basis",
    "href": "wiki/LinAlg/basis.html#basis",
    "title": "Basis, Span, Dimensions",
    "section": "Basis",
    "text": "Basis\n\n\n\n\n\n\nNote\n\n\n\nThe set of all linear independent vectors that span a vector space. There are an infinite amount of different basis for a vector space but the number is fixed to the dimension.",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Basis"
    ]
  },
  {
    "objectID": "wiki/LinAlg/basis.html#when-are-vectors-linear-independent",
    "href": "wiki/LinAlg/basis.html#when-are-vectors-linear-independent",
    "title": "Basis, Span, Dimensions",
    "section": "When are vectors linear independent?",
    "text": "When are vectors linear independent?\n\nExample\n\\(s_1*v_1 + ...+ s_3*v_3 = 0\\)\n\\[\\begin{pmatrix}\n\n-2 & 1 & 4 & |& 0 \\\\\n6 & 1 & -12 &|& 0 \\\\\n1 & 1 & -2 &| & 0\n\n\\end{pmatrix}\\]\nIf we only have trivial solution to \\(s_1*v_1 + ...+ s_3*v_3 = 0\\) than the vectors are linearly independent. So we solve with gaussian elimination and get the following echelon form:\n\\[\\begin{pmatrix}\n-2 & 1 & 4 & |& 0 \\\\\n0 & 4 & 0 & | & 0 \\\\\n0 & 0 & 0 & | & 0\n\\end{pmatrix}\\]\n\nwe se that we have a free parameter t in the \\(s_3\\) because there is no pivot.\n\\(s_3 = t\\)\n\\(s_2 = 0\\)\n\\(s_1 = 2t\\)",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Basis"
    ]
  },
  {
    "objectID": "wiki/LinAlg/linear-maps.html",
    "href": "wiki/LinAlg/linear-maps.html",
    "title": "Linear Maps",
    "section": "",
    "text": "flowchart TD\n    A(Linear Maps &lt;br/&gt; linear transformation) --&gt; D[Domain,Codomain]\n    \n    A --&gt; B[Mapping Rule: L]\n    B --&gt; BA[Linear mapping?]\n    BA --&gt; |check with|BAA[Linearity rules] \n    BA --&gt; |quick check|BAB[Zero vector &lt;br/&gt;L0=0]\n    \n    B --&gt; BB[Matrix w.r.t. &lt;br/&gt; Standard Basis]\n    B --&gt; BC[Matrix w.r.t. &lt;br/&gt;Non-Standard Basis]\n    BC --&gt; |how to &lt;br/&gt;calculate map S &lt;br/&gt; w.r.t. basis:|BCA[Image of Basis &lt;br/&gt; using L]\n    BCA --&gt; |use decomposition &lt;br/&gt; or LSE|BCB[to find coordinates. &lt;br/&gt; coefficients = coordinate. &lt;br/&gt; linear combination]\n    BCB --&gt; |use coordinates &lt;br/&gt; vectors for|BCBA[Map S w.r.t &lt;br/&gt; new basis and insight &lt;br/&gt;about linear transformation]\n    \n    B --&gt; BD[Invertible Mapping]\n    BD --&gt; |only possible &lt;br/&gt; if same dimension|BDA[inverse map of L: L^-1 &lt;br/&gt; = is unique and linear]\n    BDA --&gt; |every reflection &lt;br/&gt;is invertibale|BDAA[L = L &lt;br/&gt; self-inverse]\n    BDA --&gt; |if 2 x 2 matrix|BDAB[Use formular below]\n\n    A --&gt; C[Affine Linear Maps]\n    C --&gt; |Linear + shift|CA[Linear map + &lt;br/&gt;displacement vector w &lt;br/&gt; every linear map is &lt;br/&gt;affine map with w = 0 ]\n    CA --&gt; |Image of zero vector|CB[displacement vector]\n    CA --&gt; |used for example|CC[rotation around point other than origin]\n\n\n\n\n\n    \n\n\n\n\n\n\nFigure 1: A flow chart of concepts in linear maps\n\n\n\n\n\n\n\nPress on triangle to show Code.\n\n\nCode\nimport numpy as np\nimport scipy.linalg as la\n\n\n\nA = np.array([\n  [-3,2,2],\n  [-3,1,3],\n  [-1,2,0]\n  ])\n\n#-----------Example first vector-------#\nv_1 = np.array([\n  [2],\n  [1],\n  [2]\n])\n\n#---------Calculate Image of basis v_1---#\nr_1 = A@v_1\n\n#--------After repeating for all basis--#\nS = np.hstack([v_1, v_2, v_3])\n#' This gives us the coefficient matrix that is used as \n#' LSE to calculate coordinates\n\n#-------Solve each system----- #\n\n#LSE of image of v_1\na_1 = la.solve(S, r_1)\n\n\n#--------Final Matrix------#\nB = np.hstack([a_1, a_2, a_3])\n\n\n\n\n\nWhat matrix size do we need if we want to map \\(\\mathbb{R}^2\\rightarrow \\mathbb{R}^3\\)?\n\nwe would need a (3 x 2) matrix\n\n\n\n\n\\(A^{-1} = \\frac{1}{ad-bc}*\\begin{pmatrix}d & -b \\\\ -c & a\\end{pmatrix}\\)\nThe denominator is the determinant of A.",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Linear Maps"
    ]
  },
  {
    "objectID": "wiki/LinAlg/linear-maps.html#overview",
    "href": "wiki/LinAlg/linear-maps.html#overview",
    "title": "Linear Maps",
    "section": "",
    "text": "flowchart TD\n    A(Linear Maps &lt;br/&gt; linear transformation) --&gt; D[Domain,Codomain]\n    \n    A --&gt; B[Mapping Rule: L]\n    B --&gt; BA[Linear mapping?]\n    BA --&gt; |check with|BAA[Linearity rules] \n    BA --&gt; |quick check|BAB[Zero vector &lt;br/&gt;L0=0]\n    \n    B --&gt; BB[Matrix w.r.t. &lt;br/&gt; Standard Basis]\n    B --&gt; BC[Matrix w.r.t. &lt;br/&gt;Non-Standard Basis]\n    BC --&gt; |how to &lt;br/&gt;calculate map S &lt;br/&gt; w.r.t. basis:|BCA[Image of Basis &lt;br/&gt; using L]\n    BCA --&gt; |use decomposition &lt;br/&gt; or LSE|BCB[to find coordinates. &lt;br/&gt; coefficients = coordinate. &lt;br/&gt; linear combination]\n    BCB --&gt; |use coordinates &lt;br/&gt; vectors for|BCBA[Map S w.r.t &lt;br/&gt; new basis and insight &lt;br/&gt;about linear transformation]\n    \n    B --&gt; BD[Invertible Mapping]\n    BD --&gt; |only possible &lt;br/&gt; if same dimension|BDA[inverse map of L: L^-1 &lt;br/&gt; = is unique and linear]\n    BDA --&gt; |every reflection &lt;br/&gt;is invertibale|BDAA[L = L &lt;br/&gt; self-inverse]\n    BDA --&gt; |if 2 x 2 matrix|BDAB[Use formular below]\n\n    A --&gt; C[Affine Linear Maps]\n    C --&gt; |Linear + shift|CA[Linear map + &lt;br/&gt;displacement vector w &lt;br/&gt; every linear map is &lt;br/&gt;affine map with w = 0 ]\n    CA --&gt; |Image of zero vector|CB[displacement vector]\n    CA --&gt; |used for example|CC[rotation around point other than origin]\n\n\n\n\n\n    \n\n\n\n\n\n\nFigure 1: A flow chart of concepts in linear maps\n\n\n\n\n\n\n\nPress on triangle to show Code.\n\n\nCode\nimport numpy as np\nimport scipy.linalg as la\n\n\n\nA = np.array([\n  [-3,2,2],\n  [-3,1,3],\n  [-1,2,0]\n  ])\n\n#-----------Example first vector-------#\nv_1 = np.array([\n  [2],\n  [1],\n  [2]\n])\n\n#---------Calculate Image of basis v_1---#\nr_1 = A@v_1\n\n#--------After repeating for all basis--#\nS = np.hstack([v_1, v_2, v_3])\n#' This gives us the coefficient matrix that is used as \n#' LSE to calculate coordinates\n\n#-------Solve each system----- #\n\n#LSE of image of v_1\na_1 = la.solve(S, r_1)\n\n\n#--------Final Matrix------#\nB = np.hstack([a_1, a_2, a_3])\n\n\n\n\n\nWhat matrix size do we need if we want to map \\(\\mathbb{R}^2\\rightarrow \\mathbb{R}^3\\)?\n\nwe would need a (3 x 2) matrix\n\n\n\n\n\\(A^{-1} = \\frac{1}{ad-bc}*\\begin{pmatrix}d & -b \\\\ -c & a\\end{pmatrix}\\)\nThe denominator is the determinant of A.",
    "crumbs": [
      "Wiki",
      "Linear Algebra",
      "Linear Maps"
    ]
  },
  {
    "objectID": "wiki/CS/terminal.html",
    "href": "wiki/CS/terminal.html",
    "title": "Convert Excel File into textfile",
    "section": "",
    "text": "Convert Excel File into textfile\nWe are using following dataset and MAC terminal.\nNext we convert the excel file into a csv file\n\nChange directory in terminal the folder containing excel file\n\nssconvert Schweizer_Nahrwertdatenbank.xlsx newfile.csv\nWe check the first three lines of s_nw.csv with\n\n$ head -n 3 newfile.csv\n\nSince the first two lines are not needed we delete them for further processing\n\nsed -i '' '1,2d' newfile.csv (’’ after i is for mac terminal)\n\nWith excel we explore which column we need for our dashboard:\n\n4: name of product\n6: category of product\n8: unit\n9: energy (kJ)\n12: energy, calories (kcal)\n15: fett, total(g)\n18: fettsäure, gesättigt(g)\n21: fettsäure, einfach ungesättigt(g)\n24: fettsäure, mehrfach ungesättigt(g)\n27: cholesterin (mg)\n30: kohlenhydrate, verfügbar(g)\n33: zucker(g)\n36: stärke (g)\n39: nahrungsfasern(g)\n42: Protein (g)\n45: salz (g)\n48: alkohol(g)\n51: wasser (g)\n54: Vitamin A-Aktivität, RE (µg-RE)\n57: Vitamin A-Aktivität, RAE (µg-RE)\n60: Retinol (µg)\n63: Betacarotin-Aktivität (µg-BCE)\n66: Betacarotin (µg)\n69: Vitamin B1 (Thiamin) (mg)\n72: Vitamin B2 (Riboflavin) (mg)\n75: Vitamin B6 (Pyridoxin) (mg)\n78: Vitamin B12 (Cobalamin) (µg)\n81: Niacin (mg)\n84: Folat (µg)\n87: Pantothensäure (mg)\n90: Vitamin C (Ascorbinsäure) (mg)\n93: Vitamin D (Calciferol) (µg)\n96: Vitamin E (α-Tocopherol) (mg)\n99: Kalium (K) (mg)\n102: Natrium (Na) (mg)\n105: Chlorid (Cl) (mg)\n108: Calcium (Ca) (mg)\n111: Magnesium (Mg) (mg)\n114: Phosphor (P) (mg)\n117: Eisen (Fe) (mg)\n120: Jod (I) (µg)\n123: Zink (Zn) (mg)\n126: Selen (Se) (µg)\n\nSince we have some columns with format “…,…” (e.g, energy, calories(csv)) cut command would lead to problems so we use csvcut\ncsvcut -c 4,6,8,9,12,15,18,21,24,27,30,33,36,39,42,45,48,51,54,57,60,63,66,69,72,75,78,81,84,87,90,93,96,99,102,105,108,111,114,117,120,123,126 newfile.csv &gt; newfile_cut.csv\nNext we want to replace all occurencs of str k.a. (keine Angaben) and Sp. with NULL so tableau doesn’t confuse such coloumns as string columns\nsed -E  's/n\\.d\\./NULL/g' swiss_food_cut.csv &gt; final.csv\nsed -E 's/tr\\./0/g' final_cut_k_test.csv &gt; final_cut_k_test2.csv\nsed -E -i '' 's/[&lt;&gt;]//g' final_cut_k_test2.csv\nsed 's/\"\\([0-9]*\\),\\([0-9]*\\)\"/\\1.\\2/g' final_3.csv &gt; output_1.csv"
  },
  {
    "objectID": "wiki/DOE/doe.html",
    "href": "wiki/DOE/doe.html",
    "title": "Research Design",
    "section": "",
    "text": "Quantitative methods with hypothesis testing are designed around the principles of critical rationalism (Karl Popper). This philosophical position states that theory can never be finally verified but only falsified.\nDescriptive statistics can only describe the sample as a subset of the population. Inferential statistics draws conclusions about the population based on the sample.",
    "crumbs": [
      "Wiki",
      "Design of Experiments",
      "Research Design"
    ]
  },
  {
    "objectID": "wiki/DOE/doe.html#other-forms",
    "href": "wiki/DOE/doe.html#other-forms",
    "title": "Research Design",
    "section": "Other Forms",
    "text": "Other Forms\n\nCross-over design: during study treatment and control switches\n\nused in randomized and non-ramdomized (quasi)\n\nBefore / After comparison\nLongitudinal studies:\n\nTrend study: different times with diff. samples\nPanel study: different times with same sample",
    "crumbs": [
      "Wiki",
      "Design of Experiments",
      "Research Design"
    ]
  },
  {
    "objectID": "wiki/DOE/doe.html#one-factor-at-a-time-ofat",
    "href": "wiki/DOE/doe.html#one-factor-at-a-time-ofat",
    "title": "Research Design",
    "section": "One-Factor-at-a-time (OFAT)",
    "text": "One-Factor-at-a-time (OFAT)",
    "crumbs": [
      "Wiki",
      "Design of Experiments",
      "Research Design"
    ]
  },
  {
    "objectID": "wiki/DOE/doe.html#full-factorial-designs",
    "href": "wiki/DOE/doe.html#full-factorial-designs",
    "title": "Research Design",
    "section": "Full factorial designs",
    "text": "Full factorial designs",
    "crumbs": [
      "Wiki",
      "Design of Experiments",
      "Research Design"
    ]
  },
  {
    "objectID": "wiki/DOE/doe.html#fractional-factorial-designs",
    "href": "wiki/DOE/doe.html#fractional-factorial-designs",
    "title": "Research Design",
    "section": "Fractional factorial designs",
    "text": "Fractional factorial designs",
    "crumbs": [
      "Wiki",
      "Design of Experiments",
      "Research Design"
    ]
  },
  {
    "objectID": "wiki/DOE/doe.html#internal-validity",
    "href": "wiki/DOE/doe.html#internal-validity",
    "title": "Research Design",
    "section": "Internal Validity",
    "text": "Internal Validity\n\nis essential condition for external validity.\nChanges in dependent variable clearly attributable to changes in independent variables.\nIncreases with decreasing impact of nuisance variables (Quasi-Exp -&gt; Experiment).",
    "crumbs": [
      "Wiki",
      "Design of Experiments",
      "Research Design"
    ]
  },
  {
    "objectID": "wiki/DOE/doe.html#external-validity",
    "href": "wiki/DOE/doe.html#external-validity",
    "title": "Research Design",
    "section": "External validity",
    "text": "External validity\n\nResults of experiment can be generalized to population\nincreases with increasing naturalness (laboratory -&gt; field)\nhigh internal validity can be problematic for external validity (labor vs. field)\n\n\nPopulation validity\n\nselection of representative sample\n\n\n\nSituation validity\n\ngeneralizable to situations deviating from experiment (ecological validity)\n\n\n\nConstruct validity\n\nadequate operationalization of latent variable (construct)",
    "crumbs": [
      "Wiki",
      "Design of Experiments",
      "Research Design"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Visualizing Semantic Difference Scales\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Likert Scales\n\n\n\nlikert\n\n\nggplot2\n\n\nbar plot\n\n\n\nDiverging Bar Plots for visualizing positive vs. negative opinions\n\n\n\nOliver Zingg\n\n\nNov 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Quarto for interactive Presentations\n\n\n\nQuarto\n\n\nPresentation\n\n\nEcharts4r\n\n\nInteractive\n\n\nHMTL\n\n\n\nA simple demonstration of how to use R, Revealjs and Echarts4r to create interactive Presentations. HTML file can be easily sent to other and opened with different browsers.…\n\n\n\nOliver Zingg\n\n\nNov 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMasterarbeit\n\n\n\nData Analysis\n\n\nMixed-Effects Model\n\n\nEEG\n\n\nPsychiatric Disorders\n\n\nMasterarbeit\n\n\n\n\n\n\n\nOliver Zingg\n\n\nJun 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOwn Theme\n\n\n\nggplot2\n\n\nVisualization\n\n\nBoxplot\n\n\nCompany Theme\n\n\n\nExperimenting with own themes\n\n\n\nOliver Zingg\n\n\nNov 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy of selected predictors per sample size\n\n\n\n\n\n\n\n\n\nOct 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBachelorarbeit\n\n\n\nMachine Learning\n\n\nSystematic literature review\n\n\nMRI\n\n\nBachelorarbeit\n\n\n\n\n\n\n\nOliver Zingg\n\n\nJun 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Headlines Analysis\n\n\n\nHeadlines\n\n\nGoogle\n\n\nPython\n\n\nSentiment Analysis\n\n\n\n…\n\n\n\nOliver Zingg\n\n\nNov 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfidence Intervals\n\n\n\n\n\n\nOliver Zingg\n\n\nSep 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThemes for Data Visualization with R\n\n\n\n\n\n\nOliver Zingg\n\n\nSep 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSimpson’s Paradox\n\n\n\nLinear Mixed Effects\n\n\nStatistics\n\n\nRandom Effects\n\n\n\n\n\n\n\nOliver Zingg\n\n\nSep 5, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "wiki/Python/basics.html#functions",
    "href": "wiki/Python/basics.html#functions",
    "title": "Learning the Basics",
    "section": "Functions",
    "text": "Functions\nFunctions can only return one reference to a storage location. So returning multiple references must be saved in a list:\n\ndef function(a,b):\n  return list(a,b)\n\nLearning material for python functions (e.g., str.split()): here\n\nFunction chaining:\n\nfunction_1(function_2(a)) = function_2(a).function_1()\n\n\n\nFunction Arguments\n\na,b,c = 3,4,5\n\na,b,c = 3,4,5,6 ## Error\n\na,b,*c = 3,4,5,6 # Works: assign all remaining var to c\n\n* packs elements into a list. Use * again to unpack:\n\nprint(a,b,*c)\n\nThis can be done in function parameter aswell:\n\ndic = {'a':1, 'b':3}\ndef my_f(a,b):\n    return a+b\n\nmy_f(**dic) #Returns  4\n\n** unpacks a dictionary. can only be used as parameter input?\nUsing *args in as parameter must be then accessed with a for loop, since it is provided to the function as tupple.\n\n# print first n letters of words\n\ndef strcut(*str, n):\n    for i in str:\n        print(i[:n])\n# or \n\ndef strcut(*str,n):\n    print('\\n'.join(i[:n] for i in str))\n\n\n\nElse after Function\nElse is executed after the for loop is over (more specific after the StopIteration is raised by the iterator) but if we use break (the loop doesn’t come to and end: else is not executed.) The else keyword in try-except works similar but if try reaches it’s end.\nIf for example we search a list for a flag item we can use else for the case that the flag item is not in the list.\n\nmylist = [1,2,3,4]\ntheflag = 5\n\ndef process(i):\n    return i + 0.5\n\nfor i in mylist:\n    if i == theflag:\n        break\n    process(i)\nelse:\n    raise ValueError(\"List argument missing terminal flag.\")\n\n\n\nRecursion\nSimple recursion of factorial()\n\ndef fac(n):\n  if n == 1:\n    return 1\n  else: \n    return n * fac(n-1)\n\n\n\nString split function\n\ndef split(string, delimiter):\n  \n  result_list = []\n  \n  start = 0\n  for index, char in enumerate(string):\n      if char == delimiter:\n          result_list.append(string[start:index])\n          start = index + 1\n  if start == 0:\n      return [string]\n  result_list.append(string[start:index + 1])\n    \n  return result_list\n\n\n\nString join function\ntakes an iterable (list, tuple) and concatenates its elements into a single string. The seperator is provided as string from which join is called\n\nl = ['1','2','3']\nprint('#'.join(l))\n\n1#2#3\n\n\nIf int in list:\n\nl = [1,2,3]\nprint('#'.join(map(str,l)))\n\n1#2#3\n\n\n\n\n*args and **kwargs\nThey allow you to write functions that can accept an arbitrary number of arguments, making your code more flexible.\n*args collects arguments into a tuple:\n\ndef print_numbers(*args):\n    for number in args:\n        print(number)\n\nprint_numbers(1, 2, 3, 4)\n\n**kwargs collects arguments into a dictionary\n\ndef print_student_info(**kwargs):\n    for key, value in kwargs.items():\n        print(f\"{key}: {value}\")\n\nprint_student_info(name=\"Alice\", age=20, grade=\"A\")\n\nname: Alice\nage: 20\ngrade: A\n\n\nCombining both args and kwargs\n\ndef print_info(name, *args, **kwargs):\n    print(f\"Name: {name}\")\n    print(\"Additional info:\", args)\n    print(\"Details:\", kwargs)\n\nprint_info(\"Alice\", 25, \"Engineer\", country=\"USA\", city=\"New York\")",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#list",
    "href": "wiki/Python/basics.html#list",
    "title": "Learning the Basics",
    "section": "List",
    "text": "List\nAppend new element to list\n\nli = [1,2,2,3]\n\nli.append(4) \nli += [4]\n\n[2] + [1] ## -&gt; [2,1]",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#string-formating",
    "href": "wiki/Python/basics.html#string-formating",
    "title": "Learning the Basics",
    "section": "String formating",
    "text": "String formating\nThere are multiple ways here are two:\n\nFormatted String Literals\nStart with an f or F. Allows including variable names with {}.\nExample: right align and use 5 spaces including var1\n\nprint(f\"{var1:&gt;5}\")\n\nExample: self documentation and debugging: returns var name and what is stored var1= -&gt; var1=7\n\nprint(f\"{var1=}\")\n\n\n\nFormat() Method\nString objects provides methods/functions and apply it to string object\nMore flexible way to string literals: {} only placeholders, must be specified in format()\n\nvar1 ='world'\nvar2 = 7\n\nprint(\"hello {} {}\".format(var1, var2))\nprint(\"hello {1} {0}\".format(var2, var1))\n\ndict = {'num': 7, 'txt': 'world'}\nprint(\"hello {0[txt]} {0[num]}\".format(dict))\n\n\n\nC-style string format\n\nfirst_name = input(\"Enter your first name: \") # your input: Anna\nlast_name = input(\"Enter your last name: \") # your input: Meier\nbirth_year = input(\"Enter your birth year: \") # your input: 1998\nusername = \"%s%s%s\" % (first_name[0].lower(), last_name[:3].lower(), birth_year[-2:])\nprint(\"Generated username: %s\" % username)\n\n#output: Generated username: amei98\n\n\n\ntest whether letter of string is digit or str:\nUse string.isdigit() and string.isalpha()\n\n‘.’, ‘(space)’ or ‘,’ are not seen as letters\n\n\nstring = 'Python 3.2'\n\ncnt_digits = 0\ncnt_str = 0\n\nfor s in string:\n  if s.isdigit():\n    cnt_digits += 1 \n  elif s.isalpha():\n    cnt_str + = 1",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#file-handling",
    "href": "wiki/Python/basics.html#file-handling",
    "title": "Learning the Basics",
    "section": "File Handling",
    "text": "File Handling\nopen(file_name, mode) creates file object in python. It opens a pipe stream of the file that must be closed again close().\nPython provides a way to handle close with the function with. As soon as the block is left, the file is closed.\n\nwith open(file_name, mode) as file_object: \n  ...\n  ...\n  ...\n\n\nRead",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#object-oriented-programming",
    "href": "wiki/Python/basics.html#object-oriented-programming",
    "title": "Learning the Basics",
    "section": "Object Oriented Programming",
    "text": "Object Oriented Programming\nVariables are references to objects and objects belong to a class.\nx = 99 is a references to storage location of a object of class int\n\nClass\nBlueprint for what an object should look like and how it should function. Has several elements. Functions are called methods (inside a class)\n\nheader: indicates beginning of class\ninit(): constructor (build) for objects\nvariables: Class and object\nself: reference to the object that is created\nclass variable: -A class variable in Python is a variable that is shared among all instances of a class. It is defined within the class but outside of any instance methods. Class variables are commonly used when a value needs to be consistent across all instances of a class.\n\nIn methods you can access a class variable with for example: Customer.class_variable_name no self. is needed.\n\n\n\n\nClass Methods\n\n\nStatic Methods\nSame as regular function but defined in a class. Still they are rather independent of the class but are accessed with class.function. No self is needed.\n\nclass A:\n  def __init__(self):\n    ...\n    \n  @staticmethods\n  def function(str):\n    return function(str)\n  \nclass.function\n\n\n\nObject Methods\n\nclass PackageName: \n  \"\"\"\n  Description\n  \"\"\"\n  def __init__(self, name: str, quality: str) -&gt; none:\n      self.name = name\n      self.quality = quality\n# create an object from this class\n\nx: PackageName = PackageName(name = 'ggplot', quality = 'high') #self becomes x\n\n\n\n\nDunder Methods\nHow should objects of a class behave for certain operations\n\nclass A: \n  ...\n  \n  def __add__(self,other):\n    return f'{self.name} + {other.name}'\n  \n\n\nx = A()\ny = A()\n\nprint(x + y)\n\n\n\nSubclass\nInheritance creates a hierarchy of classes\n\nAnimal -&gt; Bird and Cow\n\ntest with issubclass(Bird, Animal). Child classes get all properties from parent class.\n\nInheritance\nWe can use super() to refer to the upper level class (parent class in this case) If we first create a class, the function of the object class if inhereted to our created object.\n\nclass Animal(object class): \n  def __init__(self, weight):\n    self.weight = weight\n    \nclass Bird(Animal):\n  def _init__(self, weight): #override\n    super().__init__(weight)\n    # or Animal.__init__(self, weight)\n\n\n\nInheritance: Override\nWe don’t want all methods or variables from parent class. The interpreter first checks methods and objects in the subclass before it checks the parents, so we can just overwrite the objects in the subclass (or extend functionality). We have to use the same name for the method or other objects!\n\n\n\nAccess Modifiers\n\npublic: access through object\nprotected: only in subclass accessible\n\ncreate with self._name. Is used to mark internal use in class and subclass\n\nprivate: only in declaration of own class accessible\n\ncreate with self.__name. Python internally renames variable. Can’t be accessed in sub classes\n\n\n\nExample private attribute and private method:\n\nclass BankAccount:\n    def __init__(self, balance):\n        self.__balance = balance  # Private attribute\n\n    def __secret_method(self):  # Private method\n        print(\"This is a secret method!\")\n\n    def get_balance(self):\n        return self.__balance  # ✅ Allowed within the class\n\n# Trying to access private members\naccount = BankAccount(1000)\n# print(account.__balance)  # ❌ AttributeError\n# account.__secret_method()  # ❌ AttributeError\n\n# Accessing private members using name mangling (not recommended)\nprint(account._BankAccount__balance)  # ✅ Works, but discouraged\naccount._BankAccount__secret_method()  # ✅ Works, but discouraged\n\n\n\nExample protected attribute and protected method:\n\nclass Car:\n    def __init__(self, brand):\n        self._brand = brand  # Protected attribute\n\n    def _show_brand(self):  # Protected method\n        print(f\"Brand: {self._brand}\")\n\nclass SportsCar(Car):\n    def display(self):\n        print(f\"Sports Car Brand: {self._brand}\")  # ✅ Allowed in subclass\n\n# Accessing protected attributes and methods\nc = Car(\"Tesla\")\nprint(c._brand)  # ⚠️ Allowed but discouraged\nc._show_brand()  # ⚠️ Allowed but discouraged",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#mulitple-inheritance",
    "href": "wiki/Python/basics.html#mulitple-inheritance",
    "title": "Learning the Basics",
    "section": "Mulitple inheritance",
    "text": "Mulitple inheritance\nMultiple parents. Can cause problems for the interpreter if same methods twice.\nSolved with Method Resolution Order (MRO): tries to find a linearization of all classes and depends on the order of inheritance. It doesn’t find a feasible solution every time. Sometimes it helps to swap order.",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#if-name-main",
    "href": "wiki/Python/basics.html#if-name-main",
    "title": "Learning the Basics",
    "section": "If name = main",
    "text": "If name = main\nthe name of the script is only main if it’s the root script.\nSo if you add:\n\nif __name__ = \"main\":\n  ...tests\n\nyou can code functionality that only runs when the script is the root and doesn’t run when the file has been imported into a different script.",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#deep-vs.-shallow-copy",
    "href": "wiki/Python/basics.html#deep-vs.-shallow-copy",
    "title": "Learning the Basics",
    "section": "Deep vs. Shallow Copy",
    "text": "Deep vs. Shallow Copy\nVariable stores references to an object that is stored in one or more(string, lists, tuples..) storage cells. So copying a variable, copies the reference:\n\nx = 99.23\ny = x # copy of reference\n\nid(x) == id(y) \n\nx = 43.12\nid(x) == id(y)\n\n# But..\na = 19.2\nb = 19.2\n\nid(a) == id(b) #two different instances of object int\n\nFalse\n\n\nAssigning a new value creates a new instance of object int by calling: b = int.__new__(value).\nLists are mutable objects: x only refers to the first element of list.\n\nx = [1,2,3,4,5]\n\ny = x  #copy\ny[2] = 6 #change 3 element, doesnt' create a new object!!\n\nprint(x)\n\ny[0] = 9\nid(x) == id(y)\n\n[1, 2, 6, 4, 5]\n\n\nTrue\n\n\nThis is a side effect. So we can create a real copy with the copy() function. Only available for mutable objects (sequences), so not for tuples.\n\nx = [1,2,3,4,5]\n\ny = x.copy()\n\nid(x) == id(y)\n\nstr_text = 'hello'\n\n#str_copy = str_text.copy() doens't work since string gernally not mutable\n\nNested Lists = special case. copy() returns a shallow copy!: Nested lists are references to new storage location\n\nnested_list = [1,2,3,[4,5]]\n\ny = nested_list.copy()\n\ny[-1][0] = 99\n\nprint(y)\nprint(f'not desired change: {nested_list=}')\n\nprint(f'{id(y) == id(nested_list)=}')\n\n[1, 2, 3, [99, 5]]\nnot desired change: nested_list=[1, 2, 3, [99, 5]]\nid(y) == id(nested_list)=False\n\n\n\n#import copy\nnested_list = [1,2,3,[4,5]]\n\n#y = nested_list.copy.deepcopy(nested_list)\ny[-1][0] = 99\n\nprint(f'didn\\'t change: {nested_list=}')",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#lambda-function",
    "href": "wiki/Python/basics.html#lambda-function",
    "title": "Learning the Basics",
    "section": "Lambda Function",
    "text": "Lambda Function\nKeep in mind:\n\ndef my_fun():\n  def inner_fun(x):\n    return x \n  return inner_fun\n\nmy_if = my_fun() #reference to inner function\nprint(my_if('hello'))\n\n#is the same as \n\nmy_if = my_fun #reference to outer function\nprint(my_if()('hello'))\n\nLambda: No header. Only body of function. Only one expression.\n\nx = lambda a,b: a + b\n\nprint('x: ', x(3,5))\n\nx:  8\n\n\n\ndef fun():\n  return 'hello' \n  \nx = lambda: fun()\n\nprint('x: ', x())\n\nx = lambda a: a()\nprint('x: ', x(fun))\n\nx:  hello\nx:  hello\n\n\nWe can use lambda for generic methods example:\n\ndef fun_ii(f, a, b):\n  return f(a,b)\n\nprint('fun_ii: ' , fun_ii(lambda x,y: x + y, 2, 3))\nprint('fun_ii: ' , fun_ii(lambda x,y: x - y, 2, 3))\n\nfun_ii:  5\nfun_ii:  -1\n\n\nUsually combined with other functions like map() or sort().\nmap(): applies a function to every single item of an iterable. Same as for loop.\n\nnumbers = [1,2,3,4]\ntest = map(lambda element: element-1,numbers) #we get a map object back\n\ntest = list(map(lambda element: element-1,numbers)) #create a list. List(test)\ntest\n\n[0, 1, 2, 3]\n\n\nsort(): sorts an iterable based on a given comparator\n\n#sorts based on remains after modulo\nnumbers = [1,2,3,4,6]\n\nnumbers.sort(key= lambda element: element %3)\nprint(numbers)\n\n[3, 6, 1, 4, 2]",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#list-comprehension",
    "href": "wiki/Python/basics.html#list-comprehension",
    "title": "Learning the Basics",
    "section": "List Comprehension",
    "text": "List Comprehension\nSingle expression of for loop:\n\nx = [1,2,3,4,5,6]\nprint(x)\n\nx2 =[i**2 for i in x]\nprint(x2)\n\nx = [i**2 for i in x]\nprint(x)\n\n[1, 2, 3, 4, 5, 6]\n[1, 4, 9, 16, 25, 36]\n[1, 4, 9, 16, 25, 36]\n\n\nAdd conditions\n\nx = [1,2,3,4,5,6]\n\nx2 = [i**2 for i in x if i%2] #unequal numbers (&gt;0 equals true)\nprint(x2)\n\nx3 = [i**2 for i in x if not i%2] #equal numbers !(&gt;0 equals true)\nprint(x3)\n\nx4 = [i**2 if not i%2 else i-10 for i in x] #if else short-hand\nprint(x4)\n\n[1, 9, 25]\n[4, 16, 36]\n[-9, 4, -7, 16, -5, 36]\n\n\nNested Looops\n\nli = [[1,2,3],[22,33]]\n\nres = [element*2 for row in li for element in row] #simple list return\nres2 = [[element*2 for element in row] for row in li] #nested list return\nprint(res)\nprint('\\n')\nprint(res2)\n\n[2, 4, 6, 44, 66]\n\n\n[[2, 4, 6], [44, 66]]\n\n\nComprehension with tuples returns a generator but you can use unpacker *.\n\nword_list = [\"apple\", \"banana\", \"cherry\", \"date\"]\n\n# Use list comprehension to add \"__\" to every entry\nmodified_list = [word + \"__\" for word in word_list]",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#try-except",
    "href": "wiki/Python/basics.html#try-except",
    "title": "Learning the Basics",
    "section": "Try-Except",
    "text": "Try-Except\nStrict order:\n\ntry:\nexcept: (optional if there is finally)\nelse: (optional, executed only if no exception is raised.)\nfinally: (optional if there is except, is executed always no matter what)\n\nfinally still breaks code if there is an error so everything after finally is not executed (if no exception is given).\nfinally is good for garbarge collection: delete used variable in try, close files etc. or to give message why something went wrong\n\n\n\ntry:\n  num = input('Enter a number: ')\n  frac = 1/int(num)\nexcept ValueError as e:\n  print('no number given:', e)\nexcept ZeroDivisionError:\n  print('can not divide by 0')\nexcept:\n  print('any other error happend')\nelse:\n  print(f'the fraction is: {frac}')\nfinally:\n  print('leaving try-except clause')\n\nExceptions are handled in order and once one is executed the reset is skipt including else statement. For example the second ZeroDivisionError will not be executed.\n\ntry:\n  num = input('Enter a number: ')\n  frac = 1/int(num)\nexcept (ValueError,ZeroDivisionError) as e:\n  print('no number given:', e)\nexcept ZeroDivisionError:\n  print('can not divide by 0')\nexcept:\n  print('any other error happend')\nelse:\n  print(f'the fraction is: {frac}')\nfinally:\n  print('leaving try-except clause')\n\nany other error happend\nleaving try-except clause\n\n\nThese examples are explicit Exceptions since we specifiy the excat error. If we use except Exception as e then this is implicit.\n\ndef strselect(string,key,p):\n    return_list = []\n    for i in string:\n        try:    \n            if i[p] == key:\n                return_list.append(i)\n        except IndexError as e:\n            print(\"string index out of range\")\n    return return_list\n\n\n#possible function call:\nprint(strselect([\"hello\", \"of\", \"help\", \"world\"], key = \"l\", p = 2))\n\nstring index out of range\n['hello', 'help']\n\n\n\nCustomize and Raise Errors\nUsing the keyword raise we can raise an error:\nIf creating an own CustError class don’t inheritage from BaseException.\n\nclass CustError(Exception):\n  def __init__(self,message):\n    super().__init__(message)\n    \n    # customize error code..\n    self.errors = \"custom error\"\n\nif input('enter number: ') == '0': \n  raise CustError('customized error message')\n\ndef invert_value(val):\n  if type(val) != int:\n    raise ValueError('the given value is not of type integer.')\n  \n  return 1/val\n\ntry: \n  invert_value(19.2)\nexcept ValueError as e:\n  print(e)\n  \n  #returns our custome message",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#generator",
    "href": "wiki/Python/basics.html#generator",
    "title": "Learning the Basics",
    "section": "Generator",
    "text": "Generator\nIterable\nObjects that implements the __iter__() method and returns an iterator Examples: lists,tuples, sets, strings\n\n# Iterable object (list)\nmy_list = [1, 2, 3]\n\n# Create an iterator from the iterable\nmy_iterator = iter(my_list)\n\n# Checking if the iterator is an iterable\nprint(isinstance(my_iterator, Iterable))  # True (because an iterator is iterable)\n\n# Iterating through the iterator\nprint(next(my_iterator))  # Output: 1\nprint(next(my_iterator))  # Output: 2\nprint(next(my_iterator))  # Output: 3\n\n# The following call will raise StopIteration as all items are exhausted\n# print(next(my_iterator))  # StopIteration\n\nIterator: \nIn for i in li: the keyword for creates an iterator and passes the provided list (iterable). The iterator points to the first element of the list and the object is returned. The we call the __next__() method of this object to get the second object in li until __next__() raises an StopIteration exception. An iterator is always an iterable but not all iterables are iterators\n\nstr, lists, tuples, sets, dictionaries, ranges, files, generators\n\n\nx = list(range(9))\n\n\nitr = iter(x)\n\nitr.__next__()\nitr.__next__()\nprint('-'*10)\n# Iterator remembers current position so for loop will only print remaining i's\nfor i in itr: \n  print(i)\n\n# raises error\n#itr.__next__()\n\n----------\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# A generator expression\ngen_expr = (x * 2 for x in range(5))\n\nnext(gen_expr)\n\n\nnon-Iterable\na non-iterable object is an object that cannot be looped over:\n\nint, float, bool, complex, none\n\n\n\nConcept of Generator (Generator Function)\nEach iteration is called on demand and not at the start: saving memory (lazy evaluation)\n\nUses yield keyword instead of return and returns a generator iterator.\nBreaks at each yield but remembers where it was and can be continuied until StopIterator is raised\ncan be written as generator expression without yield: (x**2 for x in range(4))\n\n\n\ndef my_sum(x):\n  \n  return x + 2\n\ndef my_sum_gen(x):\n  \n  y = x \n  while True:\n    \n    yield y + 2 \n    yield y + 5\n    \n\nr2 = my_sum_gen(7)\n\nprint(next(r2))\nprint(next(r2))\n\n9\n12\n\n\nExample\n“In ordered large list are there values smaller than 0?”\n\nk = range(1_000_000)\nany(x &lt; 0 for x in k)\n\nFalse\n\n\n\n# A generator function\ndef my_generator():\n    yield 1\n    yield 2\n    yield 3\n\n# Create a generator object\ngen = my_generator()\n\n# Using the generator object in a for loop (iteration)\nfor value in gen:\n    print(value)\n\n1\n2\n3",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#decorator-not-in-exam-225",
    "href": "wiki/Python/basics.html#decorator-not-in-exam-225",
    "title": "Learning the Basics",
    "section": "Decorator (not in exam) 2:25",
    "text": "Decorator (not in exam) 2:25\nFunction can be called from other function",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#non-sequence-types",
    "href": "wiki/Python/basics.html#non-sequence-types",
    "title": "Learning the Basics",
    "section": "Non-Sequence Types",
    "text": "Non-Sequence Types\nIn Python, non-sequence types are objects that do not store elements in a specific order and do not support indexing or slicing.\n\nSet, Dictionary, Numbers(int,float, complex), bool, None",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#dictionnary",
    "href": "wiki/Python/basics.html#dictionnary",
    "title": "Learning the Basics",
    "section": "Dictionnary",
    "text": "Dictionnary\n\ninfo = {\"name\": \"Alice\", \"age\": 30}\nfor key in info:\n    print(key)  # Output: name, age\n\n# Iterating over keys and values\nfor key, value in info.items():\n    print(key, value)  # Output: name Alice, age 30\n\n\nExam exercise\n\n# Example list\nmy_list = [10, 20, 30, 40, 50]\n\n# Initialize an empty dictionary\nmydict = {}\n\n# Iterate over the list using enumerate\nfor index, value in enumerate(my_list):\n    # Add the index and value to the dictionary\n    mydict[index] = value\n\n# Print the dictionary to verify the result\nprint(mydict)\n\n\nDict comprehension\n\n# Create the dictionary using dictionary comprehension\nmydict = {index: value for index, value in enumerate(my_list)}\n\n\n\n\nexercise illias\n\nq_dict = {'idx': [],'qm':[], 'cat': []}\n\n\nfor idx, value in enumerate(q):\n    q_dict['idx'].append(idx)\n    q_dict['qm'].append(value)\n\n    # you can also use if, elif, else\n    # if value &lt; 0.2\n    # elif value &lt; 0.4\n    # elif &lt; 0.6....\n    match value:\n        case value if 0.2 &gt; value &gt;= 0:\n            q_dict['cat'].append('bad')\n        case value if 0.4 &gt; value &gt;= 0.2:\n            q_dict['cat'].append('weak')\n        case value if 0.6 &gt; value &gt;= 0.4:\n            q_dict['cat'].append('balanced')\n        case value if 0.6 &lt;= value &lt; 0.8:\n            q_dict['cat'].append('good')\n        case value if 0.8 &lt;= value &lt;= 1:\n            q_dict['cat'].append('very good')",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#modules",
    "href": "wiki/Python/basics.html#modules",
    "title": "Learning the Basics",
    "section": "Modules",
    "text": "Modules\npaths when import:\n\nthe current script sets the root for the ide but not for the terminal.\nwith .. you can move up in the folder hierarchy.\ninstead of / or \\ python uses .\nfrom math import * imports everything and you can refer to the function without math.\n\nnot a good style since it can cause problems",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#pycharm",
    "href": "wiki/Python/basics.html#pycharm",
    "title": "Learning the Basics",
    "section": "",
    "text": "Use # %% Testto create executable blocks",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#dictionairy-comprehension",
    "href": "wiki/Python/basics.html#dictionairy-comprehension",
    "title": "Learning the Basics",
    "section": "Dictionairy comprehension",
    "text": "Dictionairy comprehension\nzip() returns list of tuples\n\n#turn into dict\nx = [1,2,4,5]\n\nd = {i:i for i in x}\nprint(d)\nd1 = {k:v for (k,v) in enumerate(x)}\nprint(d1)\n\nd2 = {k:v for (k,v) in zip(['a','b','c'],x)}\nprint(d2)\n\n{1: 1, 2: 2, 4: 4, 5: 5}\n{0: 1, 1: 2, 2: 4, 3: 5}\n{'a': 1, 'b': 2, 'c': 4}\n\n\n\nfrom random import sample\n\nclass Stream:\n    def __init__(self, id):\n        self.my_id = id\n    \n    def __repr__(self):\n        return f\"Stream obj: {self.my_id}\"\n\n# Create 10 stream objects with random IDs (e.g., from 1 to 100)\nstream_objects = [Stream(id) for id in sample(range(1, 101), 10)]\n\n# Print each stream object using unpack operator \nprint(*stream_objects, sep='\\n')\n\nStream obj: 40\nStream obj: 7\nStream obj: 91\nStream obj: 11\nStream obj: 71\nStream obj: 82\nStream obj: 39\nStream obj: 15\nStream obj: 45\nStream obj: 73",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#largest-number-delete-if-in-last-position",
    "href": "wiki/Python/basics.html#largest-number-delete-if-in-last-position",
    "title": "Learning the Basics",
    "section": "Largest number: delete if in last position",
    "text": "Largest number: delete if in last position\n\nfor i in numbers[:-1]: #go through numbers except last \n    if i == max(numbers):\n        break #exit loop: max number not last position\nelse: #is executed only if for loop comes to end\n    numbers.pop() #removes last number",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#oop-buildings",
    "href": "wiki/Python/basics.html#oop-buildings",
    "title": "Learning the Basics",
    "section": "OOP Buildings",
    "text": "OOP Buildings\n\nclass Building:\n    # Class variables\n    _counter = 0\n    _id_container = []\n\n    def __init__(self, number_of_floors):\n        # Object variables\n        self._number_of_floors = number_of_floors\n        self._building_id = self._get_new_id()  # Assign building id at creation\n\n    def _get_new_id(self):\n        # Increment counter and create a new building ID\n        Building._counter += 1\n        building_id = Building._counter\n        if building_id in Building._id_container:\n            raise ValueError(f\"Building ID {building_id} is already taken.\")\n        Building._id_container.append(building_id)\n        return building_id\n\n    def get_number_of_floors(self):\n        # Return the number of floors for the building\n        return self._number_of_floors\n\nclass OfficeBuilding(Building):\n    def __init__(self, number_of_floors, number_of_working_places):\n        # Call parent class constructor to initialize number_of_floors and building_id\n        super().__init__(number_of_floors)\n        # Additional object variable for office buildings\n        self._number_of_working_places = number_of_working_places\n\n    def get_number_of_working_places(self):\n        # Return the number of working places for the office building\n        return self._number_of_working_places\n\n# Create a Building object with 10 floors\nbuilding_1 = Building(10)\n# Create an OfficeBuilding object with 15 floors and 200 working places\noffice_building_1 = OfficeBuilding(15, 200)\n\n# Print building IDs and number of floors for both buildings\nprint(f\"Building 1 ID: {building_1._building_id}, Floors: {building_1.get_number_of_floors()}\")\nprint(f\"Office Building 1 ID: {office_building_1._building_id}, Floors: {office_building_1.get_number_of_floors()}, Working Places: {office_building_1.get_number_of_working_places()}\")",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#lambda-divisor-3",
    "href": "wiki/Python/basics.html#lambda-divisor-3",
    "title": "Learning the Basics",
    "section": "Lambda Divisor 3",
    "text": "Lambda Divisor 3\n\n# Define the lambda function using conditional expression\ndivisible_by_3 = lambda num: True if num % 3 == 0 else False\n\n# Example list (even_list)\neven_list = [2, 3, 6, 8, 9, 12, 15, 20, 21, 24]\n\n# Use the lambda function to filter the numbers divisible by 3\ndivisible_by_3_list = [num for num in even_list if divisible_by_3(num)]\n\n# Print the resulting list\nprint(divisible_by_3_list)",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#lambda-dict",
    "href": "wiki/Python/basics.html#lambda-dict",
    "title": "Learning the Basics",
    "section": "Lambda dict",
    "text": "Lambda dict\n\n# Example list\neven_list = [2, 3, 6, 8, 9, 12, 15, 20, 21, 24]\n\n# Create dictionary using lambda function and dictionary comprehension\ndivisible_dict = {num: (lambda x: True if 98729 % x == 0 else False)(num) for num in even_list}\n\n# Print the resulting dictionary\nprint(divisible_dict)",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "wiki/Python/basics.html#customer",
    "href": "wiki/Python/basics.html#customer",
    "title": "Learning the Basics",
    "section": "Customer",
    "text": "Customer\n\n#!/usr/bin/ python\n\nclass Customer():\n\n    custType = 'standard'\n    custNo = 0\n\n    def __init__(self, first_name, last_name, company_name):\n        self.f_name = first_name\n        self.l_name = last_name\n        self.c_name = company_name\n        self.custNo = None                      # customer number\n        self.dist2hub = None                    # distance to storage hub\n        self.avgDelPerYear = None               # average no of deliveries per year\n        self.recCargoWeight = None              # recording (list) of delivery weights\n\n    def load_cust_data(self):\n        '''\n        The method 'load_cust_data()' initializes instance variables that are\n        initialized with none. These numbers have no effect on the program\n        extension, i.e. are irrelevant for the evaluation.\n        '''\n        self.custNo = Customer.custNo\n        self.dist2hub = (self.custNo+15)*15\n        self.avgDelPerYear = 1/self.dist2hub*2250\n        self.recCargoWeight = list(range(self.custNo))\n        Customer.custNo += 1                    # increment customer number\n\n    def correctCargoWeights(self,correction):\n        '''\n        For cargoWeight correction, the passed lambda function from the parameter\n        list is applied on all elements of the cargoWeight instance variable\n        (i.e. the parameter correction points to a lambda function).\n        '''\n        self.recCargoWeight = list(map(correction, self.recCargoWeight))\n\n\nExample of Use\n\n# Create a new Customer object\ncustomer1 = Customer(\"John\", \"Doe\", \"ABC Corp\")\n\n# Load customer data\ncustomer1.load_cust_data()\n\n# Print customer details\nprint(f\"Customer {customer1.f_name} {customer1.l_name} has ID {customer1.custNo}\")\nprint(f\"Distance to Hub: {customer1.dist2hub}\")\nprint(f\"Average Deliveries per Year: {customer1.avgDelPerYear}\")\nprint(f\"Cargo Weights: {customer1.recCargoWeight}\")\n\n# Correct cargo weights using a lambda function (e.g., multiplying each by 2)\ncustomer1.correctCargoWeights(lambda x: x * 2)\n\n# Print corrected cargo weights\nprint(f\"Corrected Cargo Weights: {customer1.recCargoWeight}\")\n\nCustomer John Doe has ID 0 Distance to Hub: 225 Average Deliveries per Year: 10.0 Cargo Weights: [0] Corrected Cargo Weights: [0]\n\n\nSportCustomer in different file\n\n# Assuming that SportCustomer class should inherit from Customer (or any base class)\nfrom customer import Customer\n\nclass SportCustomer(Customer):\n    def __init__(self, first_name, last_name, company_name):\n        # Initialize the parent class (Customer)\n        super().__init__(first_name, last_name, company_name)\n        self.priority = None  # Initialize the priority instance variable\n\n    def setPriority(self, priority):\n        \"\"\"\n        Sets the 'priority' instance variable.\n        Raises a ValueError if the passed priority is greater than 100.\n        \"\"\"\n        if priority &gt; 100:\n            raise ValueError(\"Priority value cannot be greater than 100\")\n        self.priority = priority",
    "crumbs": [
      "Wiki",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Willkommen auf meiner Webseite",
    "section": "",
    "text": "Willkommen auf meiner Webseite\nHier findest du mehr Infos über mich, wie auch Beschreibungen meiner bisherigen Projekte. Für Fragen bin ich erreichbar per Email."
  },
  {
    "objectID": "posts/Semdif/index.html",
    "href": "posts/Semdif/index.html",
    "title": "Visualizing Semantic Difference Scales",
    "section": "",
    "text": "A common way to test user experience of a product of app is to use semantic difference scales where adjectives of for example opposite meaning are placed compared. The customer must chose to what degree one of the certain adjectives is more suited.\n\n\nUser Experience Questionnaire is a typical questionnaire used for this purpose.\n\nLaugwitz, B., Held, T., & Schrepp, M. (2008). Construction and evaluation of a user experience questionnaire. In HCI and Usability for Education and &gt;Work: 4th Symposium of the Workgroup Human-Computer Interaction and Usability Engineering of the Austrian Computer Society, USAB 2008, Graz, Austria, &gt;November 20-21, 2008. Proceedings 4 (pp. 63-76). Springer Berlin Heidelberg.\n\n\n\n\nExample Questionnaire: UEQ\n\n\n\n\n\n\n\n\nSemantic Difference with 95% Confidence intervalls\n\n\nCode here"
  },
  {
    "objectID": "posts/Semdif/index.html#example",
    "href": "posts/Semdif/index.html#example",
    "title": "Visualizing Semantic Difference Scales",
    "section": "",
    "text": "User Experience Questionnaire is a typical questionnaire used for this purpose.\n\nLaugwitz, B., Held, T., & Schrepp, M. (2008). Construction and evaluation of a user experience questionnaire. In HCI and Usability for Education and &gt;Work: 4th Symposium of the Workgroup Human-Computer Interaction and Usability Engineering of the Austrian Computer Society, USAB 2008, Graz, Austria, &gt;November 20-21, 2008. Proceedings 4 (pp. 63-76). Springer Berlin Heidelberg.\n\n\n\n\nExample Questionnaire: UEQ"
  },
  {
    "objectID": "posts/Semdif/index.html#visualization-using-ggplot2",
    "href": "posts/Semdif/index.html#visualization-using-ggplot2",
    "title": "Visualizing Semantic Difference Scales",
    "section": "",
    "text": "Semantic Difference with 95% Confidence intervalls\n\n\nCode here"
  },
  {
    "objectID": "posts/Presentation using Quarto/index.html",
    "href": "posts/Presentation using Quarto/index.html",
    "title": "Using Quarto for interactive Presentations",
    "section": "",
    "text": "In quarto for R environment we can create presentations that runs on revealjs. The rendered quarto file results in a HTML file that can be open in regular browsers and makes it great for sharing. Using scss the presentation can be customized. Using R packages like echarts4r we can use javascript tools for creating interactive plots. Compared to a dashboard this can be helpful to guide an audience through specific plots in a presentation style which can be easier to follow or communicate a certain message.\nAs an example here is a very simple presentation created: Link to fullscreen presentation\nUsing iframes we can incorporate the same presentation into this blog post but certain controls like F for fullscreen doesn’t work.\n\n\n\n\n\n\nControls in Iframe\n\n\n\nPress on the presentation displayed below and use right and left arrow keys to navigate."
  },
  {
    "objectID": "wiki/Python/LLM.html",
    "href": "wiki/Python/LLM.html",
    "title": "LLM",
    "section": "",
    "text": "Source: Sebastian Raschka"
  },
  {
    "objectID": "wiki/Python/LLM.html#intro",
    "href": "wiki/Python/LLM.html#intro",
    "title": "LLM",
    "section": "Intro",
    "text": "Intro\n\nLLMs have remarkable capabilities to understand, generate, and interpret human language. However, it’s important to clarify that when we say language models “under- stand,” we mean that they can process and generate text in ways that appear coher- ent and contextually relevant, not that they possess human-like consciousness or comprehension.\n\n\nLLMs have remarkable capabilities to understand, generate, and interpret human language. However, it’s important to clarify that when we say language models “under- stand,” we mean that they can process and generate text in ways that appear coher- ent and contextually relevant, not that they possess human-like consciousness or comprehension."
  },
  {
    "objectID": "wiki/Python/rag.html",
    "href": "wiki/Python/rag.html",
    "title": "Retrieval Augmented Generation",
    "section": "",
    "text": "Sources:",
    "crumbs": [
      "Wiki",
      "Python",
      "Retrueval Augmented Generation"
    ]
  },
  {
    "objectID": "wiki/Python/rag.html#vector-databases",
    "href": "wiki/Python/rag.html#vector-databases",
    "title": "Retrieval Augmented Generation",
    "section": "Vector Databases",
    "text": "Vector Databases\n\nEfficiently store, index, and search high-dimensional data points (i.e. vectors).\nEach entry is represented as a vector in a multi-dimensional space.\nVector databases excel at retrieving semantically similar data points\nVector databases are computationally intensive but scaleable.\nWith similarity search relevant vectors are retrieved.\n\n\nVector databases, also known as similarity search engines or vector index databases, play a crucial role in the retrieval component of RAG by efficiently storing and retrieving these vector embeddings. They are specialized databases designed for retrieving vectors based on similarity, making them well-suited for scenarios where similarity between data points needs to be calculated quickly and accurately.",
    "crumbs": [
      "Wiki",
      "Python",
      "Retrueval Augmented Generation"
    ]
  },
  {
    "objectID": "wiki/Python/rag.html#embedding-model",
    "href": "wiki/Python/rag.html#embedding-model",
    "title": "Retrieval Augmented Generation",
    "section": "Embedding model",
    "text": "Embedding model\n\noften complex neural network: translating data into high.dimensional numerical vectors and effectively encode the data’s essential characteristics into vector embeddings",
    "crumbs": [
      "Wiki",
      "Python",
      "Retrueval Augmented Generation"
    ]
  },
  {
    "objectID": "wiki/Python/rag.html#use-cases",
    "href": "wiki/Python/rag.html#use-cases",
    "title": "Retrieval Augmented Generation",
    "section": "Use Cases",
    "text": "Use Cases\n\nRecommendation system: similarity search between user behavior (input, etc.) and vector space\nReal-time fraud detection\nMarket research: customer feedback -&gt; text embeddings -&gt; sentiment analysis and trend spotting",
    "crumbs": [
      "Wiki",
      "Python",
      "Retrueval Augmented Generation"
    ]
  },
  {
    "objectID": "wiki/Questionnaires/start.html",
    "href": "wiki/Questionnaires/start.html",
    "title": "Questionnaires",
    "section": "",
    "text": "Definition:\n\nIn theory, not verifiable by external observation or records.\nHas evaluative component\nHow strong agree or disagree\nIn contrast to beliefs (cognitive component): How healthy is a pizza?\nIn contrast to behavioral intentions: In the next 6 months, how likely is it that you will buy a car\n\n\n\nA psychological tendecy that is expressed by evaluating a particular entity with some degree of favor or disfavor\nAttitudes are Pre-existing: - Automatic activation of attitudes (retrievable: “file-drawer”) - stable and predictive of behaviors, resistant to persuasion\n\n\nIn survey big differences are found depending on how question is asked.\n\n\n\n\n“Making it up as you go along”\n\nindividuals do not typically possess true attitudes rather they construct opinion statements on the fly based on whatever considerations are momentarily salient\n\nSchwarz 2007: evaluative judgments formed on the spot rather than as trait-like dispositions\n\n\n\nAttitudes vs. attitude expressions\n\nAny single attitude measure will be an imperfect reflection of the underlying attitude\nAttitudes are unobservable, global evaluations of objects\n\nSome attitude expressions are construced on the spot, some are retrieved almost entirely intact from memory, others are a combination.\n\n\n\nNo attitudes, but contextualized construction of evaluations/judgments on the spot drawing on what is available to Respondent (R)\n\nStable attitudes = similar evalutions arising from R drwaing on similar inputs or the sam chronically accessible information.\nStrong attitudes = R drawing on highly accessible information -Context effects = R drawing on contexutal information when constructing evaluations -Nonattitude = Same number of conflicting information accessible to R at the same time\n\n\n\n\n\nSubject of questionnaire\nInterviewer\nInterview setting\nInstructions\nPictures that accompany the question\nWording of the question\nResponse options\nQuestion order\n\n\n\n\nAssimilation effects: Rs include prior question in interpreting/understanding the current item\nContrast effects: Rs exclude prior questions in interpreting and understanding the current item\n\n\n\n\nContext prompts the accessibility or retrieval of certain considerations (assimilation) or exclude the retrieval of certain considerations (contrast)\n\n\n\nThe inclusion/exclusion model Constructing targets and standards\n\n\n\n\nOnly ask specific question and if general question then asked them first.\n\n\n\n\n\nFast for R Fewer “don’t know” R like these scales\n\n\n\nAcquiesence Bias - R wants to be polite and agreeable - R might defer to higher status (yes sure) - R adopts a satisficing strategy (do just enough)\nMulti-barrled - multi information in statement\nDisagree end is ambigious\n\n\n\n\nHow certain are you of your views on abortion? - Very certain, somewhat certain, not very certain, not certain at all"
  },
  {
    "objectID": "wiki/Questionnaires/start.html#traditional-view-on-attitude",
    "href": "wiki/Questionnaires/start.html#traditional-view-on-attitude",
    "title": "Questionnaires",
    "section": "",
    "text": "A psychological tendecy that is expressed by evaluating a particular entity with some degree of favor or disfavor\nAttitudes are Pre-existing: - Automatic activation of attitudes (retrievable: “file-drawer”) - stable and predictive of behaviors, resistant to persuasion\n\n\nIn survey big differences are found depending on how question is asked."
  },
  {
    "objectID": "wiki/Questionnaires/start.html#alternative-view",
    "href": "wiki/Questionnaires/start.html#alternative-view",
    "title": "Questionnaires",
    "section": "",
    "text": "“Making it up as you go along”\n\nindividuals do not typically possess true attitudes rather they construct opinion statements on the fly based on whatever considerations are momentarily salient\n\nSchwarz 2007: evaluative judgments formed on the spot rather than as trait-like dispositions"
  },
  {
    "objectID": "wiki/Questionnaires/start.html#rasinksis-way-resovling-these-divergent-views",
    "href": "wiki/Questionnaires/start.html#rasinksis-way-resovling-these-divergent-views",
    "title": "Questionnaires",
    "section": "",
    "text": "Attitudes vs. attitude expressions\n\nAny single attitude measure will be an imperfect reflection of the underlying attitude\nAttitudes are unobservable, global evaluations of objects\n\nSome attitude expressions are construced on the spot, some are retrieved almost entirely intact from memory, others are a combination."
  },
  {
    "objectID": "wiki/Questionnaires/start.html#schwarz-resolving",
    "href": "wiki/Questionnaires/start.html#schwarz-resolving",
    "title": "Questionnaires",
    "section": "",
    "text": "No attitudes, but contextualized construction of evaluations/judgments on the spot drawing on what is available to Respondent (R)\n\nStable attitudes = similar evalutions arising from R drwaing on similar inputs or the sam chronically accessible information.\nStrong attitudes = R drawing on highly accessible information -Context effects = R drawing on contexutal information when constructing evaluations -Nonattitude = Same number of conflicting information accessible to R at the same time"
  },
  {
    "objectID": "wiki/Questionnaires/start.html#context-effects",
    "href": "wiki/Questionnaires/start.html#context-effects",
    "title": "Questionnaires",
    "section": "",
    "text": "Subject of questionnaire\nInterviewer\nInterview setting\nInstructions\nPictures that accompany the question\nWording of the question\nResponse options\nQuestion order\n\n\n\n\nAssimilation effects: Rs include prior question in interpreting/understanding the current item\nContrast effects: Rs exclude prior questions in interpreting and understanding the current item\n\n\n\n\nContext prompts the accessibility or retrieval of certain considerations (assimilation) or exclude the retrieval of certain considerations (contrast)\n\n\n\nThe inclusion/exclusion model Constructing targets and standards"
  },
  {
    "objectID": "wiki/Questionnaires/start.html#specific-vs.-general-evaluations",
    "href": "wiki/Questionnaires/start.html#specific-vs.-general-evaluations",
    "title": "Questionnaires",
    "section": "",
    "text": "Only ask specific question and if general question then asked them first."
  },
  {
    "objectID": "wiki/Questionnaires/start.html#agree-disagree-scales-should-be-avoided",
    "href": "wiki/Questionnaires/start.html#agree-disagree-scales-should-be-avoided",
    "title": "Questionnaires",
    "section": "",
    "text": "Fast for R Fewer “don’t know” R like these scales\n\n\n\nAcquiesence Bias - R wants to be polite and agreeable - R might defer to higher status (yes sure) - R adopts a satisficing strategy (do just enough)\nMulti-barrled - multi information in statement\nDisagree end is ambigious"
  },
  {
    "objectID": "wiki/Questionnaires/start.html#construct-specific-repsonse-choices-recommended",
    "href": "wiki/Questionnaires/start.html#construct-specific-repsonse-choices-recommended",
    "title": "Questionnaires",
    "section": "",
    "text": "How certain are you of your views on abortion? - Very certain, somewhat certain, not very certain, not certain at all"
  },
  {
    "objectID": "wiki/Python/rag.html#retrieval-component",
    "href": "wiki/Python/rag.html#retrieval-component",
    "title": "Retrieval Augmented Generation",
    "section": "Retrieval Component",
    "text": "Retrieval Component\nResponsible for searching and retrieving relevant information. Using indexing, ranking and query expansion",
    "crumbs": [
      "Wiki",
      "Python",
      "Retrueval Augmented Generation"
    ]
  },
  {
    "objectID": "wiki/Python/rag.html#generation-component",
    "href": "wiki/Python/rag.html#generation-component",
    "title": "Retrieval Augmented Generation",
    "section": "Generation Component",
    "text": "Generation Component\nAfter relevant information is retrieved LLMs process the retrieved information and generates coherent and contextually accurate response to user queries.",
    "crumbs": [
      "Wiki",
      "Python",
      "Retrueval Augmented Generation"
    ]
  },
  {
    "objectID": "wiki/Python/rag.html#interaction-loop",
    "href": "wiki/Python/rag.html#interaction-loop",
    "title": "Retrieval Augmented Generation",
    "section": "Interaction Loop",
    "text": "Interaction Loop\nRetrieval-Augmented Generation often involves an interaction loop between the retrieval and generation components. The initial retrieval may not always return the perfect answer, so the generation component can refine and enhance the response iteratively by referring back to the retrieval results.",
    "crumbs": [
      "Wiki",
      "Python",
      "Retrueval Augmented Generation"
    ]
  },
  {
    "objectID": "wiki/Python/rag.html#fine-tuning",
    "href": "wiki/Python/rag.html#fine-tuning",
    "title": "Retrieval Augmented Generation",
    "section": "Fine-Tuning",
    "text": "Fine-Tuning\nSuccessful implementation of this approach often requires fine-tuning LLMs on domain-specific data. Fine-tuning adapts the model to understand and generate content relevant to the specific knowledge domain, improving the quality of responses.",
    "crumbs": [
      "Wiki",
      "Python",
      "Retrueval Augmented Generation"
    ]
  },
  {
    "objectID": "wiki/Python/rag.html#latent-space-representations",
    "href": "wiki/Python/rag.html#latent-space-representations",
    "title": "Retrieval Augmented Generation",
    "section": "Latent Space Representations",
    "text": "Latent Space Representations\nRetrieval models often convert documents and queries into latent space representations, making it easier to compare and rank documents based on their relevance to a query. These representations are crucial for efficient retrieval.",
    "crumbs": [
      "Wiki",
      "Python",
      "Retrueval Augmented Generation"
    ]
  },
  {
    "objectID": "wiki/Python/rag.html#attention-mechanisms",
    "href": "wiki/Python/rag.html#attention-mechanisms",
    "title": "Retrieval Augmented Generation",
    "section": "Attention Mechanisms",
    "text": "Attention Mechanisms\nBoth the retrieval and generation components typically employ attention mechanisms. Attention mechanisms help the model focus on the most relevant parts of the input documents and queries, improving the accuracy of responses.",
    "crumbs": [
      "Wiki",
      "Python",
      "Retrueval Augmented Generation"
    ]
  },
  {
    "objectID": "wiki/Python/rag.html#vector-embeddings",
    "href": "wiki/Python/rag.html#vector-embeddings",
    "title": "Retrieval Augmented Generation",
    "section": "Vector Embeddings",
    "text": "Vector Embeddings\n\nVector embeddings represent words, phrases, sentences, or even entire documents as points in a high-dimensional vector space. The key idea is to map each textual element into a vector in such a way that semantically similar elements are located close to each other in this space, while dissimilar elements are further apart. This geometric representation facilitates similarity calculations, clustering, and other operations.\n\n\noften complex neural network: translating data into high.dimensional numerical vectors and effectively encode the data’s essential characteristics into vector embeddings\n\nExamples:\n\nWord Embeddings (Word2Vec, GloVe): Word embeddings represent individual words as vectors. For example, “king” and “queen” may be represented as vectors that are close together in the vector space because they share similar semantic properties.\nDocument Embeddings (Doc2Vec, BERT): Document embeddings map entire documents (such as PDFs) into vectors. Two documents discussing similar topics will have embeddings that are close in the vector space.\n\nhttps://weaviate.io/blog/vector-embeddings-explained\n\nSemantic/similarity search is key. And more advanced compared to common search methods like keyword searhc or synonym search\nThe process of generating a vector for a data object is called vectorization and is done by machine learning models\n\nWord2vec: family of model architectures with “dense” vectors (all values are non-zero). NN model to learn word associations from a large corpus of text. First creates vocabulary -&gt; learns vector representation (typically 300 dimensions). Is context agnostic.\nTransformer models (state-of-the-art, BERT, ELMo)",
    "crumbs": [
      "Wiki",
      "Python",
      "Retrueval Augmented Generation"
    ]
  },
  {
    "objectID": "wiki/Python/rag.html#prepare-pdf-extract-text",
    "href": "wiki/Python/rag.html#prepare-pdf-extract-text",
    "title": "Retrieval Augmented Generation",
    "section": "prepare PDF extract text",
    "text": "prepare PDF extract text\n\ntext extraction tools: pyPDF2, pdf2txt, PDFMiner\nScanned Documents: Optical Character Recognition (OCR) software\nQuality Control",
    "crumbs": [
      "Wiki",
      "Python",
      "Retrueval Augmented Generation"
    ]
  },
  {
    "objectID": "wiki/Python/rag.html#multiple-pages",
    "href": "wiki/Python/rag.html#multiple-pages",
    "title": "Retrieval Augmented Generation",
    "section": "Multiple Pages",
    "text": "Multiple Pages\n\npage segmentation: segment into logical units(paragraphs, section) ensure context is preserved\nmetadata extraction: document titles, authors, page numbers, creation dates.",
    "crumbs": [
      "Wiki",
      "Python",
      "Retrueval Augmented Generation"
    ]
  },
  {
    "objectID": "wiki/Python/rag.html#text-cleanup-and-normalization",
    "href": "wiki/Python/rag.html#text-cleanup-and-normalization",
    "title": "Retrieval Augmented Generation",
    "section": "Text Cleanup and Normalization",
    "text": "Text Cleanup and Normalization\n\nWhitespace and Punctuation\nFormatting removal\nSpellchecking",
    "crumbs": [
      "Wiki",
      "Python",
      "Retrueval Augmented Generation"
    ]
  },
  {
    "objectID": "wiki/Python/LLM.html#transfomer",
    "href": "wiki/Python/LLM.html#transfomer",
    "title": "LLM",
    "section": "Transfomer",
    "text": "Transfomer\n\nTransformer started out to address the limitations of traditional models like RNNs when dealing with variable-length sequences and context. The heart of the Transformer is its self-attention mechanism, allowing the model to focus on different parts of an input sequence. It comprises an encoder and a decoder. The encoder processes the input sequence to create hidden representations, while the decoder generates an output sequence. Each encoder and decoder layer includes multi-head attention and feed-forward neural networks. Multi-head attention, a key component, assigns weights to tokens based on relevance, enhancing the model’s performance in various NLP tasks. The Transformer’s inherent parallelizability minimizes inductive biases, making it ideal for large-scale pre-training and adaptability to different downstream tasks."
  },
  {
    "objectID": "wiki/Python/git.html",
    "href": "wiki/Python/git.html",
    "title": "git",
    "section": "",
    "text": "Show an old version of a file\nFirst show commit history of a specific file:\ngit log -- follow -- path/to/file\nNext copy commit id of the commit you want to see:\ngit checkout &lt;commitid&gt; -- src/foo.py\nGo back to latest commit version:\ngit checkout HEAD -- src/foo.py"
  },
  {
    "objectID": "wiki/Statistics/glm.html",
    "href": "wiki/Statistics/glm.html",
    "title": "GLM",
    "section": "",
    "text": "Depending on the outcome variable, we can use different types of models. For example, if the outcome variable is binary, we can use a logistic regression model. If the outcome variable is continuous, we can use a linear regression model. If the outcome variable is count data, we can use a Poisson regression model.\n\nBinary and binomial data naturally fall within the range of 0 to 1 (and are mathematically the same: binary = total number of trials is 1). Using a linear regression model would there predict values outside of this range. This would be a extrapolation which would make sense. Furthermore, in the case of logistic regression using a identity link function (i.e. linear regression) the predicted values would be heavily be influences by extreme values. Therefore, we adapt the linear regression based on attributes of the dependent variables. Additionally, we can’t assume normal distribution of errors for inference. Therefore, with this kind of data we assume other distributions as binomial distribution.\n\n\n\n\n\n\nNote\n\n\n\nNot all proportions are binomial. Binomial data always refer to a ration among two integers. Proportions between conitous variables are not binomial data. Binomial data is something that was tested several times and has only two possible outcomes. The number of trials is essential to define the precision of each observation. So we need independent trials with outcome 0 or 1.\n\n\n\n\n\n\n\n\nNote\n\n\n\nProportions between continous variables can be analysed with LM after transformation (i.e. arc-sin-square-root)\n\n\n\n\n\n\n\n\nNote\n\n\n\nBinomial distribution is a discrete probability distribution that describes the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success. Coin tosses.\n\n\nFor binary and binomial data we use a logit link function that bounds the y between 0 and 1. The logit link function is defined as:\n\\[logit(y) = log(\\frac{y}{1-y})\\]\nWe therefore model the logit of y. The model transforms probabilities into log-odds so we can use linear modeling\n\\[ logit(\\text{probability of success: p}) = log(\\frac{p}{1-p}) = \\beta_0 + x_1*\\beta_1...\\]\n\\[exp(log(\\frac{p}{1-p})) = \\frac{p}{1-p} = exp(\\beta_0) + exp(\\beta_1)\\]\n\n\n\n\n\n\nNote\n\n\n\n\nLogit transforms probabilities into something you can model linearly.\nExponentiation converts model coefficients back to a scale you can interpret (odds ratios).\nIf you want to interpret effects on probability, use the divide by 4 rule or predict actual probabilities.\n\n\n\nFor binomial data we use cbind(), for logistic regression we don’t use cbind().\n\nglm.model &lt;- glm(cbind(\"nr_of_successes\", \"nr_of_failures\") ~ \"predictor1\" + \"predictor2\", \n                 data = \"data\", \n                 family = \"binomial\")\n\n\nIs a categorical variable with more than two levels. For example, the outcome variable could be a categorical variable with three levels: “low”, “medium”, and “high”. In this case, we can use a multinomial logistic regression model. The multinomial logistic regression model is an extension of the binary logistic regression model that allows for more than two categories. The multinomial logistic regression model uses a softmax function to model the probabilities of each category. The softmax function converts a vector of raw scores (logits) into a probability distribution, ensuring the sum of probabilities for all classes equals one.\n\nlibrary(nnet)\nmultinom.model &lt;- multinom(\"outcome_variable\" ~ \"predictor1\" + \"predictor2\", \n                             data = \"data\")\n\n\nFor logistic regression we set a cutoff the classify the outcome variable. The cutoff is the probability threshold above which we classify the outcome variable as 1 (success) and below which we classify the outcome variable as 0 (failure). The default cutoff is 0.5, but we can set a different cutoff based on the specific problem and the cost of false positives and false negatives.\n\nglm.log &lt;- glm(\"outcome_variable\" ~ \"predictor1\" + \"predictor2\", \n                 data = \"data\", \n                 family = \"binomial\")\nclassification &lt;- ifelse(fitted(glm.log) &gt; 0.5, 1, 0)\n\nNow let’s compare fitted vs. observed (confusion matrix)\n\ncompare &lt;- data.frame(obs = df$obs,\n                      fitted = classification)\n# absolute\ntable(obs = compare$obs,\n      fitted = compare$fitted)\n# percentage\ntable(obs = compare$obs,\n      fitted = compare$fitted) %&gt;% \n  prop.table() %&gt;%\n  round(digits = 2)\n\nNaturally we would continue analysing different cutoff points with ROC and AUC.\n\nThis model handels count data which is often encountered in real life situations where y data doens’t follow a normal distribution. Using a Linear model to model count data can lead to following problems:\n\nThe predicted values can be negative, which is not possible for count data.\nThe predicted values are not integers, which is also not possible for count data.\nThe variance of count data naturally increases with the expected value (i.e. mean)\n\n\n\n\n\n\n\nNote\n\n\n\nMean-variance dependence: the variance of the data increases with the mean. This is not the case for normal distribution, where the variance is constant. Therefore heteroscedasticity is a problem when using linear models to model count data.\n\n\n\nChange the linear model formular so that we only predict positive values:\n\\[\\hat y = exp(\\hat \\beta_0 + \\hat \\beta_1*x_1) \\] this is equal to:\n\\[log(\\hat y) = \\hat \\beta_0 + \\hat \\beta_1*x_1\\]\nWe see that we can use natural logarithm as a link function. Next, we want y to be integer values and that the variance is dependent on the mean. Therefore, we use a Poisson distribution. The Poisson distribution is a discrete probability distribution that describes the number of events occurring in a fixed interval of time or space, given a known average rate of occurrence. The Poisson distribution is characterized by its mean (λ), which is also equal to its variance.\n\nglm.poisson &lt;- glm(\"outcome_variable\" ~ \"predictor1\" + \"predictor2\", \n                     data = \"data\", \n                     family = \"poisson\")\n\n\nDispersion parameter for poisson family taken to be 1 refers to the assumption that the variance increases linearly with the mean. In real life situations this is not the case. The variance is often larger than the mean. This is called overdispersion. In this case we can use a quasipoisson model.\nCompare Residual deviance with the degrees of freedom. If the residual deviance is larger than the degrees of freedom, then we have overdispersion.\n\nglm.poisson &lt;- glm(\"outcome_variable\" ~ \"predictor1\" + \"predictor2\", \n                     data = \"data\", \n                     family = \"quasipoisson\")\n\nThis imokies that the variance increases faster than linearly. The estimated coefficents are identifical but the standard error change and as a consequences the p-values change. ## GAM Extension of Linear Model to non-linearity\n\n\n\nCode#install.packages(\"faraway\")\nlibrary(faraway)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nglm.complaints &lt;- glm(complaints ~ . , data = esdcomp,\n                      family = \"poisson\")\n\nsummary(glm.complaints)\n\n\nCall:\nglm(formula = complaints ~ ., family = \"poisson\", data = esdcomp)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) -0.0803448  1.1542122  -0.070  0.94450   \nvisits       0.0009499  0.0003386   2.806  0.00502 **\nresidencyY  -0.2319740  0.2029388  -1.143  0.25301   \ngenderM      0.1122391  0.2235043   0.502  0.61554   \nrevenue     -0.0033827  0.0041553  -0.814  0.41560   \nhours       -0.0001569  0.0006634  -0.237  0.81298   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 89.447  on 43  degrees of freedom\nResidual deviance: 49.995  on 38  degrees of freedom\nAIC: 184.77\n\nNumber of Fisher Scoring iterations: 5\n\n\nSince we are modelling log(y) we have to exponentiate (inverse functions) the coefficients to get the multiplicative effect of a one unit increase in the predictor variable on the expected value of y.\n\n\nexp(coef(glm.complaints)[\"genderM\"]) %&gt;% round(digits = 2)\n\ngenderM \n   1.12 \n\n\n\n\n\n\n\n\nNote\n\n\n\nMales get on average 12% more complaints than women doctors\n\n\n\n\nexp(coef(glm.complaints)[\"visits\"]) %&gt;% round(digits = 5)\n\n visits \n1.00095 \n\n\n\n\n\n\n\n\nNote\n\n\n\nFor a given doctor, increasing its number of visits by one,would results in about 0.1% more complaints.\n\n\nIncrease in visist by one is not really interesting:\n\nrange(esdcomp$visits)\n\n[1]  879 3763\n\nexp(coef(glm.complaints)[\"visits\"]*50) %&gt;% round(digits = 5)\n\n visits \n1.04864 \n\n\n\n\n\n\n\n\nNote\n\n\n\nFor a given doctor, if you were to increase its number of visits by 50, then we expect this doctor to get about 4.86% more complaints\n\n\n\n\nglm.insects &lt;- glm(cbind(dead, alive)~ conc,\n                  family = \"binomial\",\n                  data = bliss)\n\nsummary(glm.insects)\n\n\nCall:\nglm(formula = cbind(dead, alive) ~ conc, family = \"binomial\", \n    data = bliss)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -2.3238     0.4179  -5.561 2.69e-08 ***\nconc          1.1619     0.1814   6.405 1.51e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 64.76327  on 4  degrees of freedom\nResidual deviance:  0.37875  on 3  degrees of freedom\nAIC: 20.854\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nexp(coef(glm.insects)[\"conc\"]) %&gt;% round(digits = 2)\n\nconc \n 3.2 \n\n\n\n\n\n\n\n\nNote\n\n\n\nBy increasing the concentration of the insecticide by one unit, we will obtain an increased risk in the odds of about 3 times. Where odds are the ratio of the probability of success (p) to the probability of failure (1-p). In this case success is the death of the insects.\n\n\n\n\nOdds of our model\n\nWith one unit increase in concentration, the odds of death increase by a factor of 3. This means that the odds of death are 3 times higher for each unit increase in concentration:\n\\[1:\\frac{1}{3} \\rightarrow 1:1 \\rightarrow 1:3\\]\n\n\nQuick interpretation of odds as upper bound increase in probability of y = 1.\n\nYou can add quasibinomial to the family argument. This will give you a dispersion parameter that is greater than 1. But only works for binomial data and not binary.",
    "crumbs": [
      "Wiki",
      "Statistics",
      "General Linear Model"
    ]
  },
  {
    "objectID": "wiki/Statistics/glm.html#binary-and-binomial-dependet-variable",
    "href": "wiki/Statistics/glm.html#binary-and-binomial-dependet-variable",
    "title": "GLM",
    "section": "",
    "text": "Binary and binomial data naturally fall within the range of 0 to 1 (and are mathematically the same: binary = total number of trials is 1). Using a linear regression model would there predict values outside of this range. This would be a extrapolation which would make sense. Furthermore, in the case of logistic regression using a identity link function (i.e. linear regression) the predicted values would be heavily be influences by extreme values. Therefore, we adapt the linear regression based on attributes of the dependent variables. Additionally, we can’t assume normal distribution of errors for inference. Therefore, with this kind of data we assume other distributions as binomial distribution.\n\n\n\n\n\n\nNote\n\n\n\nNot all proportions are binomial. Binomial data always refer to a ration among two integers. Proportions between conitous variables are not binomial data. Binomial data is something that was tested several times and has only two possible outcomes. The number of trials is essential to define the precision of each observation. So we need independent trials with outcome 0 or 1.\n\n\n\n\n\n\n\n\nNote\n\n\n\nProportions between continous variables can be analysed with LM after transformation (i.e. arc-sin-square-root)\n\n\n\n\n\n\n\n\nNote\n\n\n\nBinomial distribution is a discrete probability distribution that describes the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success. Coin tosses.\n\n\nFor binary and binomial data we use a logit link function that bounds the y between 0 and 1. The logit link function is defined as:\n\\[logit(y) = log(\\frac{y}{1-y})\\]\nWe therefore model the logit of y. The model transforms probabilities into log-odds so we can use linear modeling\n\\[ logit(\\text{probability of success: p}) = log(\\frac{p}{1-p}) = \\beta_0 + x_1*\\beta_1...\\]\n\\[exp(log(\\frac{p}{1-p})) = \\frac{p}{1-p} = exp(\\beta_0) + exp(\\beta_1)\\]\n\n\n\n\n\n\nNote\n\n\n\n\nLogit transforms probabilities into something you can model linearly.\nExponentiation converts model coefficients back to a scale you can interpret (odds ratios).\nIf you want to interpret effects on probability, use the divide by 4 rule or predict actual probabilities.\n\n\n\nFor binomial data we use cbind(), for logistic regression we don’t use cbind().\n\nglm.model &lt;- glm(cbind(\"nr_of_successes\", \"nr_of_failures\") ~ \"predictor1\" + \"predictor2\", \n                 data = \"data\", \n                 family = \"binomial\")",
    "crumbs": [
      "Wiki",
      "Statistics",
      "General Linear Model"
    ]
  },
  {
    "objectID": "wiki/Statistics/glm.html#multinomial-dependent-variable",
    "href": "wiki/Statistics/glm.html#multinomial-dependent-variable",
    "title": "GLM",
    "section": "",
    "text": "Is a categorical variable with more than two levels. For example, the outcome variable could be a categorical variable with three levels: “low”, “medium”, and “high”. In this case, we can use a multinomial logistic regression model. The multinomial logistic regression model is an extension of the binary logistic regression model that allows for more than two categories. The multinomial logistic regression model uses a softmax function to model the probabilities of each category. The softmax function converts a vector of raw scores (logits) into a probability distribution, ensuring the sum of probabilities for all classes equals one.\n\nlibrary(nnet)\nmultinom.model &lt;- multinom(\"outcome_variable\" ~ \"predictor1\" + \"predictor2\", \n                             data = \"data\")\n\n\nFor logistic regression we set a cutoff the classify the outcome variable. The cutoff is the probability threshold above which we classify the outcome variable as 1 (success) and below which we classify the outcome variable as 0 (failure). The default cutoff is 0.5, but we can set a different cutoff based on the specific problem and the cost of false positives and false negatives.\n\nglm.log &lt;- glm(\"outcome_variable\" ~ \"predictor1\" + \"predictor2\", \n                 data = \"data\", \n                 family = \"binomial\")\nclassification &lt;- ifelse(fitted(glm.log) &gt; 0.5, 1, 0)\n\nNow let’s compare fitted vs. observed (confusion matrix)\n\ncompare &lt;- data.frame(obs = df$obs,\n                      fitted = classification)\n# absolute\ntable(obs = compare$obs,\n      fitted = compare$fitted)\n# percentage\ntable(obs = compare$obs,\n      fitted = compare$fitted) %&gt;% \n  prop.table() %&gt;%\n  round(digits = 2)\n\nNaturally we would continue analysing different cutoff points with ROC and AUC.",
    "crumbs": [
      "Wiki",
      "Statistics",
      "General Linear Model"
    ]
  },
  {
    "objectID": "wiki/Statistics/glm.html#poisson-regression",
    "href": "wiki/Statistics/glm.html#poisson-regression",
    "title": "GLM",
    "section": "",
    "text": "This model handels count data which is often encountered in real life situations where y data doens’t follow a normal distribution. Using a Linear model to model count data can lead to following problems:\n\nThe predicted values can be negative, which is not possible for count data.\nThe predicted values are not integers, which is also not possible for count data.\nThe variance of count data naturally increases with the expected value (i.e. mean)\n\n\n\n\n\n\n\nNote\n\n\n\nMean-variance dependence: the variance of the data increases with the mean. This is not the case for normal distribution, where the variance is constant. Therefore heteroscedasticity is a problem when using linear models to model count data.\n\n\n\nChange the linear model formular so that we only predict positive values:\n\\[\\hat y = exp(\\hat \\beta_0 + \\hat \\beta_1*x_1) \\] this is equal to:\n\\[log(\\hat y) = \\hat \\beta_0 + \\hat \\beta_1*x_1\\]\nWe see that we can use natural logarithm as a link function. Next, we want y to be integer values and that the variance is dependent on the mean. Therefore, we use a Poisson distribution. The Poisson distribution is a discrete probability distribution that describes the number of events occurring in a fixed interval of time or space, given a known average rate of occurrence. The Poisson distribution is characterized by its mean (λ), which is also equal to its variance.\n\nglm.poisson &lt;- glm(\"outcome_variable\" ~ \"predictor1\" + \"predictor2\", \n                     data = \"data\", \n                     family = \"poisson\")\n\n\nDispersion parameter for poisson family taken to be 1 refers to the assumption that the variance increases linearly with the mean. In real life situations this is not the case. The variance is often larger than the mean. This is called overdispersion. In this case we can use a quasipoisson model.\nCompare Residual deviance with the degrees of freedom. If the residual deviance is larger than the degrees of freedom, then we have overdispersion.\n\nglm.poisson &lt;- glm(\"outcome_variable\" ~ \"predictor1\" + \"predictor2\", \n                     data = \"data\", \n                     family = \"quasipoisson\")\n\nThis imokies that the variance increases faster than linearly. The estimated coefficents are identifical but the standard error change and as a consequences the p-values change. ## GAM Extension of Linear Model to non-linearity",
    "crumbs": [
      "Wiki",
      "Statistics",
      "General Linear Model"
    ]
  },
  {
    "objectID": "wiki/Statistics/glm.html#gam",
    "href": "wiki/Statistics/glm.html#gam",
    "title": "GLM",
    "section": "",
    "text": "Extension of Linear Model to non-linearity"
  },
  {
    "objectID": "wiki/Statistics/glm.html#interpretation-of-coefficients",
    "href": "wiki/Statistics/glm.html#interpretation-of-coefficients",
    "title": "GLM",
    "section": "",
    "text": "Code#install.packages(\"faraway\")\nlibrary(faraway)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nglm.complaints &lt;- glm(complaints ~ . , data = esdcomp,\n                      family = \"poisson\")\n\nsummary(glm.complaints)\n\n\nCall:\nglm(formula = complaints ~ ., family = \"poisson\", data = esdcomp)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) -0.0803448  1.1542122  -0.070  0.94450   \nvisits       0.0009499  0.0003386   2.806  0.00502 **\nresidencyY  -0.2319740  0.2029388  -1.143  0.25301   \ngenderM      0.1122391  0.2235043   0.502  0.61554   \nrevenue     -0.0033827  0.0041553  -0.814  0.41560   \nhours       -0.0001569  0.0006634  -0.237  0.81298   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 89.447  on 43  degrees of freedom\nResidual deviance: 49.995  on 38  degrees of freedom\nAIC: 184.77\n\nNumber of Fisher Scoring iterations: 5\n\n\nSince we are modelling log(y) we have to exponentiate (inverse functions) the coefficients to get the multiplicative effect of a one unit increase in the predictor variable on the expected value of y.\n\n\nexp(coef(glm.complaints)[\"genderM\"]) %&gt;% round(digits = 2)\n\ngenderM \n   1.12 \n\n\n\n\n\n\n\n\nNote\n\n\n\nMales get on average 12% more complaints than women doctors\n\n\n\n\nexp(coef(glm.complaints)[\"visits\"]) %&gt;% round(digits = 5)\n\n visits \n1.00095 \n\n\n\n\n\n\n\n\nNote\n\n\n\nFor a given doctor, increasing its number of visits by one,would results in about 0.1% more complaints.\n\n\nIncrease in visist by one is not really interesting:\n\nrange(esdcomp$visits)\n\n[1]  879 3763\n\nexp(coef(glm.complaints)[\"visits\"]*50) %&gt;% round(digits = 5)\n\n visits \n1.04864 \n\n\n\n\n\n\n\n\nNote\n\n\n\nFor a given doctor, if you were to increase its number of visits by 50, then we expect this doctor to get about 4.86% more complaints\n\n\n\n\nglm.insects &lt;- glm(cbind(dead, alive)~ conc,\n                  family = \"binomial\",\n                  data = bliss)\n\nsummary(glm.insects)\n\n\nCall:\nglm(formula = cbind(dead, alive) ~ conc, family = \"binomial\", \n    data = bliss)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -2.3238     0.4179  -5.561 2.69e-08 ***\nconc          1.1619     0.1814   6.405 1.51e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 64.76327  on 4  degrees of freedom\nResidual deviance:  0.37875  on 3  degrees of freedom\nAIC: 20.854\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nexp(coef(glm.insects)[\"conc\"]) %&gt;% round(digits = 2)\n\nconc \n 3.2 \n\n\n\n\n\n\n\n\nNote\n\n\n\nBy increasing the concentration of the insecticide by one unit, we will obtain an increased risk in the odds of about 3 times. Where odds are the ratio of the probability of success (p) to the probability of failure (1-p). In this case success is the death of the insects.\n\n\n\n\nOdds of our model\n\nWith one unit increase in concentration, the odds of death increase by a factor of 3. This means that the odds of death are 3 times higher for each unit increase in concentration:\n\\[1:\\frac{1}{3} \\rightarrow 1:1 \\rightarrow 1:3\\]\n\n\nQuick interpretation of odds as upper bound increase in probability of y = 1.\n\nYou can add quasibinomial to the family argument. This will give you a dispersion parameter that is greater than 1. But only works for binomial data and not binary.",
    "crumbs": [
      "Wiki",
      "Statistics",
      "General Linear Model"
    ]
  },
  {
    "objectID": "wiki/Statistics/dl.html",
    "href": "wiki/Statistics/dl.html",
    "title": "dl",
    "section": "",
    "text": "Goal: predict a label or distribution over labels for a given image (probabilty)\nOften width (pixel) x height (pixel) x 3 (red, green, blue) arrays,\nflattend to one large array where each value is an integer from 0 (black) to 255 (white)\n\nSome challenges for computer vision algorithm (must be invariant to cross product of these while retaining sensitivity to inter-class variations): - Viewpoint variation (orientation of image) - Scale variation (variation of size in image and real life) - Deformation (objects are not rigid bodies) - Occlusion (object can be hidden) - Illumination conditions (effects are drastic on pixel level) - Background clutter (objects may blend into environment) - Intra-class variation (many different types)\nRow Major Order\n\n\nTakes test image (array) and compares it to every image (array) in training set and takes image label of closest one. It simply remembers all training data - With k = 1 we create “small islands of liekly incorrect predictions, always use higher k\n\n\nL1 distance/norm: Sum of over all absolute pixel differences \\(d_1(A,B) = \\sum_i|A_i-B_i|\\)\nIf \\(A = B\\) than L1 will be zero.\n\n\nCode\nclass NearestNeighbor():\n  def __init__(self):\n    pass\n  \n  def train(self, X, y):\n    \"\"\" X is N x D where N is number of examples and D flattend pixel array.\n    Y is 1-dimension of size N.\n    Training is just remembering all data\"\"\"\n    self.X_train = X \n    self.y_train = y\n  \n  def predict(self, X):\n    \"\"\" X is N x D where each row is an example\n    we wish to predict label for \"\"\"\n    num_test = X.shape[0]\n    # lets make sure that the output type\n    # matches the input type\n    Ypred = np.zeros(num_test, dtype = self.y_train.dtype)\n    # loop over all test rows\n    for i in range(num_test):\n      #L1\n      distances = np.sum(np.abs(self.X_train - X[i,:]), axis = 1)\n      # get the index with smallest distance\n      min_index = np.argmin(distances)\n      # predict the label of the nearest example\n      Ypred[i] = self.y_train[min_index]\n    return Ypred\n  \n#------ Apply\nnn = NearestNeighbor()\nnn.train(X_train, y_train)\ny_test_predict = nn.predict(X_test)\n\nprint(f'accuracy: {np.mean(y_test_predict == y_test)}')\n\n\nL2 distance/norm euclidean distance between two vectors. Each pixelwise difference is squared, summed up and finally the square root is taken:\n\\(d_2(A,B) = \\sqrt{\\sum_i(A_i-B_i)^2}\\)\n\n\nCode\n# new distance L2: we could leave out np.sqrt and get the same final output (monotonic function)\ndistances = np.sqrt(np.sum(np.square(self.X_train - X[i,:]), axis = 1))\n\n\nDifference L1 , L2 - L2 unforgiving: prefers many medium disagreements to one big one.\n\n\n\n\nFind top k closets images and take “majority vote” of labels. Higher k-values have a smoothing effect and make classifier more resistant to outliers. - Training is very fast - Predicting is slow since it compares x_test to each x_training - In practice we want efficient predictions!\n\n\nCode\nclass KNearestNeighbor():\n  \"\"\" a kNN classifier with L2 distance \"\"\"\n  \n  def __init__(self):\n    pass\n  \n  def train(self, X, y):\n    \"\"\" same as before \"\"\"\n    \n  def predit(self, X, k = 1):\n    \"\"\" returns output of methods: compute_distances and predict_labels\"\"\" \n    \n    return self.predict_labels(dists, k = k)\n  \n  def compute_distacnes(self, X): \n    \"\"\" L2 Distance: for each image in test set X  calculate the L2 distance to each images in X_train. Return a matrix dists where each row is a test image and each column the L2 of corresponding train image\"\"\" \n    \n    num_test = X.shape[0]\n    num_train = self.X_train.shape[0]\n    dists = np.zeros((num_test, num_train))\n\n    for i in range(num_test):\n      dists[i, :] = np.sqrt(np.sum(np.square(\n                            self.X_train - X[i,:]),\n                            axis = 1))\n    return dists\n  \n  def predict_labels(self, dists, k =1): \n    \"\"\"Returns: array with predicted labels \"\"\"\n    \n    num_test = dists.shape[0]\n    y_pred = np.zeros(num_test)\n    \n    for i in range(num_test):\n\n      closest_y = []\n      # get the k indices with smallest distances\n      min_indices = np.argsort(dists[i,:])[:k]\n      closest_y = np.bincount(self.y_train[min_indices])\n      # predict the label of the nearest example\n      y_pred[i] = np.argmax(closest_y)\n    return y_pred\n\n\n\nWe could speed up with broadcasting rules (see lecture notes)\ngray regions in visualization of results are caused by ties in majority vote\n\n\n\n\n\nHyperparameters = choiches needed to be made (distances, k, etc.)\nTry out different hyperparameters and use best\nWe further take a small part of training set (e.g. 1000) as validation set which is used as a “fake test set” to tune the hyperparameter\n\n\n\nCode\nX_val = X_train[:1000, :]\ny_val = y_train[:1000]\n\nX_train = X_train[1000:, :]\ny_train = y_train[1000:]\n\nvalidation_accuracies = []\nfor k in [1, 3, 5, 10, 20, 50, 100]:\n  # use a particular value of k and evaluation on validation data\n  nn = NearestNeighbor()\n  nn.train(X_train, y_train)\n  # here we assume a modified NearestNeighbor class that can\n  # take a k as input\n  y_val_predict = nn.predict(X_val, k = k)\n  acc = np.mean(y_val_predict == y_val)\n  print('accuracy: %f' % (acc,))\n  # keep track of what works on the validation set\n  validation_accuracies.append((k, acc))\n\n\n\n\n\niterate over different validiation sets and average the performance across these\n5-fold: split training into 5 parts, iterate 5 times while changing validation part\nIn practice it is cleaner to no include the validation set in the final training of the model after determining the best hyperparameters\n\n\n\nCode\n# k-fold cross validation\nnum_folds = 5\nk_choices = [1, 3, 5, 7, 9, 10, 12, 15, 18, 20, 50, 100]\nX_train_folds = []\ny_train_folds = []\n# Split up the training data into folds. After splitting,\n# X_train_folds and y_train_folds should each be lists of\n# length num_folds, where y_train_folds[i] is the label\n# vector for the points in X_train_folds[i]\nX_train_folds = np.split(X_train, [1000, 2000, 3000, 4000])\ny_train_folds = np.split(y_train, [1000, 2000, 3000, 4000])\n# A dictionary holding the accuracies for different values of\n# k that we find when running cross-validation. After running\n# cross-validation, k_to_accuracies[k] should be a list of\n# length num_folds giving the different accuracy values that\n\n# we found when using that value of k.\nk_to_accuracies = {}\n# We perform k-fold cross validation to find the best value of k.\n# For each possible value of k, run the k-nearest-neighbor algorithm\n# num_folds times, where in each case you use all but one of the folds\n# as training data and the last fold as a validation set. Store the\n# accuracies for all fold and all values of k in the k_to_accuracies\n# dictionary.\n\nfor k in k_choices:\n  \n  k_to_accuracies[k] = []\n  classifier = KNearestNeighbor()\n  for i in range(num_folds):\n    #we use ith fold as validation set  \n    X_cv_training = np.concatenate([x for k, x in\n    enumerate(X_train_folds) if k!=i], axis=0)\n    \n    y_cv_training = np.concatenate([x for k, x in\n      enumerate(y_train_folds) if k!=i], axis=0)\n    \n    classifier.train(X_cv_training, y_cv_training)\n    dists = classifier.compute_distances_one_loop(X_train_folds[i])\n    y_test_pred = classifier.predict_labels(dists, k=k)\n    k_to_accuracies[k].append(np.mean(y_train_folds[i] == y_test_pred))\n\n\n\nin practice on may want to avoid cross-validation in favor of single validation split (computationally expensive)\nSize of single split of training data depends how many hyperparameters\nFor example if the number of hyperparameters is large you may prefer to use bigger validation splits. If the number of examples in the validation set is small (perhaps only a few hundred or so), it is safer to use cross-validation. Typical number of folds you can see in practice would be 3-fold, 5-fold or 10-fold cross-validation.",
    "crumbs": [
      "Wiki",
      "Statistics",
      "Deep Learning Python"
    ]
  },
  {
    "objectID": "wiki/Statistics/dl.html#image-classification",
    "href": "wiki/Statistics/dl.html#image-classification",
    "title": "dl",
    "section": "",
    "text": "Goal: predict a label or distribution over labels for a given image (probabilty)\nOften width (pixel) x height (pixel) x 3 (red, green, blue) arrays,\nflattend to one large array where each value is an integer from 0 (black) to 255 (white)\n\nSome challenges for computer vision algorithm (must be invariant to cross product of these while retaining sensitivity to inter-class variations): - Viewpoint variation (orientation of image) - Scale variation (variation of size in image and real life) - Deformation (objects are not rigid bodies) - Occlusion (object can be hidden) - Illumination conditions (effects are drastic on pixel level) - Background clutter (objects may blend into environment) - Intra-class variation (many different types)\nRow Major Order\n\n\nTakes test image (array) and compares it to every image (array) in training set and takes image label of closest one. It simply remembers all training data - With k = 1 we create “small islands of liekly incorrect predictions, always use higher k\n\n\nL1 distance/norm: Sum of over all absolute pixel differences \\(d_1(A,B) = \\sum_i|A_i-B_i|\\)\nIf \\(A = B\\) than L1 will be zero.\n\n\nCode\nclass NearestNeighbor():\n  def __init__(self):\n    pass\n  \n  def train(self, X, y):\n    \"\"\" X is N x D where N is number of examples and D flattend pixel array.\n    Y is 1-dimension of size N.\n    Training is just remembering all data\"\"\"\n    self.X_train = X \n    self.y_train = y\n  \n  def predict(self, X):\n    \"\"\" X is N x D where each row is an example\n    we wish to predict label for \"\"\"\n    num_test = X.shape[0]\n    # lets make sure that the output type\n    # matches the input type\n    Ypred = np.zeros(num_test, dtype = self.y_train.dtype)\n    # loop over all test rows\n    for i in range(num_test):\n      #L1\n      distances = np.sum(np.abs(self.X_train - X[i,:]), axis = 1)\n      # get the index with smallest distance\n      min_index = np.argmin(distances)\n      # predict the label of the nearest example\n      Ypred[i] = self.y_train[min_index]\n    return Ypred\n  \n#------ Apply\nnn = NearestNeighbor()\nnn.train(X_train, y_train)\ny_test_predict = nn.predict(X_test)\n\nprint(f'accuracy: {np.mean(y_test_predict == y_test)}')\n\n\nL2 distance/norm euclidean distance between two vectors. Each pixelwise difference is squared, summed up and finally the square root is taken:\n\\(d_2(A,B) = \\sqrt{\\sum_i(A_i-B_i)^2}\\)\n\n\nCode\n# new distance L2: we could leave out np.sqrt and get the same final output (monotonic function)\ndistances = np.sqrt(np.sum(np.square(self.X_train - X[i,:]), axis = 1))\n\n\nDifference L1 , L2 - L2 unforgiving: prefers many medium disagreements to one big one.\n\n\n\n\nFind top k closets images and take “majority vote” of labels. Higher k-values have a smoothing effect and make classifier more resistant to outliers. - Training is very fast - Predicting is slow since it compares x_test to each x_training - In practice we want efficient predictions!\n\n\nCode\nclass KNearestNeighbor():\n  \"\"\" a kNN classifier with L2 distance \"\"\"\n  \n  def __init__(self):\n    pass\n  \n  def train(self, X, y):\n    \"\"\" same as before \"\"\"\n    \n  def predit(self, X, k = 1):\n    \"\"\" returns output of methods: compute_distances and predict_labels\"\"\" \n    \n    return self.predict_labels(dists, k = k)\n  \n  def compute_distacnes(self, X): \n    \"\"\" L2 Distance: for each image in test set X  calculate the L2 distance to each images in X_train. Return a matrix dists where each row is a test image and each column the L2 of corresponding train image\"\"\" \n    \n    num_test = X.shape[0]\n    num_train = self.X_train.shape[0]\n    dists = np.zeros((num_test, num_train))\n\n    for i in range(num_test):\n      dists[i, :] = np.sqrt(np.sum(np.square(\n                            self.X_train - X[i,:]),\n                            axis = 1))\n    return dists\n  \n  def predict_labels(self, dists, k =1): \n    \"\"\"Returns: array with predicted labels \"\"\"\n    \n    num_test = dists.shape[0]\n    y_pred = np.zeros(num_test)\n    \n    for i in range(num_test):\n\n      closest_y = []\n      # get the k indices with smallest distances\n      min_indices = np.argsort(dists[i,:])[:k]\n      closest_y = np.bincount(self.y_train[min_indices])\n      # predict the label of the nearest example\n      y_pred[i] = np.argmax(closest_y)\n    return y_pred\n\n\n\nWe could speed up with broadcasting rules (see lecture notes)\ngray regions in visualization of results are caused by ties in majority vote\n\n\n\n\n\nHyperparameters = choiches needed to be made (distances, k, etc.)\nTry out different hyperparameters and use best\nWe further take a small part of training set (e.g. 1000) as validation set which is used as a “fake test set” to tune the hyperparameter\n\n\n\nCode\nX_val = X_train[:1000, :]\ny_val = y_train[:1000]\n\nX_train = X_train[1000:, :]\ny_train = y_train[1000:]\n\nvalidation_accuracies = []\nfor k in [1, 3, 5, 10, 20, 50, 100]:\n  # use a particular value of k and evaluation on validation data\n  nn = NearestNeighbor()\n  nn.train(X_train, y_train)\n  # here we assume a modified NearestNeighbor class that can\n  # take a k as input\n  y_val_predict = nn.predict(X_val, k = k)\n  acc = np.mean(y_val_predict == y_val)\n  print('accuracy: %f' % (acc,))\n  # keep track of what works on the validation set\n  validation_accuracies.append((k, acc))\n\n\n\n\n\niterate over different validiation sets and average the performance across these\n5-fold: split training into 5 parts, iterate 5 times while changing validation part\nIn practice it is cleaner to no include the validation set in the final training of the model after determining the best hyperparameters\n\n\n\nCode\n# k-fold cross validation\nnum_folds = 5\nk_choices = [1, 3, 5, 7, 9, 10, 12, 15, 18, 20, 50, 100]\nX_train_folds = []\ny_train_folds = []\n# Split up the training data into folds. After splitting,\n# X_train_folds and y_train_folds should each be lists of\n# length num_folds, where y_train_folds[i] is the label\n# vector for the points in X_train_folds[i]\nX_train_folds = np.split(X_train, [1000, 2000, 3000, 4000])\ny_train_folds = np.split(y_train, [1000, 2000, 3000, 4000])\n# A dictionary holding the accuracies for different values of\n# k that we find when running cross-validation. After running\n# cross-validation, k_to_accuracies[k] should be a list of\n# length num_folds giving the different accuracy values that\n\n# we found when using that value of k.\nk_to_accuracies = {}\n# We perform k-fold cross validation to find the best value of k.\n# For each possible value of k, run the k-nearest-neighbor algorithm\n# num_folds times, where in each case you use all but one of the folds\n# as training data and the last fold as a validation set. Store the\n# accuracies for all fold and all values of k in the k_to_accuracies\n# dictionary.\n\nfor k in k_choices:\n  \n  k_to_accuracies[k] = []\n  classifier = KNearestNeighbor()\n  for i in range(num_folds):\n    #we use ith fold as validation set  \n    X_cv_training = np.concatenate([x for k, x in\n    enumerate(X_train_folds) if k!=i], axis=0)\n    \n    y_cv_training = np.concatenate([x for k, x in\n      enumerate(y_train_folds) if k!=i], axis=0)\n    \n    classifier.train(X_cv_training, y_cv_training)\n    dists = classifier.compute_distances_one_loop(X_train_folds[i])\n    y_test_pred = classifier.predict_labels(dists, k=k)\n    k_to_accuracies[k].append(np.mean(y_train_folds[i] == y_test_pred))\n\n\n\nin practice on may want to avoid cross-validation in favor of single validation split (computationally expensive)\nSize of single split of training data depends how many hyperparameters\nFor example if the number of hyperparameters is large you may prefer to use bigger validation splits. If the number of examples in the validation set is small (perhaps only a few hundred or so), it is safer to use cross-validation. Typical number of folds you can see in practice would be 3-fold, 5-fold or 10-fold cross-validation.",
    "crumbs": [
      "Wiki",
      "Statistics",
      "Deep Learning Python"
    ]
  },
  {
    "objectID": "wiki/Statistics/modeldiagnostic.html",
    "href": "wiki/Statistics/modeldiagnostic.html",
    "title": "modeldiagnostic",
    "section": "",
    "text": "Shows whether residuals have non-linear patterns.\nEqual spread of residual around horizontal line without distinct patters indication of non-linear relationship\n\n\n\n\n\nShows residuals are normally distributed\n\n\n\n\n\nResiduals spread eqaully along the ranges of predictors (homoscedasticity)\nHorizontal line with equally randomly spread points\n\n\n\n\n\nThis plot helps us to find influential cases (i.e., subjects) if there are any.\nWe watch out for outlying values at the upper right corner or at the lower right corner. Those spots are the places where cases can be influential against a regression line. Look for cases outside of the dashed lines. When cases are outside of the dashed lines (meaning they have high “Cook’s distance” scores), the cases are influential to the regression results. The regression results will be altered if we exclude those cases."
  },
  {
    "objectID": "wiki/Statistics/modeldiagnostic.html#residuals-vs-fitted",
    "href": "wiki/Statistics/modeldiagnostic.html#residuals-vs-fitted",
    "title": "modeldiagnostic",
    "section": "",
    "text": "Shows whether residuals have non-linear patterns.\nEqual spread of residual around horizontal line without distinct patters indication of non-linear relationship"
  },
  {
    "objectID": "wiki/Statistics/modeldiagnostic.html#q-q-plot",
    "href": "wiki/Statistics/modeldiagnostic.html#q-q-plot",
    "title": "modeldiagnostic",
    "section": "",
    "text": "Shows residuals are normally distributed"
  },
  {
    "objectID": "wiki/Statistics/modeldiagnostic.html#scale-location-plot",
    "href": "wiki/Statistics/modeldiagnostic.html#scale-location-plot",
    "title": "modeldiagnostic",
    "section": "",
    "text": "Residuals spread eqaully along the ranges of predictors (homoscedasticity)\nHorizontal line with equally randomly spread points"
  },
  {
    "objectID": "wiki/Statistics/modeldiagnostic.html#residuals-vs-leverage",
    "href": "wiki/Statistics/modeldiagnostic.html#residuals-vs-leverage",
    "title": "modeldiagnostic",
    "section": "",
    "text": "This plot helps us to find influential cases (i.e., subjects) if there are any.\nWe watch out for outlying values at the upper right corner or at the lower right corner. Those spots are the places where cases can be influential against a regression line. Look for cases outside of the dashed lines. When cases are outside of the dashed lines (meaning they have high “Cook’s distance” scores), the cases are influential to the regression results. The regression results will be altered if we exclude those cases."
  },
  {
    "objectID": "wiki/Statistics/dl.html#nearest-neighbor-classifier-k-1",
    "href": "wiki/Statistics/dl.html#nearest-neighbor-classifier-k-1",
    "title": "",
    "section": "Nearest Neighbor Classifier (k = 1)",
    "text": "Nearest Neighbor Classifier (k = 1)\nTakes test image (array) and compares it to every image (array) in training set and takes image label of closest one. It simply remembers all training data\n\nWith k = 1 we create “small islands of liekly incorrect predictions, always use higher k\n\n\nHow compare to arrays A and B?\nL1 distance/norm: Sum of over all absolute pixel differences \\(d_1(A,B) = \\sum_i|A_i-B_i|\\) If \\(A = B\\) than L1 will be zero.\n\n\nCode\nclass NearestNeighbor():\n  def __init__(self):\n    pass\n  def train(self, X, y):\n    \"\"\" X is N x D where N is number of examples and D flattend pixel array.\n    Y is 1-dimension of size N.\n    Training is just remembering all data\"\"\"\n    self.X_train = X \n    self.y_train = y\n  \n  def predict(self, X):\n    \"\"\" X is N x D where each row is an example\n    we wish to predict label for \"\"\"\n    num_test = X.shape[0]\n    # lets make sure that the output type\n    # matches the input type\n    Ypred = np.zeros(num_test, dtype = self.y_train.dtype)\n    # loop over all test rows\n    for i in range(num_test):\n      #L1\n      distances = np.sum(np.abs(self.X_train - X[i,:]), axis = 1)\n      # get the index with smallest distance\n      min_index = np.argmin(distances)\n      # predict the label of the nearest example\n      Ypred[i] = self.y_train[min_index]\n    return Ypred\nnn = NearestNeighbor()\nnn.train(X_train, y_train)\ny_test_predict = nn.predict(X_test)\nprint(f'accuracy: {np.mean(y_test_predict == y_test)}')\n\n\nL2 distance/norm: euclidean distance between two vectors. Each pixelwise difference is squared, summed up and finally the square root is taken:\n\\(d_2(A,B) = \\sqrt{\\sum_i(A_i-B_i)^2}\\)\n\n\nCode\n# new distance L2: we could leave out np.sqrt and get the same final output (monotonic function)\ndistances = np.sqrt(np.sum(np.square(self.X_train - X[i,:]), axis = 1))\n\n\nDifference L1 , L2 - L2 unforgiving: prefers many medium disagreements to one big one."
  },
  {
    "objectID": "wiki/Statistics/dl.html#k-nearest-neighbor-classifier",
    "href": "wiki/Statistics/dl.html#k-nearest-neighbor-classifier",
    "title": "",
    "section": "k-Nearest Neighbor Classifier",
    "text": "k-Nearest Neighbor Classifier\nFind top k closets images and take “majority vote” of labels. Higher k-values have a smoothing effect and make classifier more resistant to outliers. - Training is very fast - Predicting is slow since it compares x_test to each x_training - In practice we want efficient predictions! - Space inefficient\n\n\nCode\nclass KNearestNeighbor():\n  \"\"\" a kNN classifier with L2 distance \"\"\"\n  \n  def __init__(self):\n    pass\n  \n  def train(self, X, y):\n    \"\"\" same as before \"\"\"\n    \n  def predit(self, X, k = 1):\n    \"\"\" returns output of methods: compute_distances and predict_labels\"\"\" \n    \n    return self.predict_labels(dists, k = k)\n  \n  def compute_distacnes(self, X): \n    \"\"\" L2 Distance: for each image in test set X  calculate the L2 distance to each images in X_train. Return a matrix dists where each row is a test image and each column the L2 of corresponding train image\"\"\" \n    \n    num_test = X.shape[0]\n    num_train = self.X_train.shape[0]\n    dists = np.zeros((num_test, num_train))\n\n    for i in range(num_test):\n      dists[i, :] = np.sqrt(np.sum(np.square(\n                            self.X_train - X[i,:]),\n                            axis = 1))\n    return dists\n  \n  def predict_labels(self, dists, k =1): \n    \"\"\"Returns: array with predicted labels \"\"\"\n    \n    num_test = dists.shape[0]\n    y_pred = np.zeros(num_test)\n    \n    for i in range(num_test):\n\n      closest_y = []\n      # get the k indices with smallest distances\n      min_indices = np.argsort(dists[i,:])[:k]\n      closest_y = np.bincount(self.y_train[min_indices])\n      # predict the label of the nearest example\n      y_pred[i] = np.argmax(closest_y)\n    return y_pred\n\n\n\nWe could speed up with broadcasting rules (see lecture notes)\ngray regions in visualization of results are caused by ties in majority vote\n\n\nk-fold Cross Validation (what k to chose?)\n\nHyperparameters = choiches needed to be made (distances, k, etc.)\nTry out different hyperparameters and use best\nWe further take a small part of training set (e.g. 1000) as validation set which is used as a “fake test set” to tune the hyperparameter\n\n\n\nCode\nX_val = X_train[:1000, :]\ny_val = y_train[:1000]\n\nX_train = X_train[1000:, :]\ny_train = y_train[1000:]\n\nvalidation_accuracies = []\nfor k in [1, 3, 5, 10, 20, 50, 100]:\n  # use a particular value of k and evaluation on validation data\n  nn = NearestNeighbor()\n  nn.train(X_train, y_train)\n  # here we assume a modified NearestNeighbor class that can\n  # take a k as input\n  y_val_predict = nn.predict(X_val, k = k)\n  acc = np.mean(y_val_predict == y_val)\n  print('accuracy: %f' % (acc,))\n  # keep track of what works on the validation set\n  validation_accuracies.append((k, acc))\n\n\nWhen training data is smaller or computation is not heavy\n\niterate over different validiation sets and average the performance across these\n5-fold: split training into 5 parts, iterate 5 times while changing validation part\nIn practice it is cleaner to no include the validation set in the final training of the model after determining the best hyperparameters\n\n\n\nCode\n# k-fold cross validation\nnum_folds = 5\nk_choices = [1, 3, 5, 7, 9, 10, 12, 15, 18, 20, 50, 100]\nX_train_folds = []\ny_train_folds = []\n# Split up the training data into folds. After splitting,\n# X_train_folds and y_train_folds should each be lists of\n# length num_folds, where y_train_folds[i] is the label\n# vector for the points in X_train_folds[i]\nX_train_folds = np.split(X_train, [1000, 2000, 3000, 4000])\ny_train_folds = np.split(y_train, [1000, 2000, 3000, 4000])\n# A dictionary holding the accuracies for different values of\n# k that we find when running cross-validation. After running\n# cross-validation, k_to_accuracies[k] should be a list of\n# length num_folds giving the different accuracy values that\n\n# we found when using that value of k.\nk_to_accuracies = {}\n# We perform k-fold cross validation to find the best value of k.\n# For each possible value of k, run the k-nearest-neighbor algorithm\n# num_folds times, where in each case you use all but one of the folds\n# as training data and the last fold as a validation set. Store the\n# accuracies for all fold and all values of k in the k_to_accuracies\n# dictionary.\n\nfor k in k_choices:\n  \n  k_to_accuracies[k] = []\n  classifier = KNearestNeighbor()\n  for i in range(num_folds):\n    #we use ith fold as validation set  \n    X_cv_training = np.concatenate([x for k, x in\n    enumerate(X_train_folds) if k!=i], axis=0)\n    \n    y_cv_training = np.concatenate([x for k, x in\n      enumerate(y_train_folds) if k!=i], axis=0)\n    \n    classifier.train(X_cv_training, y_cv_training)\n    dists = classifier.compute_distances_one_loop(X_train_folds[i])\n    y_test_pred = classifier.predict_labels(dists, k=k)\n    k_to_accuracies[k].append(np.mean(y_train_folds[i] == y_test_pred))\n\n\n\nin practice on may want to avoid cross-validation in favor of single validation split (computationally expensive)\nSize of single split of training data depends how many hyperparameters\nFor example if the number of hyperparameters is large you may prefer to use bigger validation splits. If the number of examples in the validation set is small (perhaps only a few hundred or so), it is safer to use cross-validation. Typical number of folds you can see in practice would be 3-fold, 5-fold or 10-fold cross-validation."
  },
  {
    "objectID": "wiki/Statistics/dl.html#linear-classificaiton-of-images",
    "href": "wiki/Statistics/dl.html#linear-classificaiton-of-images",
    "title": "",
    "section": "Linear Classificaiton of Images",
    "text": "Linear Classificaiton of Images\n\nscore function: maps raw data to class scores\nloss function: quantifies agreement between predicted and ground truth\n\n\\(f(x_i, W, b) = Wx_i + b\\)\nParameters:\n\n\\(x_i = D *1\\) vector \\(D=32*32*3\\)\n\\(W = K*D\\) weight matrix where K is number of classes\n\\(b = K*1\\) (bias: influences output without interacting with data)\n\nNote:\n\nEach single matrix multiplication \\(Wx_i\\) evaluates 10 separate classifiers in parallel (each row of W is a classifier)\n\nprovides 10 weighted sum of all image pixels: vector of class scores.\n\nAfter training we only need to save the parameters\nPrediction is just a single matrix multiplication and addition\nThe weights can “like” or “dislike” certain colors at certain positions in the images\n\n“Ship” classifier may have a lot of + weights across blue channels and negative weights in red/green channels: presence of these colors decrease score of ship\n\nEach image can be interpretated as point in D (3072) vector space\n\nEach class score is a linear function over this space, so changing weights of a row in \\(W\\) will rotate linear function in different directions\nBias allows to move line up/down/right/left and lines are not forces to go through origin\n\nOther interpretation of \\(W\\) that each row is a prototpye of class (template matching)\nOften bias in incorporated into weight matrix and x is excdented by a constant 1.\nClass scores can be thought as unnormalized log-probabilities\n\n\n\nCode\nX_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])"
  },
  {
    "objectID": "wiki/Statistics/dl.html#loss-function-cost-function",
    "href": "wiki/Statistics/dl.html#loss-function-cost-function",
    "title": "",
    "section": "Loss Function /Cost Function",
    "text": "Loss Function /Cost Function\nMeasures the unhappiness with the output. Loss is high with poor classifying the training data\n\nCross-Entropy Loss of Softmax Classifier\nSoftmax Classifier is generalization of binary logistic Regression classifier to multiple classes:\n\ngives normalized class probabilites\nis applied to every class score\nsoftmax function takes class scores \\(z_j\\) and squashes it to a vector of values between zero and one which sum to one (= probabilites). This can be interpreted as confidence in each class\n\n\\(\\frac{e^{z_j}}{\\sum_ke^{z_k}}\\)\nCross-entropy loss \\(L_i =-log(\\frac{e^z_{y_i}}{\\sum_je^{z_j}})= -z_{y_i} + log\\sum_je^{z_j} + \\lambda\\sum_k \\sum_lW^2_{k,l}\\)\nwith \\(z_j\\) being the j-th element of vector of class scores \\(z\\) and \\(z_{y_i}\\) score of true class. So we just take \\(-log()\\) of the softmax ouput for true class logit and ignore the other logits/softmax ouputs. The higher the softmax output of true class, the lower the loss.\nFull loss of dataset is the mean of \\(L_i\\) over N training examples: \\(L = \\frac{1}{N}\\sum_iL_i\\)\nNumberic stability: we subract our class scorse by the max score in vector, therefore largest value is zero\n\n\nCode\ndef softmax_loss_vectorized(W, X, y):\n  \"\"\"\n  Softmax loss function, vectorized version.\n  \"\"\"\n  # Initialize the loss\n  loss = 0.0\n  # Compute the softmax loss\n  num_train = X.shape[0]\n  f = X.dot(W)\n  # max of every sample\n  f -= np.max(f, axis=1, keepdims=True)\n  sum_f = np.sum(np.exp(f), axis=1, keepdims=True)\n  p = np.exp(f)/sum_f\n  loss = np.sum(-np.log(p[np.arange(num_train), y]))\n  return loss\n\n\n\nlowest loss is zero.\n\n\n\nHinge Loss Multiclass Support Vector Machine loss (Max Margin loss)\nUsed in SVM. Multiclass Support Vector Machine loss. Takes class scores and summes max of all wrong class scores - class score of correct to determine the loss (end up with N losses):\n\\(L_i = \\sum_{j=y_i}max(0,W_j^T*x_i-W_{y_i}^T*x_i +\\Delta)\\)\nDelta is a hyperparameter which represents a margin that the correct class needs to be higher otherwise we collect loss.\nWithout regularization weight matrix can be multiplied by any number and the same loss would be uptained. Therefore, we add a regularization term (L2 norm) to the loss function (elementwise quadratic penalty over all parameters):\n\\(R(W) = \\sum_k\\sum_lW^2_{k,l}\\)\nGiving the full loss of \\(L = \\frac{1}{N}\\sum_iL_i + \\lambda R(W)\\)\nL2-norm can improve generalization because no single parameter should have large influence. Intuitively, this is because the weights in w2 are smaller and more diffuse. Since the L2 penalty prefers smaller and more diffuse weight vectors, the final classifier is encouraged to take into account all input dimensions to small amounts rather than a few input dimensions and very strongly (e.g. [1,0,0,0] vs [0.25, 0.25, 0.25, 0.25]). With the regularization term we can never have a loss of zero, only if we set all \\(W\\) to zero.\n\nSetting \\(\\Delta\\) is more or less meaningless because weights can shrink or stretch diffferences in margin.\nTherefore only \\(\\lambda\\) reflects the real tradeoff between data loss and regularization loss\nIn practice, however, if the data is noisy or not perfectly separable, a too-large \\(\\Delta\\) can force the model to memorize the data (pushing scores very far apart), leading to overfitting.\n\n\n\nComparison of both Loss\n\nThey are comparable, with the main difference that Softmax loss is never happy\n\nMoreover, in the limit where the weights go towards tiny numbers due to very strong regularization strength \\(\\lambda\\), the output probabilities would be near uniform. Hence, the probabilities computed by the Softmax Classifier are better thought of as confidences where, similar to the SVM, the ordering of the scores is interpretable, but the absolute numbers (or their differences) technically are not. n other words, the Softmax Classifier is never fully happy with the scores it produces: the correct class could always have a higher probability and the incorrect classes always a lower probability and the loss would always get better. However, the SVM is happy once the margins are satisfied and it does not micromanage the exact scores beyond this constraint. This can intuitively be thought of as a feature: For example, a car classifier which is likely spending most of its “effort” on the difficult problem of separating cars from trucks should not be influenced by the frog examples, which it already assigns very low scores to, and which likely cluster around a completely different side of the data cloud."
  },
  {
    "objectID": "wiki/Statistics/dl.html#optimization",
    "href": "wiki/Statistics/dl.html#optimization",
    "title": "",
    "section": "Optimization",
    "text": "Optimization\n\nFinding parameters \\(W\\) that minimizes loss function\nThis is done using gradient descend in the weight/parameter space\nMathematically guaranteed direction of deepest descent = gradient of the loss function\ngradient is generalization of slope for functions that take vector of numbers and is a vector of slopes (partial derivatives) for each dimension.\nTwo calculations: numerical gradient (slow but easy), analytic gradients (fast but error prone)\nThe gradient tells us the direction in which the function has the steepest rate of increase, but it does not tell us how far along this direction we should step.\nStep size = learning rate (hyperparameter)\nLinear complecity in number of parameters: for single step D evaluations\n\n\nGradient Check\nCompute analytic gradient (faster) and compare to numerical gradien\n\n\nGradient Descent\nis procedure of computing gradient of loss function and repeatedly evaluating gradient and updating parameter\n\n\nMini-batch gradient descent\nComputing loss function only for batch (e.g. 256) to perform weight updates. This works sind data points are often correlated.\n\nStochastic Gradient Descent (SGD): extreme case with only 1 example\nSize of batch is a hyperparameter, set with crossvalidation but often a number to the power of 2 because faster in vectorization"
  },
  {
    "objectID": "wiki/Statistics/dl.html#activation-functions",
    "href": "wiki/Statistics/dl.html#activation-functions",
    "title": "",
    "section": "Activation Functions",
    "text": "Activation Functions\nSigmoid Function drawbacks:\n\nSigmoid saturate and kill gradients: If saturated (close to 0 or 1) network barely learns during backprogragtion\nSigmoid ouput are not zero-centered: (Page 35)\n\nTanh (squashes values to -1:1):\n\nis zero-centered but same problem with saturation\n\nRectified Linear Unit (ReLUs):\n\\(f(x) = max(0,x)\\)\n\nfast convergence in gradient descent\neasy implementation treshholding a matrix at zero\nbut can be fragile during training and die (if learning rate is set too high)\nLeaky ReLUs helps against dying (has a small negative slope below x = zero)\nMaxout neuron combined best of ReLU and Leaky ReLU but doubles number of parameters"
  },
  {
    "objectID": "wiki/Statistics/dl.html#nn-architectures",
    "href": "wiki/Statistics/dl.html#nn-architectures",
    "title": "",
    "section": "NN Architectures",
    "text": "NN Architectures\n\nA NN with one hidden layer can represent any continous function but this is of no practicle relevance\nNon-convex function ### Layer-wise organzization\nInput layer is not counted towards arichtecture: so not when taking about layers and not when taking about untis/neurons\nInput layer don’t have weights or biases but output layer does\nFor 2-layer NN(4 units + 2 output) with 3 inputs we have: \\(3 * 4 + 4 *2 = 20\\) weights and 1 bias parameter for each unit/neuron. Total = 26 learnable parameters\n\n\nFeed-Foward Computation\nThe forward pass of a fully-connected layer corresponds to one matrix multiplication followed by a bias offset and an activation function. Matrix form of parameter (e.g. Fully connected 3 * 4 * 4 * 1, with first as input layer):\n\nInput layer 3 * 1\nFirst hidden layer matrix of 4 * 3, where each row are the 3 weights of a neuron. This allows for easy multiplication from left to right\nFirst hidden layer has a bias vector of 4 * 1\nSecond hidden layer: 4 * 4 matrix\nOutput layer: 1 * 4\n\n\n\nCode\n# forward-pass of a 3-layer neural network:\n# activation function (use sigmoid)\nf = lambda x: 1.0/(1.0 + np.exp(-x))\n# random input vector of three numbers (3x1)\nx = np.random.randn(3, 1)\n# calculate first hidden layer activations (4x1)\nh1 = f(np.dot(W1, x) + b1)\n# calculate second hidden layer activations (4x1)\nh2 = f(np.dot(W2, h1) + b2)\n# output neuron (1x1)\nout = np.dot(W3, h2) + b3\n\n\n\noutput layer has no activation function\nx in the input layer could be a batch of data where each example is a column vector and would be evaluted in parallel\nIn practice, it is always better to use these methods to control overfitting instead of the number of neurons.\n\nBecause smaller NN are harder to train with graident descent. Local minima are increased with larger NN and of more value.\nOn the other hand, if you train a large network you’ll start to find many different solutions, but the variance in the final achieved loss will be much smaller. In other words, all solutions are about equally as good, and rely less on the luck of random initialization."
  },
  {
    "objectID": "wiki/Statistics/dl.html#optimization-with-backpropagation",
    "href": "wiki/Statistics/dl.html#optimization-with-backpropagation",
    "title": "",
    "section": "Optimization with Backpropagation",
    "text": "Optimization with Backpropagation\nComputing gradients through recursive application of chain rule.\n\nwe get gradients for all parameters: weight matrix and bias vector\nto do\n\n\nKeras\n\n\nCode\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\nlayers.Dense(500, activation=\"relu\", input_shape=(784,)),\nlayers.Dense(50, activation=\"relu\"),\nlayers.Dense(10, activation=\"softmax\")\n])\n\n\n\nFully connect hidden layer with 500 units, each unit takes input from all 784 units in previous layer\nFully connected hidden layer with 50 units, each unit takes input from all 500 units\nEach unit outpus one value everytime!\nOuput layer: 10 units softmax\n\n\n\nCode\nmodel.compile(optimizer='sgd',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()\n\n# train model with:\nhistory = model.fit(X_train, y_train_cat, epochs=5, batch_size=128)\n\n# test on new data\ntest_loss, test_acc = model.evaluate(X_test, y_test_cat)\nprint(f\"test_acc: {test_acc}\")\n\n\n\nBatch: During each epoch (one full pass through the data), the model divides the data into mini-batches of 128 examples. (ca. 60000 /128 = 469 batches)\nFor each batch: forward pass, compute loss, backward pass, update weights\n\nAfter all 469 batches, one epoche is complete\n\n\nSequential adding of layers:\n\n\nCode\nmodel = tf.keras.Sequential()\n# From Input to first hidden layer\nmodel.add(tf.keras.layers.Dense(100, activation= \"relu\",\n                                input_shape=(2,)))\n# From first hidden layer to output layer\nmodel.add(tf.keras.layers.Dense(3, activation=\"softmax\"))\n\n\nCompile():\n\nCompiling the Keras model calls the backend Tensorflow and binds the optimizer, loss function, and other parameters required before the model can be run on any input data."
  },
  {
    "objectID": "wiki/Statistics/dl.html#training-and-optimizing",
    "href": "wiki/Statistics/dl.html#training-and-optimizing",
    "title": "",
    "section": "Training and Optimizing",
    "text": "Training and Optimizing\nPreprocessing a data matrix x with N x D.\n\nMean Subtraction (Zero-Centered)\nSubtracting mean of all features. Centering data around origin\n\n\nCode\nX -= np.mean(X, axis = 0)\n\n# images we could do it for each channel seperatly\nX -= np.mean(X, axis = (0,1,2))\n\n\n\n\nNormalization\nFeatures are same scale. Divide by standard deviation after zero-centered\n\n\nPCA and Whitening\nFirst centered. Next covariance matrix (tell us about correlation structure). Then we can compute the SD factorization of the data covariance matrix"
  },
  {
    "objectID": "wiki/Statistics/dl.html#simple-celsisus-to-fahrenheit-fun-part",
    "href": "wiki/Statistics/dl.html#simple-celsisus-to-fahrenheit-fun-part",
    "title": "",
    "section": "Simple Celsisus to fahrenheit fun part",
    "text": "Simple Celsisus to fahrenheit fun part\n\n\nCode\ninput = tf.keras.layers.Input((1,))\nl0 = tf.keras.layers.Dense(units=4)\nl1 = tf.keras.layers.Dense(units=4)\nl2 = tf.keras.layers.Dense(units=1)\nmodel = tf.keras.Sequential([input, l0, l1, l2])\nmodel.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.1))\nmodel.fit(celsius_q, fahrenheit_a, epochs=500, verbose=False)\nprint(\"Finished training the model\")\nprint(\"Model predicts that 100 degrees Celsius is: {} degrees Fahrenheit\".format(model.predict(np.array([100.0]))))\nprint(\"These are the l0 variables: {}\".format(l0.get_weights()))\nprint(\"These are the l1 variables: {}\".format(l1.get_weights()))\nprint(\"These are the l2 variables: {}\".format(l2.get_weights()))"
  },
  {
    "objectID": "wiki/Statistics/dl.html#linear-classifier",
    "href": "wiki/Statistics/dl.html#linear-classifier",
    "title": "",
    "section": "Linear Classifier",
    "text": "Linear Classifier\n\n\nCode\n#Train a Linear Classifier\n\n# initialize parameters randomly\nW = 0.01 * np.random.randn(D,K)\nb = np.zeros((1,K))\n\n# some hyperparameters\nstep_size = 1e-0\nreg = 1e-3 # regularization strength\n\n# gradient descent loop\nnum_examples = X.shape[0]\nfor i in range(200):\n\n  # evaluate class scores, [N x K]\n  scores = np.dot(X, W) + b\n\n  # compute the class probabilities\n  exp_scores = np.exp(scores)\n  # [N x K]\n  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n  # compute the loss: average cross-entropy loss and regularization\n  correct_logprobs = -np.log(probs[range(num_examples),y])\n  data_loss = np.sum(correct_logprobs)/num_examples\n  reg_loss = 0.5*reg*np.sum(W*W)\n  loss = data_loss + reg_loss\n  if i % 10 == 0:\n    print(\"iteration %d: loss %f\" % (i, loss))\n\n  # compute the gradient on scores\n  dscores = probs\n  dscores[range(num_examples),y] -= 1\n  dscores /= num_examples\n\n  # backpropate the gradient to the parameters (W,b)\n  dW = np.dot(X.T, dscores)\n  db = np.sum(dscores, axis=0, keepdims=True)\n\n  dW += reg*W # regularization gradient\n\n  # perform a parameter update\n  W += -step_size * dW\n  b += -step_size * db"
  },
  {
    "objectID": "wiki/Statistics/dl.html#nn",
    "href": "wiki/Statistics/dl.html#nn",
    "title": "",
    "section": "NN",
    "text": "NN\n\nbroadcasting of bias vector b:np.dot gives 300x100 matrix and bias is 100x1. Broadcasting give us 300x100 by repeating the vector.\nInput values can be bound from 0 to 1: e.g. for greyscale divide by 255, or use normalization\nOne hot encoding of labels\n\n\n\nCode\nhidden_layer = np.maximum(0, np.dot(X, W) + b) # b This is called broadcasting: it adds the same bias vector to each row.\n\n\n\n\nCode\n# initialize parameters randomly\nh = 100 # size of hidden layer\nW = 0.01 * np.random.randn(D,h)\nb = np.zeros((1,h))\nW2 = 0.01 * np.random.randn(h,K)\nb2 = np.zeros((1,K))\n\n# some hyperparameters\nstep_size = 1e-0\nreg = 1e-3 # regularization strength\n\n# gradient descent loop\nnum_examples = X.shape[0]\nfor i in range(10000):\n\n  # evaluate class scores, [N x K]\n  hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n  scores = np.dot(hidden_layer, W2) + b2\n\n  # compute the class probabilities\n  exp_scores = np.exp(scores)\n  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n\n  # compute the loss: average cross-entropy loss and regularization\n  correct_logprobs = -np.log(probs[range(num_examples),y])\n  data_loss = np.sum(correct_logprobs)/num_examples\n  reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n  loss = data_loss + reg_loss\n  if i % 1000 == 0:\n    print(\"iteration %d: loss %f\" % (i, loss))\n\n  # compute the gradient on scores\n  dscores = probs\n  dscores[range(num_examples),y] -= 1\n  dscores /= num_examples\n\n  # backpropagate the gradient to the parameters\n  # first backprop into parameters W2 and b2\n  dW2 = np.dot(hidden_layer.T, dscores)\n  db2 = np.sum(dscores, axis=0, keepdims=True)\n  # next backprop into hidden layer\n  dhidden = np.dot(dscores, W2.T)\n  # backprop the ReLU non-linearity\n  dhidden[hidden_layer &lt;= 0] = 0\n  # finally into W,b\n  dW = np.dot(X.T, dhidden)\n  db = np.sum(dhidden, axis=0, keepdims=True)\n\n  # add regularization gradient contribution\n  dW2 += reg * W2\n  dW += reg * W\n\n  # perform a parameter update\n  W += -step_size * dW\n  b += -step_size * db\n  W2 += -step_size * dW2\n  b2 += -step_size * db2"
  },
  {
    "objectID": "wiki/Python/rag.html#generation-part",
    "href": "wiki/Python/rag.html#generation-part",
    "title": "Retrieval Augmented Generation",
    "section": "Generation part",
    "text": "Generation part\nThe query from a user is embedded with the same embedding model into the vector space. Next using similarity measures the nearest vectors are retrieved. This information is than passed into a LLM as context to provided an answer to the query.",
    "crumbs": [
      "Wiki",
      "Python",
      "Retrueval Augmented Generation"
    ]
  },
  {
    "objectID": "wiki/Python/rag.html#impact-of-text-splitting-on-rag-quality",
    "href": "wiki/Python/rag.html#impact-of-text-splitting-on-rag-quality",
    "title": "Retrieval Augmented Generation",
    "section": "Impact of Text splitting on RAG quality",
    "text": "Impact of Text splitting on RAG quality\n\nSplitting by character\nAdvantages:\n\nFine-grained context\n\nChallenges:\n\nLong Sequences, token limitations\nIncreased Inference time\n\n\n\nSplitting by Token\nAdvantages:\n\nToken Efficiency\nBalanced Context (meaningful, granularity)\nScalability\n\nChallgenges:\n\nContextual information",
    "crumbs": [
      "Wiki",
      "Python",
      "Retrueval Augmented Generation"
    ]
  },
  {
    "objectID": "wiki/Python/rag.html#impact-of-metadata",
    "href": "wiki/Python/rag.html#impact-of-metadata",
    "title": "Retrieval Augmented Generation",
    "section": "Impact of Metadata",
    "text": "Impact of Metadata",
    "crumbs": [
      "Wiki",
      "Python",
      "Retrueval Augmented Generation"
    ]
  }
]