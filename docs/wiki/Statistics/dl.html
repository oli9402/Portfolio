<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Oliver Zingg</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../wiki/LinAlg/linalg_start.html" rel="next">
<link href="../../wiki/Statistics/glm.html" rel="prev">
<link href="../../oli.jpg" rel="icon" type="image/jpeg">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Oliver Zingg</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> <i class="bi bi-house-door" role="img">
</i> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> <i class="bi bi-file-person" role="img">
</i> 
<span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects.html"> <i class="bi bi-list-task" role="img">
</i> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../../wiki.html" aria-current="page"> <i class="bi bi-book-half" role="img">
</i> 
<span class="menu-text">Wiki</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../wiki/Statistics/stats_start.html">Statistics</a></li><li class="breadcrumb-item"><a href="../../wiki/Statistics/dl.pdf">Deep Learning Python</a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../wiki.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Personal Wiki</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../wiki/Python/python_start.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Python</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../wiki/Python/basics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Basics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../wiki/Python/wrangling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Wrangling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../wiki/Python/rag.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Retrueval Augmented Generation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../wiki/Statistics/stats_start.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../wiki/Statistics/ci.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Confidence Intervals</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../wiki/Statistics/Glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Glossary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../wiki/Statistics/glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">General Linear Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../wiki/Statistics/dl.pdf" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Deep Learning Python</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../wiki/LinAlg/linalg_start.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear Algebra</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../wiki/LinAlg/ttr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Things to remember</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../wiki/LinAlg/sys.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Systems of Linear Equations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../wiki/LinAlg/basis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Basis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../wiki/LinAlg/linear-maps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear Maps</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../wiki/LinAlg/eigen.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Eigenvalues and -vectors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../wiki/LinAlg/norm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Orthonormal Basis</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../wiki/DOE/int.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Design of Experiments</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../wiki/DOE/doe.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Research Design</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#section" id="toc-section" class="nav-link active" data-scroll-target="#section"></a></li>
  <li><a href="#image-classification-lecture-1" id="toc-image-classification-lecture-1" class="nav-link" data-scroll-target="#image-classification-lecture-1">Image Classification (Lecture 1)</a>
  <ul class="collapse">
  <li><a href="#nearest-neighbor-classifier-k-1" id="toc-nearest-neighbor-classifier-k-1" class="nav-link" data-scroll-target="#nearest-neighbor-classifier-k-1">Nearest Neighbor Classifier (<em>k = 1</em>)</a>
  <ul class="collapse">
  <li><a href="#how-compare-to-arrays-a-and-b" id="toc-how-compare-to-arrays-a-and-b" class="nav-link" data-scroll-target="#how-compare-to-arrays-a-and-b">How compare to arrays A and B?</a></li>
  </ul></li>
  <li><a href="#k-nearest-neighbor-classifier" id="toc-k-nearest-neighbor-classifier" class="nav-link" data-scroll-target="#k-nearest-neighbor-classifier">k-Nearest Neighbor Classifier</a>
  <ul class="collapse">
  <li><a href="#k-fold-cross-validation-what-k-to-chose" id="toc-k-fold-cross-validation-what-k-to-chose" class="nav-link" data-scroll-target="#k-fold-cross-validation-what-k-to-chose">k-fold Cross Validation (what k to chose?)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#neural-networks-lecture-2" id="toc-neural-networks-lecture-2" class="nav-link" data-scroll-target="#neural-networks-lecture-2">Neural Networks (Lecture 2)</a>
  <ul class="collapse">
  <li><a href="#linear-classificaiton-of-images" id="toc-linear-classificaiton-of-images" class="nav-link" data-scroll-target="#linear-classificaiton-of-images">Linear Classificaiton of Images</a></li>
  <li><a href="#loss-function-cost-function" id="toc-loss-function-cost-function" class="nav-link" data-scroll-target="#loss-function-cost-function">Loss Function /Cost Function</a>
  <ul class="collapse">
  <li><a href="#cross-entropy-loss-of-softmax-classifier" id="toc-cross-entropy-loss-of-softmax-classifier" class="nav-link" data-scroll-target="#cross-entropy-loss-of-softmax-classifier">Cross-Entropy Loss of Softmax Classifier</a></li>
  <li><a href="#hinge-loss-multiclass-support-vector-machine-loss-max-margin-loss" id="toc-hinge-loss-multiclass-support-vector-machine-loss-max-margin-loss" class="nav-link" data-scroll-target="#hinge-loss-multiclass-support-vector-machine-loss-max-margin-loss">Hinge Loss Multiclass Support Vector Machine loss (Max Margin loss)</a></li>
  <li><a href="#comparison-of-both-loss" id="toc-comparison-of-both-loss" class="nav-link" data-scroll-target="#comparison-of-both-loss">Comparison of both Loss</a></li>
  </ul></li>
  <li><a href="#optimization" id="toc-optimization" class="nav-link" data-scroll-target="#optimization">Optimization</a>
  <ul class="collapse">
  <li><a href="#gradient-check" id="toc-gradient-check" class="nav-link" data-scroll-target="#gradient-check">Gradient Check</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient Descent</a></li>
  <li><a href="#mini-batch-gradient-descent" id="toc-mini-batch-gradient-descent" class="nav-link" data-scroll-target="#mini-batch-gradient-descent">Mini-batch gradient descent</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#neural-network" id="toc-neural-network" class="nav-link" data-scroll-target="#neural-network">Neural Network</a>
  <ul class="collapse">
  <li><a href="#activation-functions" id="toc-activation-functions" class="nav-link" data-scroll-target="#activation-functions">Activation Functions</a></li>
  <li><a href="#nn-architectures" id="toc-nn-architectures" class="nav-link" data-scroll-target="#nn-architectures">NN Architectures</a>
  <ul class="collapse">
  <li><a href="#feed-foward-computation" id="toc-feed-foward-computation" class="nav-link" data-scroll-target="#feed-foward-computation">Feed-Foward Computation</a></li>
  </ul></li>
  <li><a href="#optimization-with-backpropagation" id="toc-optimization-with-backpropagation" class="nav-link" data-scroll-target="#optimization-with-backpropagation">Optimization with Backpropagation</a>
  <ul class="collapse">
  <li><a href="#keras" id="toc-keras" class="nav-link" data-scroll-target="#keras">Keras</a></li>
  </ul></li>
  <li><a href="#training-and-optimizing" id="toc-training-and-optimizing" class="nav-link" data-scroll-target="#training-and-optimizing">Training and Optimizing</a>
  <ul class="collapse">
  <li><a href="#mean-subtraction-zero-centered" id="toc-mean-subtraction-zero-centered" class="nav-link" data-scroll-target="#mean-subtraction-zero-centered">Mean Subtraction (Zero-Centered)</a></li>
  <li><a href="#normalization" id="toc-normalization" class="nav-link" data-scroll-target="#normalization">Normalization</a></li>
  <li><a href="#pca-and-whitening" id="toc-pca-and-whitening" class="nav-link" data-scroll-target="#pca-and-whitening">PCA and Whitening</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#exercise" id="toc-exercise" class="nav-link" data-scroll-target="#exercise">Exercise</a>
  <ul class="collapse">
  <li><a href="#simple-celsisus-to-fahrenheit-fun-part" id="toc-simple-celsisus-to-fahrenheit-fun-part" class="nav-link" data-scroll-target="#simple-celsisus-to-fahrenheit-fun-part">Simple Celsisus to fahrenheit fun part</a></li>
  <li><a href="#linear-classifier" id="toc-linear-classifier" class="nav-link" data-scroll-target="#linear-classifier">Linear Classifier</a></li>
  <li><a href="#nn" id="toc-nn" class="nav-link" data-scroll-target="#nn">NN</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="dl.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">




<section id="section" class="level1">
<h1></h1>
</section>
<section id="image-classification-lecture-1" class="level1">
<h1>Image Classification (Lecture 1)</h1>
<ul>
<li>Goal: predict a label or distribution over labels for a given image (probabilty)</li>
<li>Often width (pixel) x height (pixel) x 3 (red, green, blue) arrays,</li>
<li>flattend to one large array where each value is an integer from 0 (black) to 255 (white)</li>
</ul>
<p>Some challenges for computer vision algorithm (must be invariant to cross product of these while retaining sensitivity to inter-class variations):</p>
<ul>
<li>Viewpoint variation (orientation of image)</li>
<li>Scale variation (variation of size in image and real life)</li>
<li>Deformation (objects are not rigid bodies)</li>
<li>Occlusion (object can be hidden)</li>
<li>Illumination conditions (effects are drastic on pixel level)</li>
<li>Background clutter (objects may blend into environment)</li>
<li>Intra-class variation (many different types)</li>
</ul>
<section id="nearest-neighbor-classifier-k-1" class="level2">
<h2 class="anchored" data-anchor-id="nearest-neighbor-classifier-k-1">Nearest Neighbor Classifier (<em>k = 1</em>)</h2>
<p>Takes test image (array) and compares it to every image (array) in training set and takes image label of closest one. <em>It simply remembers all training data</em></p>
<ul>
<li>With k = 1 we create “small islands of liekly incorrect predictions, always use higher k</li>
</ul>
<section id="how-compare-to-arrays-a-and-b" class="level3">
<h3 class="anchored" data-anchor-id="how-compare-to-arrays-a-and-b">How compare to arrays A and B?</h3>
<p><strong>L1 distance/norm</strong>: Sum of over all absolute pixel differences <span class="math inline">\(d_1(A,B) = \sum_i|A_i-B_i|\)</span> If <span class="math inline">\(A = B\)</span> than L1 will be zero.</p>
<div id="eed0c6b7" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NearestNeighbor():</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> train(<span class="va">self</span>, X, y):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" X is N x D where N is number of examples and D flattend pixel array.</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Y is 1-dimension of size N.</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Training is just remembering all data"""</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.X_train <span class="op">=</span> X </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.y_train <span class="op">=</span> y</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" X is N x D where each row is an example</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">    we wish to predict label for """</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    num_test <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># lets make sure that the output type</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># matches the input type</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    Ypred <span class="op">=</span> np.zeros(num_test, dtype <span class="op">=</span> <span class="va">self</span>.y_train.dtype)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loop over all test rows</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_test):</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>      <span class="co">#L1</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>      distances <span class="op">=</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(<span class="va">self</span>.X_train <span class="op">-</span> X[i,:]), axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>      <span class="co"># get the index with smallest distance</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>      min_index <span class="op">=</span> np.argmin(distances)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>      <span class="co"># predict the label of the nearest example</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>      Ypred[i] <span class="op">=</span> <span class="va">self</span>.y_train[min_index]</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Ypred</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>nn <span class="op">=</span> NearestNeighbor()</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>nn.train(X_train, y_train)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>y_test_predict <span class="op">=</span> nn.predict(X_test)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'accuracy: </span><span class="sc">{</span>np<span class="sc">.</span>mean(y_test_predict <span class="op">==</span> y_test)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>L2 distance/norm</strong>: euclidean distance between two vectors. Each pixelwise difference is squared, summed up and finally the square root is taken:</p>
<p><span class="math inline">\(d_2(A,B) = \sqrt{\sum_i(A_i-B_i)^2}\)</span></p>
<div id="bb8cc7db" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># new distance L2: we could leave out np.sqrt and get the same final output (monotonic function)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>distances <span class="op">=</span> np.sqrt(np.<span class="bu">sum</span>(np.square(<span class="va">self</span>.X_train <span class="op">-</span> X[i,:]), axis <span class="op">=</span> <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Difference L1 , L2</strong> - L2 unforgiving: prefers many medium disagreements to one big one.</p>
</section>
</section>
<section id="k-nearest-neighbor-classifier" class="level2">
<h2 class="anchored" data-anchor-id="k-nearest-neighbor-classifier">k-Nearest Neighbor Classifier</h2>
<p>Find top k closets images and take “majority vote” of labels. Higher k-values have a smoothing effect and make classifier more resistant to outliers. - Training is very fast - Predicting is slow since it compares x_test to each x_training - In practice we want efficient predictions! - Space inefficient</p>
<div id="0dc50dd5" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> KNearestNeighbor():</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">""" a kNN classifier with L2 distance """</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> train(<span class="va">self</span>, X, y):</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" same as before """</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> predit(<span class="va">self</span>, X, k <span class="op">=</span> <span class="dv">1</span>):</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" returns output of methods: compute_distances and predict_labels"""</span> </span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>.predict_labels(dists, k <span class="op">=</span> k)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> compute_distacnes(<span class="va">self</span>, X): </span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" L2 Distance: for each image in test set X  calculate the L2 distance to each images in X_train. Return a matrix dists where each row is a test image and each column the L2 of corresponding train image"""</span> </span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    num_test <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    num_train <span class="op">=</span> <span class="va">self</span>.X_train.shape[<span class="dv">0</span>]</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    dists <span class="op">=</span> np.zeros((num_test, num_train))</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_test):</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>      dists[i, :] <span class="op">=</span> np.sqrt(np.<span class="bu">sum</span>(np.square(</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>                            <span class="va">self</span>.X_train <span class="op">-</span> X[i,:]),</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>                            axis <span class="op">=</span> <span class="dv">1</span>))</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dists</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> predict_labels(<span class="va">self</span>, dists, k <span class="op">=</span><span class="dv">1</span>): </span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns: array with predicted labels """</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    num_test <span class="op">=</span> dists.shape[<span class="dv">0</span>]</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> np.zeros(num_test)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_test):</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>      closest_y <span class="op">=</span> []</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>      <span class="co"># get the k indices with smallest distances</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>      min_indices <span class="op">=</span> np.argsort(dists[i,:])[:k]</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>      closest_y <span class="op">=</span> np.bincount(<span class="va">self</span>.y_train[min_indices])</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>      <span class="co"># predict the label of the nearest example</span></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>      y_pred[i] <span class="op">=</span> np.argmax(closest_y)</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y_pred</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>We could speed up with broadcasting rules (see lecture notes)</li>
<li>gray regions in visualization of results are caused by ties in majority vote</li>
</ul>
<section id="k-fold-cross-validation-what-k-to-chose" class="level3">
<h3 class="anchored" data-anchor-id="k-fold-cross-validation-what-k-to-chose">k-fold Cross Validation (what k to chose?)</h3>
<ul>
<li>Hyperparameters = choiches needed to be made (distances, k, etc.)</li>
<li>Try out different hyperparameters and use best</li>
<li>We further take a small part of training set (e.g.&nbsp;1000) as validation set which is used as a “<strong>fake test set</strong>” to tune the hyperparameter</li>
</ul>
<div id="588e25ab" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>X_val <span class="op">=</span> X_train[:<span class="dv">1000</span>, :]</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>y_val <span class="op">=</span> y_train[:<span class="dv">1000</span>]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X_train[<span class="dv">1000</span>:, :]</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> y_train[<span class="dv">1000</span>:]</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>validation_accuracies <span class="op">=</span> []</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">50</span>, <span class="dv">100</span>]:</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># use a particular value of k and evaluation on validation data</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>  nn <span class="op">=</span> NearestNeighbor()</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>  nn.train(X_train, y_train)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># here we assume a modified NearestNeighbor class that can</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># take a k as input</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>  y_val_predict <span class="op">=</span> nn.predict(X_val, k <span class="op">=</span> k)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>  acc <span class="op">=</span> np.mean(y_val_predict <span class="op">==</span> y_val)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">'accuracy: </span><span class="sc">%f</span><span class="st">'</span> <span class="op">%</span> (acc,))</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># keep track of what works on the validation set</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>  validation_accuracies.append((k, acc))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>When training data is smaller or computation is not heavy</p>
<ul>
<li>iterate over different validiation sets and average the performance across these</li>
<li>5-fold: split training into 5 parts, iterate 5 times while changing validation part</li>
<li>In practice it is cleaner to no include the validation set in the final training of the model after determining the best hyperparameters</li>
</ul>
<div id="fd689ba9" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># k-fold cross validation</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>num_folds <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>k_choices <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">10</span>, <span class="dv">12</span>, <span class="dv">15</span>, <span class="dv">18</span>, <span class="dv">20</span>, <span class="dv">50</span>, <span class="dv">100</span>]</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>X_train_folds <span class="op">=</span> []</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>y_train_folds <span class="op">=</span> []</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Split up the training data into folds. After splitting,</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># X_train_folds and y_train_folds should each be lists of</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># length num_folds, where y_train_folds[i] is the label</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># vector for the points in X_train_folds[i]</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>X_train_folds <span class="op">=</span> np.split(X_train, [<span class="dv">1000</span>, <span class="dv">2000</span>, <span class="dv">3000</span>, <span class="dv">4000</span>])</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>y_train_folds <span class="op">=</span> np.split(y_train, [<span class="dv">1000</span>, <span class="dv">2000</span>, <span class="dv">3000</span>, <span class="dv">4000</span>])</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co"># A dictionary holding the accuracies for different values of</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co"># k that we find when running cross-validation. After running</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># cross-validation, k_to_accuracies[k] should be a list of</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co"># length num_folds giving the different accuracy values that</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># we found when using that value of k.</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>k_to_accuracies <span class="op">=</span> {}</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># We perform k-fold cross validation to find the best value of k.</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="co"># For each possible value of k, run the k-nearest-neighbor algorithm</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co"># num_folds times, where in each case you use all but one of the folds</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="co"># as training data and the last fold as a validation set. Store the</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="co"># accuracies for all fold and all values of k in the k_to_accuracies</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="co"># dictionary.</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> k_choices:</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>  k_to_accuracies[k] <span class="op">=</span> []</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>  classifier <span class="op">=</span> KNearestNeighbor()</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_folds):</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="co">#we use ith fold as validation set  </span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    X_cv_training <span class="op">=</span> np.concatenate([x <span class="cf">for</span> k, x <span class="kw">in</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">enumerate</span>(X_train_folds) <span class="cf">if</span> k<span class="op">!=</span>i], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    y_cv_training <span class="op">=</span> np.concatenate([x <span class="cf">for</span> k, x <span class="kw">in</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>      <span class="bu">enumerate</span>(y_train_folds) <span class="cf">if</span> k<span class="op">!=</span>i], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>    classifier.train(X_cv_training, y_cv_training)</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>    dists <span class="op">=</span> classifier.compute_distances_one_loop(X_train_folds[i])</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>    y_test_pred <span class="op">=</span> classifier.predict_labels(dists, k<span class="op">=</span>k)</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>    k_to_accuracies[k].append(np.mean(y_train_folds[i] <span class="op">==</span> y_test_pred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>in practice on may want to avoid cross-validation in favor of single validation split (computationally expensive)</li>
<li>Size of single split of training data depends how many hyperparameters</li>
<li>For example if the number of hyperparameters is large you may prefer to use bigger validation splits. If the number of examples in the validation set is small (perhaps only a few hundred or so), it is safer to use cross-validation. Typical number of folds you can see in practice would be 3-fold, 5-fold or 10-fold cross-validation.</li>
</ul>
</section>
</section>
</section>
<section id="neural-networks-lecture-2" class="level1">
<h1>Neural Networks (Lecture 2)</h1>
<section id="linear-classificaiton-of-images" class="level2">
<h2 class="anchored" data-anchor-id="linear-classificaiton-of-images">Linear Classificaiton of Images</h2>
<ol type="1">
<li>score function: maps raw data to class scores</li>
<li>loss function: quantifies agreement between predicted and ground truth</li>
</ol>
<p><span class="math inline">\(f(x_i, W, b) = Wx_i + b\)</span></p>
<p>Parameters:</p>
<ul>
<li><span class="math inline">\(x_i = D *1\)</span> vector <span class="math inline">\(D=32*32*3\)</span></li>
<li><span class="math inline">\(W = K*D\)</span> weight matrix where K is number of classes</li>
<li><span class="math inline">\(b = K*1\)</span> (bias: influences output without interacting with data)</li>
</ul>
<p>Note:</p>
<ul>
<li>Each single matrix multiplication <span class="math inline">\(Wx_i\)</span> evaluates 10 separate classifiers in parallel (each row of W is a classifier)
<ul>
<li>provides 10 weighted sum of all image pixels: vector of class scores.</li>
</ul></li>
<li>After training we only need to save the parameters</li>
<li>Prediction is just a single matrix multiplication and addition</li>
<li>The weights can “like” or “dislike” certain colors at certain positions in the images
<ul>
<li>“Ship” classifier may have a lot of + weights across blue channels and negative weights in red/green channels: presence of these colors decrease score of ship</li>
</ul></li>
<li>Each image can be interpretated as point in D (3072) vector space
<ul>
<li>Each class score is a linear function over this space, so changing weights of a row in <span class="math inline">\(W\)</span> will rotate linear function in different directions</li>
<li>Bias allows to move line up/down/right/left and lines are not forces to go through origin</li>
</ul></li>
<li>Other interpretation of <span class="math inline">\(W\)</span> that each row is a prototpye of class (template matching)</li>
<li>Often bias in incorporated into weight matrix and x is excdented by a constant 1.</li>
<li>Class scores can be thought as unnormalized log-probabilities</li>
</ul>
<div id="b1c21a71" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> np.hstack([X_train, np.ones((X_train.shape[<span class="dv">0</span>], <span class="dv">1</span>))])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="loss-function-cost-function" class="level2">
<h2 class="anchored" data-anchor-id="loss-function-cost-function">Loss Function /Cost Function</h2>
<p>Measures the <strong>unhappiness</strong> with the output. Loss is high with poor classifying the training data</p>
<section id="cross-entropy-loss-of-softmax-classifier" class="level3">
<h3 class="anchored" data-anchor-id="cross-entropy-loss-of-softmax-classifier">Cross-Entropy Loss of Softmax Classifier</h3>
<p>Softmax Classifier is generalization of binary logistic Regression classifier to multiple classes:</p>
<ul>
<li>gives normalized class probabilites</li>
<li>is applied to every class score</li>
<li>softmax function takes class scores <span class="math inline">\(z_j\)</span> and squashes it to a vector of values between zero and one which sum to one (= probabilites). This can be interpreted as confidence in each class</li>
</ul>
<p><span class="math inline">\(\frac{e^{z_j}}{\sum_ke^{z_k}}\)</span></p>
<p><strong>Cross-entropy loss</strong> <span class="math inline">\(L_i =-log(\frac{e^z_{y_i}}{\sum_je^{z_j}})= -z_{y_i} + log\sum_je^{z_j} + \lambda\sum_k \sum_lW^2_{k,l}\)</span></p>
<p>with <span class="math inline">\(z_j\)</span> being the j-th element of vector of class scores <span class="math inline">\(z\)</span> and <span class="math inline">\(z_{y_i}\)</span> score of true class. So we just take <span class="math inline">\(-log()\)</span> of the softmax ouput for <strong>true class logit</strong> and ignore the other logits/softmax ouputs. The <em>higher</em> the softmax output of true class, the <em>lower</em> the loss.</p>
<p>Full loss of dataset is the <strong>mean</strong> of <span class="math inline">\(L_i\)</span> over N training examples: <span class="math inline">\(L = \frac{1}{N}\sum_iL_i\)</span></p>
<p><strong>Numberic stability</strong>: we subract our class scorse by the max score in vector, therefore largest value is zero</p>
<div id="290ee79f" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax_loss_vectorized(W, X, y):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">  Softmax loss function, vectorized version.</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Initialize the loss</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute the softmax loss</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  num_train <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>  f <span class="op">=</span> X.dot(W)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># max of every sample</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>  f <span class="op">-=</span> np.<span class="bu">max</span>(f, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>  sum_f <span class="op">=</span> np.<span class="bu">sum</span>(np.exp(f), axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>  p <span class="op">=</span> np.exp(f)<span class="op">/</span>sum_f</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> np.<span class="bu">sum</span>(<span class="op">-</span>np.log(p[np.arange(num_train), y]))</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>lowest loss is zero.</li>
</ul>
</section>
<section id="hinge-loss-multiclass-support-vector-machine-loss-max-margin-loss" class="level3">
<h3 class="anchored" data-anchor-id="hinge-loss-multiclass-support-vector-machine-loss-max-margin-loss">Hinge Loss Multiclass Support Vector Machine loss (Max Margin loss)</h3>
<p>Used in SVM. Multiclass Support Vector Machine loss. Takes class scores and summes max of all wrong class scores - class score of correct to determine the loss (end up with N losses):</p>
<p><span class="math inline">\(L_i = \sum_{j=y_i}max(0,W_j^T*x_i-W_{y_i}^T*x_i +\Delta)\)</span></p>
<p>Delta is a hyperparameter which represents a margin that the correct class needs to be higher otherwise we collect loss.</p>
<p>Without regularization weight matrix can be multiplied by any number and the same loss would be uptained. Therefore, we add a regularization term (L2 norm) to the loss function (elementwise quadratic penalty over all parameters):</p>
<p><span class="math inline">\(R(W) = \sum_k\sum_lW^2_{k,l}\)</span></p>
<p>Giving the full loss of <span class="math inline">\(L = \frac{1}{N}\sum_iL_i + \lambda R(W)\)</span></p>
<p>L2-norm can improve generalization because no single parameter should have large influence. Intuitively, this is because the weights in w2 are smaller and more diffuse. Since the L2 penalty prefers smaller and more diffuse weight vectors, the final classifier is encouraged to take into account all input dimensions to small amounts rather than a few input dimensions and very strongly (e.g.&nbsp;[1,0,0,0] vs [0.25, 0.25, 0.25, 0.25]). With the regularization term we can never have a loss of zero, only if we set all <span class="math inline">\(W\)</span> to zero.</p>
<ul>
<li>Setting <span class="math inline">\(\Delta\)</span> is more or less meaningless because weights can shrink or stretch diffferences in margin.</li>
<li>Therefore only <span class="math inline">\(\lambda\)</span> reflects the real tradeoff between data loss and regularization loss</li>
<li>In practice, however, if the data is noisy or not perfectly separable, a too-large <span class="math inline">\(\Delta\)</span> can force the model to memorize the data (pushing scores very far apart), leading to overfitting.</li>
</ul>
</section>
<section id="comparison-of-both-loss" class="level3">
<h3 class="anchored" data-anchor-id="comparison-of-both-loss">Comparison of both Loss</h3>
<ul>
<li>They are comparable, with the main difference that Softmax loss is never happy</li>
</ul>
<p>Moreover, in the limit where the weights go towards tiny numbers due to very strong regularization strength <span class="math inline">\(\lambda\)</span>, the output probabilities would be near uniform. Hence, the probabilities computed by the Softmax Classifier are better thought of as confidences where, similar to the SVM, the ordering of the scores is interpretable, but the absolute numbers (or their differences) technically are not. n other words, the Softmax Classifier is never fully happy with the scores it produces: the correct class could always have a higher probability and the incorrect classes always a lower probability and the loss would always get better. However, the SVM is happy once the margins are satisfied and it does not micromanage the exact scores beyond this constraint. This can intuitively be thought of as a feature: For example, a car classifier which is likely spending most of its “effort” on the difficult problem of separating cars from trucks should not be influenced by the frog examples, which it already assigns very low scores to, and which likely cluster around a completely different side of the data cloud.</p>
</section>
</section>
<section id="optimization" class="level2">
<h2 class="anchored" data-anchor-id="optimization">Optimization</h2>
<ul>
<li>Finding parameters <span class="math inline">\(W\)</span> that minimizes loss function</li>
<li>This is done using gradient descend in the weight/parameter space</li>
<li>Mathematically guaranteed direction of deepest descent = gradient of the loss function</li>
<li>gradient is generalization of slope for functions that take vector of numbers and is a vector of slopes (partial derivatives) for each dimension.</li>
<li>Two calculations: numerical gradient (slow but easy), analytic gradients (fast but error prone)</li>
<li>The gradient tells us the direction in which the function has the steepest rate of increase, but it does not tell us how far along this direction we should step.</li>
<li>Step size = learning rate (hyperparameter)</li>
<li>Linear complecity in number of parameters: for single step D evaluations</li>
</ul>
<section id="gradient-check" class="level3">
<h3 class="anchored" data-anchor-id="gradient-check">Gradient Check</h3>
<p>Compute analytic gradient (faster) and compare to numerical gradien</p>
</section>
<section id="gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent">Gradient Descent</h3>
<p>is procedure of computing gradient of loss function and repeatedly evaluating gradient and updating parameter</p>
</section>
<section id="mini-batch-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="mini-batch-gradient-descent">Mini-batch gradient descent</h3>
<p>Computing loss function only for batch (e.g.&nbsp;256) to perform weight updates. This works sind data points are often correlated.</p>
<ul>
<li>Stochastic Gradient Descent (SGD): extreme case with only 1 example</li>
<li>Size of batch is a hyperparameter, set with crossvalidation but often a number to the power of 2 because faster in vectorization</li>
</ul>
</section>
</section>
</section>
<section id="neural-network" class="level1">
<h1>Neural Network</h1>
<p>A single neuron can be trained as Binary SVM Classifier or Binary Softmax Classifier</p>
<p>Use <strong>normalization</strong> (don’t touch training data):</p>
<div id="c8dab994" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> train_data.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">-=</span> mean</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span> train_data.std(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">/=</span> std</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">-=</span> mean</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">/=</span> std</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="activation-functions" class="level2">
<h2 class="anchored" data-anchor-id="activation-functions">Activation Functions</h2>
<p><strong>Sigmoid Function</strong> drawbacks:</p>
<ul>
<li>Sigmoid saturate and kill gradients: If saturated (close to 0 or 1) network barely learns during backprogragtion</li>
<li>Sigmoid ouput are not zero-centered: (Page 35)</li>
</ul>
<p><strong>Tanh (squashes values to -1:1)</strong>:</p>
<ul>
<li>is zero-centered but same problem with saturation</li>
</ul>
<p><strong>Rectified Linear Unit (ReLUs)</strong>:</p>
<p><span class="math inline">\(f(x) = max(0,x)\)</span></p>
<ul>
<li>fast convergence in gradient descent</li>
<li>easy implementation treshholding a matrix at zero</li>
<li>but can be fragile during training and die (if learning rate is set too high)</li>
<li><strong>Leaky ReLUs</strong> helps against dying (has a small negative slope below x = zero)</li>
<li><strong>Maxout neuron</strong> combined best of ReLU and Leaky ReLU but doubles number of parameters</li>
</ul>
</section>
<section id="nn-architectures" class="level2">
<h2 class="anchored" data-anchor-id="nn-architectures">NN Architectures</h2>
<ul>
<li><p>A NN with one hidden layer can represent any continous function but this is of no practicle relevance</p></li>
<li><p>Non-convex function ### Layer-wise organzization</p></li>
<li><p>Input layer is not counted towards arichtecture: so not when taking about layers and not when taking about untis/neurons</p></li>
<li><p>Input layer don’t have weights or biases but output layer does</p></li>
<li><p>For 2-layer NN(4 units + 2 output) with 3 inputs we have: <span class="math inline">\(3 * 4 + 4 *2 = 20\)</span> weights and 1 bias parameter for each unit/neuron. Total = 26 learnable parameters</p></li>
</ul>
<section id="feed-foward-computation" class="level3">
<h3 class="anchored" data-anchor-id="feed-foward-computation">Feed-Foward Computation</h3>
<p>The forward pass of a fully-connected layer corresponds to one matrix multiplication followed by a bias offset and an activation function. Matrix form of parameter (e.g.&nbsp;Fully connected 3 * 4 * 4 * 1, with first as input layer):</p>
<ul>
<li>Input layer 3 * 1</li>
<li>First hidden layer matrix of 4 * 3, where each row are the 3 weights of a neuron. This allows for easy multiplication from left to right</li>
<li>First hidden layer has a bias vector of 4 * 1</li>
<li>Second hidden layer: 4 * 4 matrix</li>
<li>Output layer: 1 * 4</li>
</ul>
<div id="b9c66fcb" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># forward-pass of a 3-layer neural network:</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># activation function (use sigmoid)</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x: <span class="fl">1.0</span><span class="op">/</span>(<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># random input vector of three numbers (3x1)</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.randn(<span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate first hidden layer activations (4x1)</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>h1 <span class="op">=</span> f(np.dot(W1, x) <span class="op">+</span> b1)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate second hidden layer activations (4x1)</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>h2 <span class="op">=</span> f(np.dot(W2, h1) <span class="op">+</span> b2)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># output neuron (1x1)</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> np.dot(W3, h2) <span class="op">+</span> b3</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>output layer has no activation function</li>
<li>x in the input layer could be a batch of data where each example is a column vector and would be evaluted in parallel</li>
<li>In practice, it is always better to use these methods to control overfitting instead of the number of neurons.
<ul>
<li>Because smaller NN are harder to train with graident descent. Local minima are increased with larger NN and of more value.</li>
<li>On the other hand, if you train a large network you’ll start to find many different solutions, but the variance in the final achieved loss will be much smaller. In other words, all solutions are about equally as good, and rely less on the luck of random initialization.</li>
</ul></li>
</ul>
</section>
</section>
<section id="optimization-with-backpropagation" class="level2">
<h2 class="anchored" data-anchor-id="optimization-with-backpropagation">Optimization with Backpropagation</h2>
<p>Computing gradients through recursive application of <strong>chain rule</strong>.</p>
<ul>
<li>we get gradients for all parameters: weight matrix and bias vector</li>
<li>to do</li>
</ul>
<section id="keras" class="level3">
<h3 class="anchored" data-anchor-id="keras">Keras</h3>
<div id="0d77b535" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.Sequential([</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>layers.Dense(<span class="dv">500</span>, activation<span class="op">=</span><span class="st">"relu"</span>, input_shape<span class="op">=</span>(<span class="dv">784</span>,)),</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>layers.Dense(<span class="dv">50</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Fully connect hidden layer with 500 units, each unit takes input from all 784 units in previous layer</li>
<li>Fully connected hidden layer with 50 units, each unit takes input from all 500 units</li>
<li>Each unit outpus one value everytime!</li>
<li>Ouput layer: 10 units softmax</li>
</ul>
<div id="6b7c028f" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'sgd'</span>,</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span><span class="st">'categorical_crossentropy'</span>,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># train model with:</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train, y_train_cat, epochs<span class="op">=</span><span class="dv">5</span>, batch_size<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># test on new data</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>test_loss, test_acc <span class="op">=</span> model.evaluate(X_test, y_test_cat)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"test_acc: </span><span class="sc">{</span>test_acc<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Batch: During each epoch (one full pass through the data), the model divides the data into mini-batches of 128 examples. (ca. 60000 /128 = 469 batches)</li>
<li>For each batch: forward pass, compute loss, backward pass, update weights
<ul>
<li>After all 469 batches, one epoche is complete</li>
</ul></li>
</ul>
<p>Sequential adding of layers:</p>
<div id="fbaf1889" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tf.keras.Sequential()</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># From Input to first hidden layer</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>model.add(tf.keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span> <span class="st">"relu"</span>,</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>                                input_shape<span class="op">=</span>(<span class="dv">2</span>,)))</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># From first hidden layer to output layer</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>model.add(tf.keras.layers.Dense(<span class="dv">3</span>, activation<span class="op">=</span><span class="st">"softmax"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Compile():</p>
<ul>
<li>Compiling the Keras model calls the backend Tensorflow and binds the optimizer, loss function, and other parameters required before the model can be run on any input data.</li>
</ul>
</section>
</section>
<section id="training-and-optimizing" class="level2">
<h2 class="anchored" data-anchor-id="training-and-optimizing">Training and Optimizing</h2>
<p>Preprocessing a data matrix x with N x D.</p>
<section id="mean-subtraction-zero-centered" class="level3">
<h3 class="anchored" data-anchor-id="mean-subtraction-zero-centered">Mean Subtraction (Zero-Centered)</h3>
<p>Subtracting mean of all features. Centering data around origin</p>
<div id="026ea811" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">-=</span> np.mean(X, axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># images we could do it for each channel seperatly</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">-=</span> np.mean(X, axis <span class="op">=</span> (<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="normalization" class="level3">
<h3 class="anchored" data-anchor-id="normalization">Normalization</h3>
<p>Features are same scale. Divide by standard deviation after zero-centered</p>
</section>
<section id="pca-and-whitening" class="level3">
<h3 class="anchored" data-anchor-id="pca-and-whitening">PCA and Whitening</h3>
<p>First centered. Next covariance matrix (tell us about correlation structure). Then we can compute the SD factorization of the data covariance matrix</p>
</section>
</section>
</section>
<section id="exercise" class="level1">
<h1>Exercise</h1>
<section id="simple-celsisus-to-fahrenheit-fun-part" class="level2">
<h2 class="anchored" data-anchor-id="simple-celsisus-to-fahrenheit-fun-part">Simple Celsisus to fahrenheit fun part</h2>
<div id="a9c71dd6" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> tf.keras.layers.Input((<span class="dv">1</span>,))</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>l0 <span class="op">=</span> tf.keras.layers.Dense(units<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>l1 <span class="op">=</span> tf.keras.layers.Dense(units<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>l2 <span class="op">=</span> tf.keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tf.keras.Sequential([<span class="bu">input</span>, l0, l1, l2])</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">'mean_squared_error'</span>, optimizer<span class="op">=</span>tf.keras.optimizers.Adam(<span class="fl">0.1</span>))</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>model.fit(celsius_q, fahrenheit_a, epochs<span class="op">=</span><span class="dv">500</span>, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Finished training the model"</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model predicts that 100 degrees Celsius is: </span><span class="sc">{}</span><span class="st"> degrees Fahrenheit"</span>.<span class="bu">format</span>(model.predict(np.array([<span class="fl">100.0</span>]))))</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"These are the l0 variables: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(l0.get_weights()))</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"These are the l1 variables: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(l1.get_weights()))</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"These are the l2 variables: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(l2.get_weights()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="linear-classifier" class="level2">
<h2 class="anchored" data-anchor-id="linear-classifier">Linear Classifier</h2>
<div id="a2f6c5a1" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Train a Linear Classifier</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize parameters randomly</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> <span class="fl">0.01</span> <span class="op">*</span> np.random.randn(D,K)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.zeros((<span class="dv">1</span>,K))</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># some hyperparameters</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>step_size <span class="op">=</span> <span class="fl">1e-0</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>reg <span class="op">=</span> <span class="fl">1e-3</span> <span class="co"># regularization strength</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co"># gradient descent loop</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>num_examples <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># evaluate class scores, [N x K]</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>  scores <span class="op">=</span> np.dot(X, W) <span class="op">+</span> b</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute the class probabilities</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>  exp_scores <span class="op">=</span> np.exp(scores)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># [N x K]</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>  probs <span class="op">=</span> exp_scores <span class="op">/</span> np.<span class="bu">sum</span>(exp_scores, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute the loss: average cross-entropy loss and regularization</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>  correct_logprobs <span class="op">=</span> <span class="op">-</span>np.log(probs[<span class="bu">range</span>(num_examples),y])</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>  data_loss <span class="op">=</span> np.<span class="bu">sum</span>(correct_logprobs)<span class="op">/</span>num_examples</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>  reg_loss <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>reg<span class="op">*</span>np.<span class="bu">sum</span>(W<span class="op">*</span>W)</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> data_loss <span class="op">+</span> reg_loss</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"iteration </span><span class="sc">%d</span><span class="st">: loss </span><span class="sc">%f</span><span class="st">"</span> <span class="op">%</span> (i, loss))</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute the gradient on scores</span></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>  dscores <span class="op">=</span> probs</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>  dscores[<span class="bu">range</span>(num_examples),y] <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>  dscores <span class="op">/=</span> num_examples</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>  <span class="co"># backpropate the gradient to the parameters (W,b)</span></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>  dW <span class="op">=</span> np.dot(X.T, dscores)</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>  db <span class="op">=</span> np.<span class="bu">sum</span>(dscores, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>  dW <span class="op">+=</span> reg<span class="op">*</span>W <span class="co"># regularization gradient</span></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>  <span class="co"># perform a parameter update</span></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>  W <span class="op">+=</span> <span class="op">-</span>step_size <span class="op">*</span> dW</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>  b <span class="op">+=</span> <span class="op">-</span>step_size <span class="op">*</span> db</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="nn" class="level2">
<h2 class="anchored" data-anchor-id="nn">NN</h2>
<ul>
<li>broadcasting of bias vector b:np.dot gives 300x100 matrix and bias is 100x1. Broadcasting give us 300x100 by repeating the vector.</li>
<li>Input values can be bound from 0 to 1: e.g.&nbsp;for greyscale divide by 255, or use normalization</li>
<li>One hot encoding of labels</li>
</ul>
<div id="5d03b077" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>hidden_layer <span class="op">=</span> np.maximum(<span class="dv">0</span>, np.dot(X, W) <span class="op">+</span> b) <span class="co"># b This is called broadcasting: it adds the same bias vector to each row.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="6516d9dd" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize parameters randomly</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> <span class="dv">100</span> <span class="co"># size of hidden layer</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> <span class="fl">0.01</span> <span class="op">*</span> np.random.randn(D,h)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.zeros((<span class="dv">1</span>,h))</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> <span class="fl">0.01</span> <span class="op">*</span> np.random.randn(h,K)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> np.zeros((<span class="dv">1</span>,K))</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co"># some hyperparameters</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>step_size <span class="op">=</span> <span class="fl">1e-0</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>reg <span class="op">=</span> <span class="fl">1e-3</span> <span class="co"># regularization strength</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co"># gradient descent loop</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>num_examples <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10000</span>):</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># evaluate class scores, [N x K]</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>  hidden_layer <span class="op">=</span> np.maximum(<span class="dv">0</span>, np.dot(X, W) <span class="op">+</span> b) <span class="co"># note, ReLU activation</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>  scores <span class="op">=</span> np.dot(hidden_layer, W2) <span class="op">+</span> b2</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute the class probabilities</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>  exp_scores <span class="op">=</span> np.exp(scores)</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>  probs <span class="op">=</span> exp_scores <span class="op">/</span> np.<span class="bu">sum</span>(exp_scores, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>) <span class="co"># [N x K]</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute the loss: average cross-entropy loss and regularization</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>  correct_logprobs <span class="op">=</span> <span class="op">-</span>np.log(probs[<span class="bu">range</span>(num_examples),y])</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>  data_loss <span class="op">=</span> np.<span class="bu">sum</span>(correct_logprobs)<span class="op">/</span>num_examples</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>  reg_loss <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>reg<span class="op">*</span>np.<span class="bu">sum</span>(W<span class="op">*</span>W) <span class="op">+</span> <span class="fl">0.5</span><span class="op">*</span>reg<span class="op">*</span>np.<span class="bu">sum</span>(W2<span class="op">*</span>W2)</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> data_loss <span class="op">+</span> reg_loss</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> i <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"iteration </span><span class="sc">%d</span><span class="st">: loss </span><span class="sc">%f</span><span class="st">"</span> <span class="op">%</span> (i, loss))</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute the gradient on scores</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>  dscores <span class="op">=</span> probs</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>  dscores[<span class="bu">range</span>(num_examples),y] <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>  dscores <span class="op">/=</span> num_examples</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>  <span class="co"># backpropagate the gradient to the parameters</span></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>  <span class="co"># first backprop into parameters W2 and b2</span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>  dW2 <span class="op">=</span> np.dot(hidden_layer.T, dscores)</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>  db2 <span class="op">=</span> np.<span class="bu">sum</span>(dscores, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>  <span class="co"># next backprop into hidden layer</span></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>  dhidden <span class="op">=</span> np.dot(dscores, W2.T)</span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>  <span class="co"># backprop the ReLU non-linearity</span></span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>  dhidden[hidden_layer <span class="op">&lt;=</span> <span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>  <span class="co"># finally into W,b</span></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>  dW <span class="op">=</span> np.dot(X.T, dhidden)</span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>  db <span class="op">=</span> np.<span class="bu">sum</span>(dhidden, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>  <span class="co"># add regularization gradient contribution</span></span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>  dW2 <span class="op">+=</span> reg <span class="op">*</span> W2</span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>  dW <span class="op">+=</span> reg <span class="op">*</span> W</span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>  <span class="co"># perform a parameter update</span></span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a>  W <span class="op">+=</span> <span class="op">-</span>step_size <span class="op">*</span> dW</span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>  b <span class="op">+=</span> <span class="op">-</span>step_size <span class="op">*</span> db</span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a>  W2 <span class="op">+=</span> <span class="op">-</span>step_size <span class="op">*</span> dW2</span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a>  b2 <span class="op">+=</span> <span class="op">-</span>step_size <span class="op">*</span> db2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../wiki/Statistics/glm.html" class="pagination-link" aria-label="General Linear Model">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">General Linear Model</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../wiki/LinAlg/linalg_start.html" class="pagination-link" aria-label="Linear Algebra">
        <span class="nav-page-text">Linear Algebra</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/oli9402">
      <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/oliver_zingg">
      <i class="bi bi-twitter" role="img" aria-label="Twitter">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/oliver-zingg-321b0228a/">
      <i class="bi bi-linkedin" role="img" aria-label="LinkedIn">
</i> 
    </a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>