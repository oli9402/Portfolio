[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "SkillsSprachkenntnisse\n\n\nProgramming | R | Matlab | C | Python\nProgramme | RStudio | SPSS | LaTex | MS-Office | Abelton Live | Markdown\n\n\nEnglisch | Bilingual\nDeutsch | Bilingual"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Oliver Zingg",
    "section": "",
    "text": "Willkommen auf meiner Webseite. Hier findest du mehr Infos über mich, wie auch Beschreibungen meiner bisherigen Projekte. Für Fragen bin ich erreichbar per Email.\n\nAusbildungBerufserfahrungAktueller Status\n\n\nM.Sc. in Psychologie | September 2021 – Aktuell | Universität Zürich | Praktikum noch ausstehend\nB.Sc. in Psychologie | September 2018 - Juli 2021 | Universität Zürich | Nebenfach: Filmwissenschaft\nAssessmentjahr Wirtschaftsinformatik | September 2017 - Juli 2018 | Universität Zürich\n\n\nKabinenreiniger (Flugzeuge) | Juli 2019 - November 2019 | Vebego Airport AG | Zürich\nMitarbeiter Wäscherei Bodensee | November 2015 - Juli 2016 | Wäscherei Bodensee AG | Münsterlingen (TG)\nKommissionierer | August 2015 - September 2015 | Lidl Schweiz | Weinfelden (TG)\n\n\nAktuell bin ich auf der Suche nach einem Praktikum im Bereich Marketing, Meinungsforschung oder User Experience."
  },
  {
    "objectID": "posts/Bachelorarbeit/index.html",
    "href": "posts/Bachelorarbeit/index.html",
    "title": "Bachelorarbeit",
    "section": "",
    "text": "My bachelor thesis “Comparison between Support Vector Machine and Convolutional Neural Network for Alzheimer’s Disease Classification” can be found on GitHub.\n\nKey Takeaways\n\nTopic: Using machine learning for analysing MRI images.\nMethod: Systematic review\nMain pitfalls: Overfitting, unknown ground truth, black box\nGrade: 6\n\n\n\nWhat was is about?\nThere is a rise in using machine learning to analyse MRI images to help identifying Alzheimer’s Disease. In my thesis the current literature on this topic was systematically reviewed. More specifically, two mayor types of machine learning methods were compared: Support Vector Machine and Convolutional Neural Network.\n\n\nWhat I’ve learned?\nMachine learning provides big potential in a lot of areas. Still, there are many pitfalls that need to be addressed. A main problem is overfitting where a statistical model learns random noise that is tied to the training data. Many studies use methods to account for overfitting (e.g., crossvalidiation or independent test data). Nevertheless, inexpensive diagnostical questionnaires tend to perform equally well as more expensive machine learning methods."
  },
  {
    "objectID": "posts/confidence_intervals/index.html",
    "href": "posts/confidence_intervals/index.html",
    "title": "Variable selection",
    "section": "",
    "text": "Author: Oliver Zingg\nFull credit goes to: https://www.kaggle.com/code/hamelg/intro-to-r-part-23-confidence-intervals/notebook\nMy contribution is only adding comments to learn from this code as much as possible"
  },
  {
    "objectID": "posts/confidence_intervals/index.html#generating-data",
    "href": "posts/confidence_intervals/index.html#generating-data",
    "title": "Variable selection",
    "section": "Generating data",
    "text": "Generating data\nThe following example simulates the process of finding the mean age value of a population of voters.\n\nFirst we generate a population with their age values\n\n1 Million values a generated from an exponential distribution and 1.5 Million values from a poisson distribution\n18 is added to make sure that no age value is below 18 years old.\n\n\nFor numbers higher than 100 the modulo (%%) is taken and 18 is added to make sure we don’t end up with too large numbers.\n\n\nCodeset.seed(12)\n\n# Generate a population\npopulation_ages &lt;- c(rexp(1000000,0.015)+18,   \n                    rpois(500000,20)+18,\n                    rpois(500000,32.5)+18,\n                    rpois(500000,45)+18)\n\npopulation_ages &lt;- ifelse(population_ages&lt;100, \n                          population_ages, population_ages%%100+18)\n\n\n\n\n\n\n\n\nCodetrue_mean &lt;- mean(population_ages)   # Check the population mean\n\ntrue_mean"
  },
  {
    "objectID": "posts/confidence_intervals/index.html#drawing-different-sample-from-the-population-and-calculating-the-mean",
    "href": "posts/confidence_intervals/index.html#drawing-different-sample-from-the-population-and-calculating-the-mean",
    "title": "Variable selection",
    "section": "Drawing different sample from the population and calculating the mean",
    "text": "Drawing different sample from the population and calculating the mean\n\nThe central limit theorem states that the distribution of the mean values are normally distributed no matter what the distribution is of the population.\n\nLet’s visualize the distribution of mean values. A skewness of 0 would be a normal distribution.\n\n\n\n\n\n[1] -0.2609927\n\n\nThe skewness is -0.1349 indicating a left sided skewdness"
  },
  {
    "objectID": "posts/confidence_intervals/index.html#confidence-intervals-for-the-mean-values",
    "href": "posts/confidence_intervals/index.html#confidence-intervals-for-the-mean-values",
    "title": "Variable selection",
    "section": "Confidence Intervals for the mean values",
    "text": "Confidence Intervals for the mean values\n\nFirst the sample size is determined then an empty vector is created that will store the boundaries of all confidence intervals\nThe for loop is set to 25, meaning we draw a sample of size 1000 25 times.\nWith qnorm we can get the z value that is used to calculate the boundaries of the confidence intervals\n\n\nCodeset.seed(12)\nsample_size &lt;- 1000\n\nintervals &lt;- c()  # Create and store 25 intervals\n \nfor (sample in 1:25){\nsample_ages &lt;- sample(population_ages, size=sample_size)  # Take a sample of 1000 ages\n\nsample_mean &lt;- mean(sample_ages)  # Get the sample mean\n\nz_critical &lt;- qnorm(0.975)        # Get the z-critical value*\n\npop_stdev &lt;- sd(population_ages)  # Get the population standard deviation\n\nmargin_of_error &lt;- z_critical * (pop_stdev / sqrt(sample_size)) # Calculate margin of error\n\nconfidence_interval  &lt;- c(sample_mean - margin_of_error,  # Calculate the the interval\n                          sample_mean + margin_of_error)  \n\nintervals &lt;- c(intervals, confidence_interval)    \n}\n\ninterval_df &lt;- data.frame(t(matrix(intervals,2,25)))  # Store intervals as data frame"
  },
  {
    "objectID": "posts/confidence_intervals/index.html#plotting-the-25-confidence-intervals",
    "href": "posts/confidence_intervals/index.html#plotting-the-25-confidence-intervals",
    "title": "Variable selection",
    "section": "Plotting the 25 confidence intervals",
    "text": "Plotting the 25 confidence intervals\nThe red line shows the true mean. We see the first confidence interval not including the true mean. (1/25 = 0.04)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nSummary\n\nSimulating confidence intervall shows that some will not include the true value."
  },
  {
    "objectID": "posts/Masterarbeit/index.html",
    "href": "posts/Masterarbeit/index.html",
    "title": "Masterarbeit",
    "section": "",
    "text": "My master thesis “Investigating the Neuronal Basis of Learning Processes and Memory Formation in Children and Adolescents with Various Psychiatric Disorders” can be found on GitHub\n\nKey Takeaways\n\nTopic: Using EEG to track successful learning in children and adolescents with psychiatric disorders\nMethod: Analyzing existing data set\nStand out: Large data set (n &gt; 1000),\nMain pitfalls: Model fit, confoundation, validity of EEG\nGrade: 6\n\n\n\nWhat was is about?\nIt has been suggested that an EEG brain marker can be used to track successful learning. To investigate whether this still holds in a sample of children and adolescents with psychiatric disorders and/or learning problems, such a sample was analysed. More specific a sample of over 1’000 participants with a longitudinal data structure (i.e., data point every 2 ms during a learning task) was analysed using linear mixed-effects models. The results indicated that the same EEG marker can be used in such a population, therefore paving the way for using brain data to aid learning and help with identifying individuals with learning problems.\n\n\nWhat I’ve learned?\nA big part of this thesis consisted of preparing and analyzing a large complex data set (12 GB). This was an interesting experience in which I could learn a lot about programming and challenges that come with such a data set. Additionally, a deeper understanding of complex mixed-effects models was gained. Especially, the limits when it comes to fitting non-normal data and interpretation of the resulting coefficients was challenging. There are still questions regarding the validity of the investigated EEG marker that couldn’t be answered with my thesis. This was due to the nature of EEG data and confounding variables that couldn’t be controlled for. Nevertheless, research in this area could possible lead to promising tools that may be helpful in clinical or school settings."
  },
  {
    "objectID": "posts/Simpsons_Paradox/index.html",
    "href": "posts/Simpsons_Paradox/index.html",
    "title": "Simpson’s Paradox",
    "section": "",
    "text": "Simpson’s Paradox bezeichnet ein statistische Phänomen, in dem die Daten eine hierarchisch angeordnete Struktur aufweisen. Die Analyse der Daten kann deshalb auf unterschiedliche Ebenen dieser Sturktur gemacht werden und Einfluss auf das Resultat haben. Teilweise kann wird dadurch die Interpretation der Forschungsfrage beeinflusst je nach untersuchten Ebenen. Simpson’s Paradox wird oft verwendet um die Vorteile von Linear Mixed Effects Model im Vergleich zur einfacher linearen Regression darzustellen."
  },
  {
    "objectID": "posts/Simpsons_Paradox/index.html#simpsons-paradox",
    "href": "posts/Simpsons_Paradox/index.html#simpsons-paradox",
    "title": "Simpson’s Paradox",
    "section": "",
    "text": "Simpson’s Paradox bezeichnet ein statistische Phänomen, in dem die Daten eine hierarchisch angeordnete Struktur aufweisen. Die Analyse der Daten kann deshalb auf unterschiedliche Ebenen dieser Sturktur gemacht werden und Einfluss auf das Resultat haben. Teilweise kann wird dadurch die Interpretation der Forschungsfrage beeinflusst je nach untersuchten Ebenen. Simpson’s Paradox wird oft verwendet um die Vorteile von Linear Mixed Effects Model im Vergleich zur einfacher linearen Regression darzustellen."
  },
  {
    "objectID": "posts/Simpsons_Paradox/index.html#hierarchische-datenstruktur",
    "href": "posts/Simpsons_Paradox/index.html#hierarchische-datenstruktur",
    "title": "Simpson’s Paradox",
    "section": "Hierarchische Datenstruktur",
    "text": "Hierarchische Datenstruktur\nIm Querschnitt\nBeispiele sind:\n\nSchüler genested in Schulen (Zwei Ebenen)\nSchüler genested in Schulen genested in Kantonen (Drei Ebenen)\nMitarbeiter genested in Unternehmen (Zwei Ebenen)\nIm Längschnitt\nBeispiele sind:\n\nPrüfungsnoten genested in Schüler (Zwei Ebenen)\nKundenzufriedenheit genested Person genested in Altersgruppen (Drei Ebenen)"
  },
  {
    "objectID": "posts/Simpsons_Paradox/index.html#beispiel-pinguine",
    "href": "posts/Simpsons_Paradox/index.html#beispiel-pinguine",
    "title": "Simpson’s Paradox",
    "section": "Beispiel Pinguine",
    "text": "Beispiel Pinguine\nDaten (Gorman (2014)):\n\npalmerpenguins\n\n344 Pinguine\n3 Arten (Adéline, chinstrap und gentoo)\n\n\n\nDie Daten wurden von Dr Allison Horst, Alison Hill und Kristen Gorman zu einem R Packet palmerpenguins verarbeitet. Das folgende Projekt basiert auf dem Turtorial von Silvia Canelon.\nZuerst, R Packete installieren\n\nCodeinstall.packages(\"palmerpenguins\") #Data\ninstall.packages(\"tibble\") #Data handling\ninstall.packges(\"ggplot2\") #Plotting\ninstall.packages(\"dplyr\")\n\n\nPackete laden\n\nCodelibrary(palmerpenguins)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttache Paket: 'dplyr'\n\n\nDie folgenden Objekte sind maskiert von 'package:stats':\n\n    filter, lag\n\n\nDie folgenden Objekte sind maskiert von 'package:base':\n\n    intersect, setdiff, setequal, union\n\nCodepenguins &lt;- palmerpenguins::penguins\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA &lt;NA&gt;   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\nDaten explorieren\n\nCodeggplot(data = penguins,\n       aes(x = sex, y = body_mass_g))+\n  geom_boxplot(aes(fill = species))\n\n\n\n\nDie Boxplotte deuten darauf hin, dass bei Gentoo ein geschlechtsspezifischer Unterschiedlich im Body Mass zu sehen ist, wobei dies möglicherweise nicht der Fall ist bei Adelie und Chinstrap.\n\nCodepenguins %&gt;%\n  select(species, sex, body_mass_g) %&gt;%\n  arrange(desc(body_mass_g))\n\n# A tibble: 344 × 3\n   species sex   body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;       &lt;int&gt;\n 1 Gentoo  male         6300\n 2 Gentoo  male         6050\n 3 Gentoo  male         6000\n 4 Gentoo  male         6000\n 5 Gentoo  male         5950\n 6 Gentoo  male         5950\n 7 Gentoo  male         5850\n 8 Gentoo  male         5850\n 9 Gentoo  male         5850\n10 Gentoo  male         5800\n# … with 334 more rows\n\n\n\nCodepenguins %&gt;%\n  select(species, sex, body_mass_g) %&gt;%\n  group_by(species,sex) %&gt;%\n  summarize(mean = mean(body_mass_g))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex     mean\n  &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt;\n1 Adelie    female 3369.\n2 Adelie    male   4043.\n3 Adelie    &lt;NA&gt;     NA \n4 Chinstrap female 3527.\n5 Chinstrap male   3939.\n6 Gentoo    female 4680.\n7 Gentoo    male   5485.\n8 Gentoo    &lt;NA&gt;     NA \n\n\n\nCodepenguins %&gt;%\n  group_by(species) %&gt;%\n  mutate(n_species = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(species,sex,n_species) %&gt;%\n  summarize(n=n())\n\n`summarise()` has grouped output by 'species', 'sex'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   species, sex [8]\n  species   sex    n_species     n\n  &lt;fct&gt;     &lt;fct&gt;      &lt;int&gt; &lt;int&gt;\n1 Adelie    female       152    73\n2 Adelie    male         152    73\n3 Adelie    &lt;NA&gt;         152     6\n4 Chinstrap female        68    34\n5 Chinstrap male          68    34\n6 Gentoo    female       124    58\n7 Gentoo    male         124    61\n8 Gentoo    &lt;NA&gt;         124     5\n\n\n\nCodepenguins %&gt;%\n  count(species, sex) %&gt;%\n  add_count(species, wt = n,\n            name = \"n_species\")\n\n# A tibble: 8 × 4\n  species   sex        n n_species\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;     &lt;int&gt;\n1 Adelie    female    73       152\n2 Adelie    male      73       152\n3 Adelie    &lt;NA&gt;       6       152\n4 Chinstrap female    34        68\n5 Chinstrap male      34        68\n6 Gentoo    female    58       124\n7 Gentoo    male      61       124\n8 Gentoo    &lt;NA&gt;       5       124\n\n\n\nCodepenguins %&gt;%\n  count(species, sex) %&gt;%\n  add_count(species, wt = n,\n            name = \"n_species\") %&gt;%\n  mutate(prop = n/n_species*100)\n\n# A tibble: 8 × 5\n  species   sex        n n_species  prop\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Adelie    female    73       152 48.0 \n2 Adelie    male      73       152 48.0 \n3 Adelie    &lt;NA&gt;       6       152  3.95\n4 Chinstrap female    34        68 50   \n5 Chinstrap male      34        68 50   \n6 Gentoo    female    58       124 46.8 \n7 Gentoo    male      61       124 49.2 \n8 Gentoo    &lt;NA&gt;       5       124  4.03\n\n\n\nCodepenguins %&gt;%\n  count(species, sex) %&gt;%\n  add_count(species, wt = n,\n            name = \"n_species\") %&gt;%\n  mutate(prop = n/n_species*100) %&gt;%\n  filter(species == \"Chinstrap\")\n\n# A tibble: 2 × 5\n  species   sex        n n_species  prop\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Chinstrap female    34        68    50\n2 Chinstrap male      34        68    50\n\n\n\nCodepenguins_new &lt;-\n  penguins %&gt;%\n  mutate(year_factor = factor(year, levels = unique(year)))\npenguins_new\n\n# A tibble: 344 × 9\n   species island    bill_length_mm bill_d…¹ flipp…² body_…³ sex    year year_…⁴\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;    &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;  \n 1 Adelie  Torgersen           39.1     18.7     181    3750 male   2007 2007   \n 2 Adelie  Torgersen           39.5     17.4     186    3800 fema…  2007 2007   \n 3 Adelie  Torgersen           40.3     18       195    3250 fema…  2007 2007   \n 4 Adelie  Torgersen           NA       NA        NA      NA &lt;NA&gt;   2007 2007   \n 5 Adelie  Torgersen           36.7     19.3     193    3450 fema…  2007 2007   \n 6 Adelie  Torgersen           39.3     20.6     190    3650 male   2007 2007   \n 7 Adelie  Torgersen           38.9     17.8     181    3625 fema…  2007 2007   \n 8 Adelie  Torgersen           39.2     19.6     195    4675 male   2007 2007   \n 9 Adelie  Torgersen           34.1     18.1     193    3475 &lt;NA&gt;   2007 2007   \n10 Adelie  Torgersen           42       20.2     190    4250 &lt;NA&gt;   2007 2007   \n# … with 334 more rows, and abbreviated variable names ¹​bill_depth_mm,\n#   ²​flipper_length_mm, ³​body_mass_g, ⁴​year_factor\n\n\nVisualize Simpson’s Paradox\n\nCodeggplot(data = penguins,\n       mapping = aes(x = bill_length_mm, y = bill_depth_mm))+\n  geom_point() +\n  geom_smooth(method='lm', formula = y ~x)\n\n\n\n\n\nCodepenguins %&gt;%\n  ggplot(mapping = aes(x = bill_length_mm, y = bill_depth_mm, group = species, col = species)) + \n  geom_point() +\n  geom_smooth(method = 'lm', formula = y ~x)\n\n\n\n\nMore information on mixed-effects models and simpson’s paradox (see Hox, Moerbeek, and Van de Schoot 2017)"
  },
  {
    "objectID": "posts/variable_selection/index.html",
    "href": "posts/variable_selection/index.html",
    "title": "Accuracy of selected predictor per sample size",
    "section": "",
    "text": "With this exercise we want to inspect how lm function and step function vary when it comes to selection of correct variables depending on sample size\nOur team: Maué Pantoja , Chiara Breuer, Oliver Zingg\nCode# Load packages \nlibrary(lme4)\nlibrary(dplyr)\nlibrary(ggplot2)\n\noptions(scipen=0)\nset.seed(101)\n\n# Variable and Dataset preparation\nN &lt;- c(25, 30, 40, 60, 100 ,500, 1000)\n# create a data frame to later add which variables were selected \npredi_selection_lm &lt;- as.data.frame(matrix(NA, nrow = 19, ncol = length(N)))\npredi_selection_step &lt;- as.data.frame(matrix(NA, nrow = 19, ncol = length(N)))\n\nnames(predi_selection_lm ) &lt;- c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")\nnames(predi_selection_step ) &lt;- c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")\n\n#counter to use as index for table predi_selection (loop)\na = 1\n\n#loop over different N (sample size)\nfor(i in N){\n  # Simulation dataset\n  x1 &lt;- runif(i, 0, 10)\n  x2 &lt;- runif(i, 0, 10)\n  x3 &lt;- runif(i, 0, 10)\n  x4 &lt;- runif(i, 0, 10)\n  x5 &lt;- runif(i, 0, 10)\n  x6 &lt;- runif(i, 0, 10)\n  x7 &lt;- runif(i, 0, 10)\n  x8 &lt;- runif(i, 0, 10)\n  x9 &lt;- runif(i, 0, 10)\n  x10 &lt;- runif(i, 0, 10)\n  x11 &lt;- runif(i, 0, 10)\n  x12 &lt;- runif(i, 0, 10)\n  x13 &lt;- runif(i, 0, 10)\n  x14 &lt;- runif(i, 0, 10)\n  x15 &lt;- runif(i, 0, 10)\n  x16 &lt;- runif(i, 0, 10)\n  x17 &lt;- runif(i, 0, 10)\n  x18 &lt;- runif(i, 0, 10)\n  x19 &lt;- runif(i, 0, 10)\n  err &lt;- runif(i, 0, 5)\n  \n  data &lt;- cbind(x1,x2,x3,x4,x5,x6, x7, x8, x9 ,x10, x11, x12, x13, x14, x15, x16, x17, x18, x19,  err)\n  data &lt;- as.data.frame(data)\n \n  # generate y values.\n  y &lt;- 30 + 0.4*x1 + 1*x2 + 2.3*x3 + 0.7*x4 + 0.2*x5  + 1*x6 + 2*x7 + 3*x8 + 1.5 *x9 + 0.5*x10 + err\n  \n  # add generated y values to data frame\n  data_2 &lt;- cbind(data, y)\n  \n  # Modelling with all variables as predi.\n  mod &lt;- lm(y~ x1 + x2 + x3 + x4 + x5 + x5 +x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18 + x19, data =  data_2)\n  summary(mod)\n  \n  ########################\n  #lets try step function#\n  ########################\n \n\n  # use step function and save final model as stepwise_model\n  stepwise_model &lt;-  step(mod)\n\n  \n  # Extract p-val: lm\n  p_val_lm &lt;- (summary(mod)$coefficients[,4])\n  p_val_lm &lt;- p_val_lm[2:20] #take out intercept\n  \n  # Extract p-val: step\n  p_val_step &lt;- summary(stepwise_model)$coefficients[,4]\n  p_val_step &lt;- p_val_step[2:20] #take out intercept\n  \n  # Store value: lm \n  predi_selection_lm[a] &lt;- ifelse(p_val_lm &lt; 0.05, 1, 0)\n  \n  #store values for step different, because summary output dosen't include all variable like in lm \n  p_val_step &lt;-  na.omit(ifelse(p_val_step &lt; 0.05, 1, 0))\n \n   #remove selected pred. without 0.05\n  p_val_step &lt;- p_val_step[! p_val_step %in% 0]\n  \n  # save the names of all selected coefficients as coes\n  coes &lt;- names(p_val_step)\n  \n  # create dummy var with all possible variables \n  dummy_var &lt;- c('x1','x2','x3','x4','x5','x6','x7','x8','x9','x10','x11','x12','x13','x14','x15','x16','x17','x18','x19')\n  \n  # which out of all possible variable have been selected? \n  selected_coe &lt;- ifelse(dummy_var %in% coes, 1,0)\n  \n  predi_selection_step[a] &lt;- selected_coe\n  \n  #increase counter by one\n  a = a + 1\n}\n\n# Summary accuracy predictors selection\ntrue_pred_dummy &lt;- c(1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0) #dummy code for the existing predictor \n\n# new data frame to compare true predi. to selected predi.\npredi_accuracy_lm &lt;- data.frame(matrix(NA, nrow = 19, ncol = length(N)))\nnames(predi_accuracy_lm) &lt;- c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")\n\npredi_accuracy_step &lt;- data.frame(matrix(NA, nrow = 19, ncol = length(N)))\nnames(predi_accuracy_step) &lt;- c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")\n\n#if all correct than coloumn all 1. \nfor (j in 1:length(N)){\n  predi_accuracy_lm[j] &lt;- ifelse(predi_selection_lm[j] == true_pred_dummy, 1,0)\n  predi_accuracy_step[j] &lt;- ifelse(predi_selection_step[j] == true_pred_dummy, 1,0)\n}"
  },
  {
    "objectID": "posts/variable_selection/index.html#now-lets-visualize-these-results",
    "href": "posts/variable_selection/index.html#now-lets-visualize-these-results",
    "title": "Accuracy of selected predictor per sample size",
    "section": "Now lets visualize these results",
    "text": "Now lets visualize these results\n\nCode# final data frame used for visualization\nsample_s &lt;- c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")\nsummary_accuracy &lt;- data.frame(cbind(sample_s,rep(NA, times = length(sample_s)),rep(NA, times = length(sample_s))))\nnames(summary_accuracy) &lt;- c(\"sample_s\", \"accuracy_lm\", \"accuracy_step\")\n\n\n\nfor( i in 1: nrow(summary_accuracy)){\n  summary_accuracy[i,2] &lt;-  mean(predi_accuracy_lm[,i])\n  summary_accuracy[i,3] &lt;-  mean(predi_accuracy_step[,i])\n\n}\nsummary_accuracy$accuracy_lm &lt;- as.numeric(summary_accuracy$accuracy_lm)\nsummary_accuracy$accuracy_step &lt;- as.numeric(summary_accuracy$accuracy_step)\nggplot(data = summary_accuracy,aes(x=factor(sample_s, \n                                           level = c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")) ,\n                                            y = accuracy_lm)) + \n  geom_bar(stat= \"identity\")+\n  xlab(\"sample size\")+\n  labs(title = \"Accuracy of selected predictor per sample size\")\n\n\n\n\nThis is the accuracy of the normal lm function with all predictors used.\n\nCodeggplot(data = summary_accuracy,aes(x=factor(sample_s, \n                                           level = c(\"N25\", \"N30\", \"N40\", \"N60\", \"N100\", \"N500\", \"N1000\")) ,\n                                            y = accuracy_step)) + \n  geom_bar(stat= \"identity\")+\n  xlab(\"sample size\")+\n  labs(title = \"Accuracy of selected predictor per sample size\")\n\n\n\n\nThis is the accuracy of the step function default = backwards selection (starting with full model: all predictors) ."
  },
  {
    "objectID": "posts/variable_selection/index.html#summary",
    "href": "posts/variable_selection/index.html#summary",
    "title": "Accuracy of selected predictor per sample size",
    "section": "Summary:",
    "text": "Summary:\n\nespecially with low sample size N25 step function seems to perform less well than normal lm function\nwith increasing sample size both perform well and use all relevant predictors"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Confidence Intervals\n\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\nOliver Zingg\n\n\n\n\n\n\n  \n\n\n\n\nSimpson’s Paradox\n\n\n\n\n\n\n\nLinear Mixed Effects\n\n\nStatistics\n\n\nRandom Effects\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nOliver Zingg\n\n\n\n\n\n\n  \n\n\n\n\nMasterarbeit\n\n\n\n\n\n\n\nData Analysis\n\n\nMixed-Effects Model\n\n\nEEG\n\n\nPsychiatric Disorders\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\nOliver Zingg\n\n\n\n\n\n\n  \n\n\n\n\nAccuracy of selected predictors per sample size\n\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2022\n\n\n\n\n\n\n  \n\n\n\n\nBachelorarbeit\n\n\n\n\n\n\n\nMachine Learning\n\n\nSystematic literature review\n\n\nMRI\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2021\n\n\nOliver Zingg\n\n\n\n\n\n\nNo matching items"
  }
]