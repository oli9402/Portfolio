---
title: "LLM"
format: html
---

Source: Sebastian Raschka 

## Intro

> LLMs have remarkable capabilities to understand, generate, and interpret human
language. However, it’s important to clarify that when we say language models “under-
stand,” we mean that they can process and generate text in ways that appear coher-
ent and contextually relevant, not that they possess human-like consciousness or
comprehension.


> LLMs have remarkable capabilities to understand, generate, and interpret human
language. However, it’s important to clarify that when we say language models “under-
stand,” we mean that they can process and generate text in ways that appear coher-
ent and contextually relevant, not that they possess human-like consciousness or
comprehension.


## Transfomer

> Transformer started out to address the limitations of traditional models like RNNs when dealing with variable-length sequences and context. The heart of the Transformer is its self-attention mechanism, allowing the model to focus on different parts of an input sequence. It comprises an encoder and a decoder. The encoder processes the input sequence to create hidden representations, while the decoder generates an output sequence. Each encoder and decoder layer includes multi-head attention and feed-forward neural networks. Multi-head attention, a key component, assigns weights to tokens based on relevance, enhancing the model’s performance in various NLP tasks. The Transformer’s inherent parallelizability minimizes inductive biases, making it ideal for large-scale pre-training and adaptability to different downstream tasks.
