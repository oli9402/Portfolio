{
  "hash": "680bffce7fa4f9d5b5f6312090d0b316",
  "result": {
    "engine": "jupyter",
    "markdown": "---\nformat: \n  pdf:\n    documentclass: article\n    fontsize: 7pt\n    geometry: margin=0.5in\n    linestretch: 0.95\n    code-block-font-size: \\tiny\n    include-in-header: \n      - text: |\n          \\let\\maketitle\\relax\n          \\usepackage{etoolbox}\n          \\AtBeginEnvironment{Shaded}{\\tiny\\setstretch{0.85}\\vspace{-0.7em}}\n          \\AtEndEnvironment{Shaded}{\\vspace{-0.7em}}\n          \\usepackage{multicol}\n          \\setlength{\\columnsep}{20pt}\n    pdf-engine: lualatex\nexecute: \n  eval: false\n  code-fold: false\n---\n\n#\n# Image Classification (Lecture 1)\n- Goal: predict a label or distribution over labels for a given image (probabilty)\n- Often width (pixel) x height (pixel) x 3 (red, green, blue) arrays, \n- flattend to one large array where each value is an integer from 0 (black) to 255 (white)\n\nSome challenges for computer vision algorithm (must be invariant to cross product of these while retaining sensitivity to inter-class variations): \n\n- Viewpoint variation (orientation of image)\n- Scale variation (variation of size in image and real life)\n- Deformation (objects are not rigid bodies)\n- Occlusion (object can be hidden)\n- Illumination conditions (effects are drastic on pixel level)\n- Background clutter (objects may blend into environment)\n- Intra-class variation (many different types)\n\n\n## Nearest Neighbor Classifier (*k = 1*)\nTakes test image (array) and compares it to every image (array) in training set and takes image label of closest one. *It simply remembers all training data*\n\n- With k = 1 we create \"small islands of liekly incorrect predictions, always use higher k\n\n### How compare to arrays A and B?\n\n**L1 distance/norm**: Sum of over all absolute pixel differences\n$d_1(A,B) = \\sum_i|A_i-B_i|$ If $A = B$ than L1 will be zero.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nclass NearestNeighbor():\n  def __init__(self):\n    pass\n  def train(self, X, y):\n    \"\"\" X is N x D where N is number of examples and D flattend pixel array.\n    Y is 1-dimension of size N.\n    Training is just remembering all data\"\"\"\n    self.X_train = X \n    self.y_train = y\n  \n  def predict(self, X):\n    \"\"\" X is N x D where each row is an example\n    we wish to predict label for \"\"\"\n    num_test = X.shape[0]\n    # lets make sure that the output type\n    # matches the input type\n    Ypred = np.zeros(num_test, dtype = self.y_train.dtype)\n    # loop over all test rows\n    for i in range(num_test):\n      #L1\n      distances = np.sum(np.abs(self.X_train - X[i,:]), axis = 1)\n      # get the index with smallest distance\n      min_index = np.argmin(distances)\n      # predict the label of the nearest example\n      Ypred[i] = self.y_train[min_index]\n    return Ypred\nnn = NearestNeighbor()\nnn.train(X_train, y_train)\ny_test_predict = nn.predict(X_test)\nprint(f'accuracy: {np.mean(y_test_predict == y_test)}')\n```\n:::\n\n\n**L2 distance/norm**:\neuclidean distance between two vectors. Each pixelwise difference is squared, summed up and finally the square root is taken:\n\n$d_2(A,B) = \\sqrt{\\sum_i(A_i-B_i)^2}$\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# new distance L2: we could leave out np.sqrt and get the same final output (monotonic function)\ndistances = np.sqrt(np.sum(np.square(self.X_train - X[i,:]), axis = 1))\n```\n:::\n\n\n**Difference L1 , L2**\n- L2 unforgiving: prefers many medium disagreements to one big one.\n\n## k-Nearest Neighbor Classifier\n\nFind top k closets images and take \"majority vote\" of labels. Higher k-values have a smoothing effect and make classifier more resistant to outliers. \n- Training is very fast\n- Predicting is slow since it compares x_test to each x_training\n- In practice we want efficient predictions!\n- Space inefficient \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nclass KNearestNeighbor():\n  \"\"\" a kNN classifier with L2 distance \"\"\"\n  \n  def __init__(self):\n    pass\n  \n  def train(self, X, y):\n    \"\"\" same as before \"\"\"\n    \n  def predit(self, X, k = 1):\n    \"\"\" returns output of methods: compute_distances and predict_labels\"\"\" \n    \n    return self.predict_labels(dists, k = k)\n  \n  def compute_distacnes(self, X): \n    \"\"\" L2 Distance: for each image in test set X  calculate the L2 distance to each images in X_train. Return a matrix dists where each row is a test image and each column the L2 of corresponding train image\"\"\" \n    \n    num_test = X.shape[0]\n    num_train = self.X_train.shape[0]\n    dists = np.zeros((num_test, num_train))\n\n    for i in range(num_test):\n      dists[i, :] = np.sqrt(np.sum(np.square(\n                            self.X_train - X[i,:]),\n                            axis = 1))\n    return dists\n  \n  def predict_labels(self, dists, k =1): \n    \"\"\"Returns: array with predicted labels \"\"\"\n    \n    num_test = dists.shape[0]\n    y_pred = np.zeros(num_test)\n    \n    for i in range(num_test):\n\n      closest_y = []\n      # get the k indices with smallest distances\n      min_indices = np.argsort(dists[i,:])[:k]\n      closest_y = np.bincount(self.y_train[min_indices])\n      # predict the label of the nearest example\n      y_pred[i] = np.argmax(closest_y)\n    return y_pred\n\n```\n:::\n\n\n- We could speed up with broadcasting rules (see lecture notes)\n- gray regions in visualization of results are caused by ties in majority vote\n\n### k-fold Cross Validation (what k to chose?)\n\n- Hyperparameters = choiches needed to be made (distances, k, etc.)\n- Try out different hyperparameters and use best \n- We further take a small part of training set (e.g. 1000) as validation set which is used as a \"**fake test set**\" to tune the hyperparameter\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nX_val = X_train[:1000, :]\ny_val = y_train[:1000]\n\nX_train = X_train[1000:, :]\ny_train = y_train[1000:]\n\nvalidation_accuracies = []\nfor k in [1, 3, 5, 10, 20, 50, 100]:\n  # use a particular value of k and evaluation on validation data\n  nn = NearestNeighbor()\n  nn.train(X_train, y_train)\n  # here we assume a modified NearestNeighbor class that can\n  # take a k as input\n  y_val_predict = nn.predict(X_val, k = k)\n  acc = np.mean(y_val_predict == y_val)\n  print('accuracy: %f' % (acc,))\n  # keep track of what works on the validation set\n  validation_accuracies.append((k, acc))\n```\n:::\n\n\nWhen training data is smaller or computation is not heavy\n\n- iterate over different validiation sets and average the performance across these\n- 5-fold: split training into 5 parts, iterate 5 times while changing validation part\n- In practice it is cleaner to no include the validation set in the final training of the model after determining the best hyperparameters\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# k-fold cross validation\nnum_folds = 5\nk_choices = [1, 3, 5, 7, 9, 10, 12, 15, 18, 20, 50, 100]\nX_train_folds = []\ny_train_folds = []\n# Split up the training data into folds. After splitting,\n# X_train_folds and y_train_folds should each be lists of\n# length num_folds, where y_train_folds[i] is the label\n# vector for the points in X_train_folds[i]\nX_train_folds = np.split(X_train, [1000, 2000, 3000, 4000])\ny_train_folds = np.split(y_train, [1000, 2000, 3000, 4000])\n# A dictionary holding the accuracies for different values of\n# k that we find when running cross-validation. After running\n# cross-validation, k_to_accuracies[k] should be a list of\n# length num_folds giving the different accuracy values that\n\n# we found when using that value of k.\nk_to_accuracies = {}\n# We perform k-fold cross validation to find the best value of k.\n# For each possible value of k, run the k-nearest-neighbor algorithm\n# num_folds times, where in each case you use all but one of the folds\n# as training data and the last fold as a validation set. Store the\n# accuracies for all fold and all values of k in the k_to_accuracies\n# dictionary.\n\nfor k in k_choices:\n  \n  k_to_accuracies[k] = []\n  classifier = KNearestNeighbor()\n  for i in range(num_folds):\n    #we use ith fold as validation set  \n    X_cv_training = np.concatenate([x for k, x in\n    enumerate(X_train_folds) if k!=i], axis=0)\n    \n    y_cv_training = np.concatenate([x for k, x in\n      enumerate(y_train_folds) if k!=i], axis=0)\n    \n    classifier.train(X_cv_training, y_cv_training)\n    dists = classifier.compute_distances_one_loop(X_train_folds[i])\n    y_test_pred = classifier.predict_labels(dists, k=k)\n    k_to_accuracies[k].append(np.mean(y_train_folds[i] == y_test_pred))\n```\n:::\n\n\n- in practice on may want to avoid cross-validation in favor of single validation split (computationally expensive)\n- Size of single split of training data depends how many hyperparameters\n- For\nexample if the number of hyperparameters is large you may prefer to use bigger validation splits. If the number of examples in the validation set is small (perhaps only a few hundred or so), it is safer to use cross-validation. Typical number of folds you can see in practice would be 3-fold, 5-fold or 10-fold cross-validation.\n\n\\newpage \n# Neural Networks (Lecture 2)\n\n## Linear Classificaiton of Images\n\n1. score function: maps raw data to class scores\n2. loss function: quantifies agreement between predicted and ground truth\n\n$f(x_i, W, b) = Wx_i + b$\n\nParameters:\n\n- $x_i = D *1$ vector $D=32*32*3$\n- $W = K*D$ weight matrix where K is number of classes \n- $b = K*1$ (bias: influences output without interacting with data)\n\nNote:\n\n- Each single matrix multiplication $Wx_i$ evaluates 10 separate classifiers in parallel (each row of W is a classifier)\n  - provides 10 weighted sum of all image pixels: vector of class scores.\n- After training we only need to save the parameters \n- Prediction is just a single matrix multiplication and addition\n- The weights can \"like\" or \"dislike\" certain colors at certain positions in the images\n  - \"Ship\" classifier may have a lot of + weights across blue channels and negative weights in red/green channels: presence of these colors decrease score of ship\n- Each image can be interpretated as point in D (3072) vector space\n  - Each class score is a linear function over this space, so changing weights of a row in $W$ will rotate linear function in different directions \n  - Bias allows to move line up/down/right/left and lines are not forces to go through origin\n- Other interpretation of $W$ that each row is a prototpye of class (template matching)\n- Often bias in incorporated into weight matrix and x is excdented by a constant 1.\n- Class scores can be thought as unnormalized log-probabilities\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nX_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n```\n:::\n\n\n## Loss Function /Cost Function\nMeasures the **unhappiness** with the output. Loss is high with poor classifying the training data\n\n### Cross-Entropy Loss of Softmax Classifier\nSoftmax Classifier is generalization of binary logistic Regression classifier to multiple classes:\n\n- gives normalized class probabilites \n- is applied to every class score\n- softmax function takes class scores $z_j$ and squashes it to a vector of values between zero and one which sum to one (= probabilites). This can be interpreted as confidence in each class\n\n$\\frac{e^{z_j}}{\\sum_ke^{z_k}}$ \n\n\n**Cross-entropy loss**\n$L_i =-log(\\frac{e^z_{y_i}}{\\sum_je^{z_j}})= -z_{y_i} + log\\sum_je^{z_j} + \\lambda\\sum_k \\sum_lW^2_{k,l}$\n\nwith $z_j$ being the j-th element of vector of class scores $z$ and $z_{y_i}$ score of true class. So we just take $-log()$ of the softmax ouput for **true class logit** and ignore the other logits/softmax ouputs. The *higher* the softmax output of true class, the *lower* the loss.\n\nFull loss of dataset is the **mean** of $L_i$ over N training examples:\n$L = \\frac{1}{N}\\sum_iL_i$\n\n**Numberic stability**: we subract our class scorse by the max score in vector, therefore largest value is zero\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndef softmax_loss_vectorized(W, X, y):\n  \"\"\"\n  Softmax loss function, vectorized version.\n  \"\"\"\n  # Initialize the loss\n  loss = 0.0\n  # Compute the softmax loss\n  num_train = X.shape[0]\n  f = X.dot(W)\n  # max of every sample\n  f -= np.max(f, axis=1, keepdims=True)\n  sum_f = np.sum(np.exp(f), axis=1, keepdims=True)\n  p = np.exp(f)/sum_f\n  loss = np.sum(-np.log(p[np.arange(num_train), y]))\n  return loss\n```\n:::\n\n\n- lowest loss is zero.\n\n### Hinge Loss Multiclass Support Vector Machine loss (Max Margin loss)\n\nUsed in SVM. Multiclass Support Vector Machine loss. Takes class scores and summes max of all wrong class scores - class score of correct to determine the loss (end up with N losses):\n\n$L_i = \\sum_{j=y_i}max(0,W_j^T*x_i-W_{y_i}^T*x_i +\\Delta)$\n\nDelta is a hyperparameter which represents a margin that the correct class needs to be higher otherwise we collect loss.\n\nWithout regularization weight matrix can be multiplied by any number and the same loss would be uptained. Therefore, we add a regularization term (L2 norm) to the loss function (elementwise quadratic penalty over all parameters):\n\n$R(W) = \\sum_k\\sum_lW^2_{k,l}$\n\nGiving the full loss of\n$L = \\frac{1}{N}\\sum_iL_i + \\lambda R(W)$\n\nL2-norm can improve generalization because no single parameter should have large influence. Intuitively, this is because the weights in w2 are smaller and more diffuse. Since the L2 penalty prefers smaller and more diffuse weight vectors, the final classifier is encouraged to take into account all input dimensions to small amounts rather than a few input dimensions and very strongly (e.g. [1,0,0,0] vs [0.25, 0.25, 0.25, 0.25]). With the regularization term we can never have a loss of zero, only if we set all $W$ to zero.\n\n- Setting $\\Delta$ is more or less meaningless because weights can shrink or stretch diffferences in margin. \n- Therefore only $\\lambda$ reflects the real tradeoff between data loss and regularization loss\n- In practice, however, if the data is noisy or not perfectly separable, a too-large $\\Delta$ can force the model to memorize the data (pushing scores very far apart), leading to overfitting.\n\n### Comparison of both Loss\n\n- They are comparable, with the main difference that Softmax loss is never happy\n\nMoreover, in the limit where the weights go towards tiny numbers due to very strong regularization strength $\\lambda$, the output probabilities would be near uniform. Hence, the probabilities computed by the Softmax Classifier are better thought of as confidences where, similar to the SVM, the ordering of the scores is interpretable, but the absolute numbers (or their differences) technically are not. n other words, the\nSoftmax Classifier is never fully happy with the scores it produces: the correct class could always have a higher probability and the incorrect classes always a lower probability and the loss would always get better. However, the SVM is happy once the margins are satisfied and it does not micromanage the exact scores beyond this constraint. This can intuitively be thought of as a feature: For example, a car classifier which is likely spending most of its “effort” on the difficult problem of separating cars from trucks should not be influenced by the frog examples, which it already assigns very low scores to, and which likely cluster around a completely different side of the data cloud.\n\n## Optimization\n\n- Finding parameters $W$ that minimizes loss function\n- This is done using gradient descend in the weight/parameter space\n- Mathematically guaranteed direction of deepest descent = gradient of the loss function\n- gradient is generalization of slope for functions that take vector of numbers and is a vector of slopes (partial derivatives) for each dimension. \n- Two calculations: numerical gradient (slow but easy), analytic gradients (fast but error prone)\n- The gradient tells us the direction in which the function has the steepest rate of increase, but it does not tell us how far along this direction we should step.\n- Step size = learning rate (hyperparameter)\n- Linear complecity in number of parameters: for single step D evaluations\n\n### Gradient Check\nCompute analytic gradient (faster) and compare to numerical gradien\n\n### Gradient Descent\nis procedure of computing gradient of loss function and repeatedly evaluating gradient and updating parameter\n\n### Mini-batch gradient descent \n\nComputing loss function only for batch (e.g. 256) to perform weight updates. This works sind data points are often correlated. \n\n- Stochastic Gradient Descent (SGD): extreme case with only 1 example\n- Size of batch is a hyperparameter, set with crossvalidation but often a number to the power of 2 because faster in vectorization\n\n# Neural Network\nA single neuron can be trained as Binary SVM Classifier or Binary Softmax Classifier\n\nUse **normalization** (don't touch training data):\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nmean = train_data.mean(axis=0)\ntrain_data -= mean\nstd = train_data.std(axis=0)\ntrain_data /= std\ntest_data -= mean\ntest_data /= std\n```\n:::\n\n\n## Activation Functions \n\n**Sigmoid Function** drawbacks: \n\n- Sigmoid saturate and kill gradients: If saturated (close to 0 or 1) network barely learns during backprogragtion\n- Sigmoid ouput are not zero-centered: (Page 35)\n\n**Tanh (squashes values to -1:1)**:\n\n- is zero-centered but same problem with saturation\n\n**Rectified Linear Unit (ReLUs)**:\n\n$f(x) = max(0,x)$\n\n- fast convergence in gradient descent\n- easy implementation treshholding a matrix at zero\n- but can be fragile during training and die (if learning rate is set too high)\n- **Leaky ReLUs** helps against dying (has a small negative slope below x = zero)\n- **Maxout neuron** combined best of ReLU and Leaky ReLU but doubles number of parameters\n\n## NN Architectures\n\n- A NN with one hidden layer can represent any continous function but this is of no practicle relevance\n- Non-convex function\n### Layer-wise organzization\n\n- Input layer is not counted towards arichtecture: so not when taking about layers and not when taking about untis/neurons\n- Input layer don't have weights or biases but output layer does\n- For 2-layer NN(4 units + 2 output) with 3 inputs we have: $3 * 4 + 4 *2 = 20$ weights and 1 bias parameter for each unit/neuron. Total = 26 learnable parameters\n\n### Feed-Foward Computation\nThe forward pass of a fully-connected layer corresponds to one matrix multiplication followed by a bias offset and an activation function.\nMatrix form of parameter (e.g. Fully connected 3 * 4 * 4 * 1, with first as input layer):\n\n- Input layer 3 * 1 \n- First hidden layer matrix of 4 * 3, where each row are the 3 weights of a neuron. This allows for easy multiplication from left to right\n- First hidden layer has a bias vector of 4 * 1\n- Second hidden layer: 4 * 4 matrix\n- Output layer: 1 * 4\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# forward-pass of a 3-layer neural network:\n# activation function (use sigmoid)\nf = lambda x: 1.0/(1.0 + np.exp(-x))\n# random input vector of three numbers (3x1)\nx = np.random.randn(3, 1)\n# calculate first hidden layer activations (4x1)\nh1 = f(np.dot(W1, x) + b1)\n# calculate second hidden layer activations (4x1)\nh2 = f(np.dot(W2, h1) + b2)\n# output neuron (1x1)\nout = np.dot(W3, h2) + b3\n```\n:::\n\n\n- output layer has no activation function\n- x in the input layer could be a batch of data where each example is a column vector and would be evaluted in parallel\n- In practice, it is always better to use these methods to control overfitting instead\nof the number of neurons.\n  - Because smaller NN are harder to train with graident descent. Local minima are increased with larger NN and of more value.\n  - On the other hand, if you train a large network you’ll start to find many different solutions, but the variance in the final achieved loss will be much smaller. In other words, all solutions are about equally as good, and rely less on the luck of random initialization.\n  \n## Optimization with Backpropagation\nComputing gradients through recursive application of **chain rule**.\n\n- we get gradients for all parameters: weight matrix and bias vector\n- to do\n\n### Keras \n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\nlayers.Dense(500, activation=\"relu\", input_shape=(784,)),\nlayers.Dense(50, activation=\"relu\"),\nlayers.Dense(10, activation=\"softmax\")\n])\n```\n:::\n\n\n- Fully connect hidden layer with 500 units, each unit takes input from all 784 units in previous layer\n- Fully connected hidden layer with 50 units, each unit takes input from all 500 units\n- Each unit outpus one value everytime! \n- Ouput layer: 10 units softmax\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nmodel.compile(optimizer='sgd',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()\n\n# train model with:\nhistory = model.fit(X_train, y_train_cat, epochs=5, batch_size=128)\n\n# test on new data\ntest_loss, test_acc = model.evaluate(X_test, y_test_cat)\nprint(f\"test_acc: {test_acc}\")\n```\n:::\n\n\n- Batch: During each epoch (one full pass through the data), the model divides the data into mini-batches of 128 examples. (ca. 60000 /128 = 469 batches)\n- For each batch: forward pass, compute loss, backward pass, update weights\n  - After all 469 batches, one epoche is complete \n\nSequential adding of layers:\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nmodel = tf.keras.Sequential()\n# From Input to first hidden layer\nmodel.add(tf.keras.layers.Dense(100, activation= \"relu\",\n                                input_shape=(2,)))\n# From first hidden layer to output layer\nmodel.add(tf.keras.layers.Dense(3, activation=\"softmax\"))\n```\n:::\n\n\nCompile():\n\n-  Compiling the Keras model calls the backend Tensorflow and binds the optimizer, loss function, and other parameters required before the model can be run on any input data. \n\n## Training and Optimizing \nPreprocessing a data matrix x with N x D. \n\n### Mean Subtraction (Zero-Centered)\nSubtracting mean of all features. Centering data around origin \n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nX -= np.mean(X, axis = 0)\n\n# images we could do it for each channel seperatly\nX -= np.mean(X, axis = (0,1,2))\n```\n:::\n\n\n### Normalization \nFeatures are same scale. Divide by standard deviation after zero-centered\n\n### PCA and Whitening \nFirst centered. Next covariance matrix (tell us about correlation structure). Then we can compute the SD factorization of the data covariance matrix \n\n\n\n# Exercise \n\n## Simple Celsisus to fahrenheit fun part\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ninput = tf.keras.layers.Input((1,))\nl0 = tf.keras.layers.Dense(units=4)\nl1 = tf.keras.layers.Dense(units=4)\nl2 = tf.keras.layers.Dense(units=1)\nmodel = tf.keras.Sequential([input, l0, l1, l2])\nmodel.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.1))\nmodel.fit(celsius_q, fahrenheit_a, epochs=500, verbose=False)\nprint(\"Finished training the model\")\nprint(\"Model predicts that 100 degrees Celsius is: {} degrees Fahrenheit\".format(model.predict(np.array([100.0]))))\nprint(\"These are the l0 variables: {}\".format(l0.get_weights()))\nprint(\"These are the l1 variables: {}\".format(l1.get_weights()))\nprint(\"These are the l2 variables: {}\".format(l2.get_weights()))\n```\n:::\n\n\n## Linear Classifier\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n#Train a Linear Classifier\n\n# initialize parameters randomly\nW = 0.01 * np.random.randn(D,K)\nb = np.zeros((1,K))\n\n# some hyperparameters\nstep_size = 1e-0\nreg = 1e-3 # regularization strength\n\n# gradient descent loop\nnum_examples = X.shape[0]\nfor i in range(200):\n\n  # evaluate class scores, [N x K]\n  scores = np.dot(X, W) + b\n\n  # compute the class probabilities\n  exp_scores = np.exp(scores)\n  # [N x K]\n  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n  # compute the loss: average cross-entropy loss and regularization\n  correct_logprobs = -np.log(probs[range(num_examples),y])\n  data_loss = np.sum(correct_logprobs)/num_examples\n  reg_loss = 0.5*reg*np.sum(W*W)\n  loss = data_loss + reg_loss\n  if i % 10 == 0:\n    print(\"iteration %d: loss %f\" % (i, loss))\n\n  # compute the gradient on scores\n  dscores = probs\n  dscores[range(num_examples),y] -= 1\n  dscores /= num_examples\n\n  # backpropate the gradient to the parameters (W,b)\n  dW = np.dot(X.T, dscores)\n  db = np.sum(dscores, axis=0, keepdims=True)\n\n  dW += reg*W # regularization gradient\n\n  # perform a parameter update\n  W += -step_size * dW\n  b += -step_size * db\n```\n:::\n\n\n## NN\n\n\n- broadcasting of bias vector b:np.dot gives 300x100 matrix and bias is 100x1. Broadcasting give us 300x100 by repeating the vector.\n- Input values can be bound from 0 to 1: e.g. for greyscale divide by 255, or use normalization\n- One hot encoding of labels\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nhidden_layer = np.maximum(0, np.dot(X, W) + b) # b This is called broadcasting: it adds the same bias vector to each row.\n```\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# initialize parameters randomly\nh = 100 # size of hidden layer\nW = 0.01 * np.random.randn(D,h)\nb = np.zeros((1,h))\nW2 = 0.01 * np.random.randn(h,K)\nb2 = np.zeros((1,K))\n\n# some hyperparameters\nstep_size = 1e-0\nreg = 1e-3 # regularization strength\n\n# gradient descent loop\nnum_examples = X.shape[0]\nfor i in range(10000):\n\n  # evaluate class scores, [N x K]\n  hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n  scores = np.dot(hidden_layer, W2) + b2\n\n  # compute the class probabilities\n  exp_scores = np.exp(scores)\n  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n\n  # compute the loss: average cross-entropy loss and regularization\n  correct_logprobs = -np.log(probs[range(num_examples),y])\n  data_loss = np.sum(correct_logprobs)/num_examples\n  reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n  loss = data_loss + reg_loss\n  if i % 1000 == 0:\n    print(\"iteration %d: loss %f\" % (i, loss))\n\n  # compute the gradient on scores\n  dscores = probs\n  dscores[range(num_examples),y] -= 1\n  dscores /= num_examples\n\n  # backpropagate the gradient to the parameters\n  # first backprop into parameters W2 and b2\n  dW2 = np.dot(hidden_layer.T, dscores)\n  db2 = np.sum(dscores, axis=0, keepdims=True)\n  # next backprop into hidden layer\n  dhidden = np.dot(dscores, W2.T)\n  # backprop the ReLU non-linearity\n  dhidden[hidden_layer <= 0] = 0\n  # finally into W,b\n  dW = np.dot(X.T, dhidden)\n  db = np.sum(dhidden, axis=0, keepdims=True)\n\n  # add regularization gradient contribution\n  dW2 += reg * W2\n  dW += reg * W\n\n  # perform a parameter update\n  W += -step_size * dW\n  b += -step_size * db\n  W2 += -step_size * dW2\n  b2 += -step_size * db2\n```\n:::\n\n\n",
    "supporting": [
      "dl_files/figure-pdf"
    ],
    "filters": []
  }
}